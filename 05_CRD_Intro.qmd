
# Introduction 

Completely Randomized Designs (CRDs) are the simplest experimental designs. They are used when experimental units are uniform enough. We expect them to react simirlary to a given treatment; we have no reason to suspect that a group of experimental units might react differently to the treatments. We also don't expect any effects (besides possibly a treatment effect) to cause any systematic changes in the response. In other words, we don't have to block for nuisance factors. 

Remember experimental design is the procedure for how experimental units are grouped and treatments are applied. We have already said that there are no blocks in CRDs. So randomisation occurs without restriction and to all experimental units. More generally, the $a$ treatments are randomly assigned to $r$ experimental units, such that each experimental unit is equally likely to receive any of the treatments. This means that there are $N = r \times a$ experimentnal units in total. We only consider designs that are *balanced* meaning that there an equal number of experimental units per treatment, i.e. a treatment is applied to $r$ units. 


## Example: The effect of social media multitasking on classroom performance.

Can we really multitask? I remember as a student I thought I could multitask in lectures, while studying or driving and listening to a podcast. It felt like I was paying attention but in hindsight I can't remember those podcasts well, I know I had to revisit lectures and restart studying sessions. This extends beyond student life, where in the average workspace, tasks are interspersed with social media or email checks and notifcations (). I think most of us are almost always a little bit tempted by our cellphones when we study or work. 

So, if we live in an age of perceived multitasking and getting distracted by our phones and devices, what are the efffects of social media multitasking on our academic performance?  

::: {#warning-example1 .callout-warning icon="false"}

## Example 5.1 

Two researchers from Turkey, @multitask2018, conducted a study to investigate this question. Specifically, they examined the impact of social media multitasking during live lectures on students' academic performance. 

A total of 120 undergraduate students were randomly assigned to one of three groups:

1. **Control Group:** Students used traditional pen-and-paper note-taking.
2. **Experimental Group 1 (Exp 1):** Students engaged in SMS texting during the lecture.
3. **Experimental Group 2 (Exp 2):** Students used Facebook during the lecture.

Over a three-week period, participants attended lectures on Microsoft Excel. Pre-tests and post-tests were administered to measure learning outcomes.

::: 

**The analysis of experimental data is determined by the design.** The design dictates the terms that we will include in our statistical model and so it is crucial to be able to identify the design and blocking and treatment factors. It is also important to check that randomisation has been done correctly and determine the number of replicates used. In the previous chapter we started doing this by creating a summary of the design and we do the same here. From the description of the study, it is clear that:

- **Response Variable:** Academic performance, as measured by test scores.
- **Treatment Factor:** Level of social media multitasking.
- **Treatment Levels (Groups):** Control, Exp 1, and Exp 2.

Students were randomly assigned to one of the three groups, and performance was measured for each individual. Although this may seem obvious, they only took one measurement per student, so we don't have to worry about pseudoreplication. This setup indicates that the students are both the experimental units and the observational units in this study. With a total of 120 experimental units and three treatments, the experiment has 40 replicates. Since only one treatment factor was investigated, and no blocking was performed, this is classified as a **single-factor Completely Randomized Design (CRD).** Here is the study breakdown: 

- **Response Variable:** Academic Performance  
- **Treatment Factor:** Level of Social Media Multitasking  
- **Treatment Levels:** Control, Experimental 1 (SMS), Experimental 2 (Facebook)  
- **Treatments:** Control, SMS multitasking, Facebook multitasking  
- **Experimental Unit:** Student (120)  
- **Observational Unit:** Student (120)  
- **Replicates:** 40 students per group  
- **Design Type:** Single-Factor Completely Randomized Design (CRD)  

Before we continue, now is the time to note that we won't be using the real data collected in this experiment. It wasn't available but I have simulated data to match their results. I've also made some other modifications such as the original study included 122 students but to ensure a balanced design I include only 120. 


## Exploratory data analysis

Before we start any analyses, two things need to be done. First, we start by exploring our data to get familar with the format and to get a feel for any patterns. In R, we read in the data set and then peform a few fommands to check the dataset: 

```{r eda}

multitask <- read.csv("multitask_performance.csv")
nrow(multitask) # check number of rows
head(multitask) # check first 5 rows 
tail(multitask) # check last 5 rows 

summary(multitask)

```

The dataset consits of 120 rows (each row representing a student) and two columns (`Group` and `Posttest`). The first column, `Groups`, contains the treatment the student was assigned and the `Posttest` column contains the response measure. Using the functions `head` and `tail`, we can look at the first and last 5 rows and the function `summary` provies us with a descrption of each column. We do this to check that R has read in our data correctly (you can view the whole data set by running `view(multitask)` as well). The summary tells us that the `Group` column is of the class "character". For our analysis, we want it to be read as a factor: 

```{r factor}

multitask$Group <- as.factor(multitask$Group)
summary(multitask)
```
Now, we can see that there are 40 replicates per treatment group, confirming that the experiment was balanced. I have assumed that based on the resuts shown that the `Posttest` scores were stored as percentages and using the sumamry we can quickly checked whether there are any observations that are not on the appropriate scale. Looks good so far! 

##  Model checking 

@multitask2018 had several research questions, but here we only consider the following: Are there any signficant differences in mean academic performance between the three groups? 

You might think that we could perform three t-tests (Control vs Exp 1, Control vs Exp 3, Exp 1 vs Exp 2). We could, but the problem with this approach is what we call mutliple testing. When conducting many tests, there is an increased risk of making a Type 1 Error (rejecting the null hypothesis when it is in fact true) [^1]. 

[^1]: Can't remember what a $t$-test is and/or need a refresher on hypothesis testing? Have a look <a href="https://www.youtube.com/watch?v=VekJxtk4BYM" target="_blank">this video on t-tests</a>
and document for a brief reminder. **Also, a quick (and cool) sidenote:** This study by @chen2024effect used a Completely Randomized Design (CRD), randomly assigning undergraduate students to playback speed groups (1x, 1.5x, 2x, and 2.5x) to measure the effect on comprehension of recorded lectures. Using ANOVA they found that comprehension was preserved up to 2x speed. I personally like to increase the playback speed to 1.5px if I just need to revise something quickly. 

When we have more than two groups, we can use a one-way analysis of variance (ANOVA) which can be seen as an extention of $t$-test and is called one-way because there is a single factor being considered. For both of these statistical approaches the data should meet certain distributional assumptions: 

1. There are no outliers.
2. All groups have equal population variances.
3. The errors are normally distributed. 
4. The errors are independent. 

### Outliers {.unnumbered}

Outliers are unusual observations (response values) that deviate substantially from the remaining data points. They can have a large influence on the estimates of our model. Think of statistics such as means and variances, outlying observations will shift the mean towards them and distort the variance of the data. 

Sometimes outliers are artefacts of data recording/entering issues, such as a missing decimal points or incorrect scaling (called error outliers). These types of outliers can be corrected and the analysis can be done as usual. If, however, there are freak observations that are not clearly due to anything like data inputting, then they are likely unusual responses and should not be discarded (interesting outliers). There are many ways of identifying and dealing with outliers (@outlier found 29 different ways). Here, it is recommended that the analyses should then be run with and without the outliers to see whether the conclusion depends on their inclusion. When dealing with outliers, it is best to be transparent and clear about how they were handled. Simply removing outliers with no explanation is questionable research practice. 


A good way to check for outliers, is to inspect the data visually with a boxplot of your data grouped by treatment. 

```{r plot}

boxplot(Posttest ~ Group, data = multitask, col = c("skyblue", "lightgreen", "pink"), 
        main = "Posttest Scores by Group", 
        xlab = "Group", 
        ylab = "Posttest Scores")

stripchart(Posttest~Group, data = multitask, vertical = TRUE, add = TRUE, method = "jitter")

```

The first line of code plots the boxplot and by inputting `Posttest~Groups` as the first argument we are say plot the values of `Posttest` by `Groups`. There are extra graphical parameters specfied to make the plot look a bit nicer. Then the function `stripchart` is used to overlay the data points. Based on these plots, there aren't any obvious outlying observations. 


### Equal population variances {.unnumbered}

Since we only have sample data, we would not expect that the variance in each treatment are the same, so just because they are different does not mean the population variances are different. We expect them to differ a bit due to chance simply because we are sampling from a population and everytime we take a sample, the dataset will be different. The sample variances need to be similar enough so that our assumption of qual population variances is reasonable. To check this assumption, we can inspect the boxplots again and their respective heights. More specifically, we look at the interquartile ranges. From looking at the plot, the IQRs do not vary widely. If you prefer to look at the actual values, we can use R to obtain them: 

```{r}
sort(tapply(multitask$Posttest,multitask$Group,IQR))
```

Another measure of variability we can look at, are the standard deviations (sd's). With the same line of code but just replacing the function we want to apply, we obtain the sd's of each group: 

```{r}
sort(tapply(multitask$Posttest,multitask$Group,sd))
```

The rule of thumb is to use the ratio of the smallest to largest standard deviation and check whether it is smaller than five. In our case, the smallest sd (of the Control group) is about 1.5 times smaller than the largest sd (of the Exp 2 group)n . 


### Normally distributed errors {.unnumbered}
### Independent erros {.unnumbered}

TIPS: 

- increase focus, improve studying 
