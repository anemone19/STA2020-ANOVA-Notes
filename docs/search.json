[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA2020 ANOVA Notes",
    "section": "",
    "text": "Preface\nWelcome to the Experimental Design and ANOVA section of STA2020.\nThis book is not an exhaustive guide to designing experiments or conducting ANOVA. Instead, it has been tailored specifically to align with the learning outcomes and methods covered in STA2020.\nThis module consists of four main sections:\n\nExperimental Design\nCompletely Randomized Designs\nRandomized Complete Block Designs\nFactorial Experiments\n\nThe first two chapters lay the groundwork for the module. Once you grasp these concepts, the remaining sections should be easier to follow. Before diving into these topics, there are two preliminary sections:\n\nA brief introduction to statistical modeling\nA guide to hypothesis testing\n\nI encourage you to read through these first, as they provide essential context for the rest of the material.\nThroughout the book, you will find R code presented in chunks like this:\n\nx &lt;- c(1,2,3,4,5)\nmean(x) # Computes the mean of a set of numbers  \n\n[1] 3\n\n\nR is consistently used to visualize, illustrate, and demonstrate key methods and concepts. Running the code yourself will greatly enhance your understanding, so I encourage you to do so.\n\nSome parts of these notes have been adapted from the STA2007 notes, authored by Dr. Res Altwegg and Dr. Birgit Erni, as well as from various textbooks.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00_ModelConcepts.html",
    "href": "00_ModelConcepts.html",
    "title": "Statistical Modelling",
    "section": "",
    "text": "What is a Model?\nWhat do we mean by modelling? There are different kinds of models. There are mathematical models and statistical models. And then we have some other models.\nWhen we speak about models, we mean a representation of the thing we are interested in. Models share some features with the ‘real thing’, e.g. model aeroplanes can fly and look like real aeroplanes. However, they are a simplification and do not have all the features of the ‘real thing’. For example, these model aeroplanes cannot carry people. Models have some advantages: even though they can crash like a real aeroplane, model aeroplanes are less likely to hurt people in the process; and, because they are simpler, they are easier to understand and fix.\nMathematical and statistical models, in analogy to the model aeroplanes, are a simplified representation of a complex system we would like to understand. These models are not meant to represent the original system exactly but should provide useful abstractions. Their value is that they are easier to study and understand than the real system. Since models necessarily simplify reality, they are usually good for studying certain aspects of the real system but they fail at explaining other features. The art of modelling is therefore to come up with a simple representation that helps you understand the particular aspect you are interested in. To go back to the analogy of model planes: if you are interested in understanding aerodynamics, a disconnected wing may give you answers to some questions and a toy aeroplane with the right shape and proportions may give you answers to other questions. If you want to learn to fly a real aeroplane, a simulator may be a better model for you.\nLikewise, you need to choose the appropriate mathematical and statistical model to answer your question. The eminent statistician George Box famously said “All models are wrong but some are useful”. Remember this quote. The conclusions you will draw from data always also depend on the models you used to obtain your results. It is incorrect to say “my data show…”. You can say “in the light of my model, the data show…”. As a data analyst, you will hear people complain that your model is wrong. Of course it is wrong; it is meant to be. If reality was simple you would not need a model to understand it and if your model is as complex as reality, it won’t help you understand anything. However, you are responsible for ensuring that your model is useful. This is in no way guaranteed.\nA statistical model is a mathematical representation of how data is generated. It describes the relationship between observed data and underlying factors (parameters) while accounting for random variation. Suppose that we are interested in estimating the age of a tree from its stem diameter. To do this we need to know by how much the stem diameter increases per year. We could describe this relationship or process as follows:\n\\[D = \\alpha + \\beta \\times Age\\] describing a linear increase of diameter with age. Once we have a good idea of how fast diameter increases with age (β) we can predict diameter from age. The (mathematical) model above is a very simple representation of this process with only two parameters, the intercept and the growth rate.\nWith the chosen parameter values, diameter increases linearly with age. Of course, this model is not realistic except for special situations but it gives us powerful insights. In reality we don’t know \\(\\beta\\), but usually need to estimate it from data. Also, not every tree grows equally fast, because of environmental and individual differences between trees. We can accept that the above is a simple model for the average behaviour of a tree, but to capture variability between trees (because of variability between environmental conditions from tree to tree, variability between individual trees, measurement error), we add an error term.\n\\[D = \\alpha + \\beta \\times Age_i + e_i\\]\nThe response that we observe is then described by an average behaviour, but the actual observed value will vary around this average. To summarise, the statistical model has a stochastic component which captures variability in the response that cannot be explained by the deterministic part of the model. Another distinguishing feature of statistical modelling is that we obtain estimates of the parameter values from the data, e.g. by fitting a line to the observations, i.e. we learn from data.",
    "crumbs": [
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "00_ModelConcepts.html#more-generally",
    "href": "00_ModelConcepts.html#more-generally",
    "title": "Statistical Modelling",
    "section": "More generally",
    "text": "More generally\nStatistical models are not perfect predictors of the data, rather they attempt to describe the “central tendency” of the observations. To get to the actual observed value some deviation from the central tendency needs to added (i.e. error). Such models typically have the following the form:\n\\[\n\\text{Observed Response} = \\text{Model Predicted Response} + \\text{Error}\n\\]\nMathematically this can be stated as:\n\\[Y = \\hat{Y} + e\\]\nA simple example of a statistical model you may have encountered is the mean as a predictor. Suppose you measure the number of customers entering two stores over 20 days. The observed counts for each store fluctuate daily, but you may want to summarize the data using the average number of customers.\nFor each store \\(i\\), a basic statistical model for these observations would be:\n\\[\nY_{ij} = \\mu_i + e_{ij}\n\\]\nwhere:\n\n\\(Y_{ij}\\) is the number of customers observed on day \\(j\\) at store 1,\n\\(\\mu_i\\) is the true mean number of customers at store \\(i\\),\n\\(e_{ij}\\) is the error term, representing deviations from the mean.\n\nThe error term \\(e_{ij}\\) accounts for day-to-day fluctuations that cause the actual number of customers to vary around the mean. Below this data is simulated and plotted, with the model overlain. The black line is the mean and the red dashed line represents the error for one observation, i.e. deviation from the fitted model response, in this case the mean.\n\n\nCode\nstore1 &lt;- rpois(20, 50)\nstore2 &lt;- rpois(20, 15)\nstoredata &lt;- data.frame(numcust = c(store1, store2),\n                        store = factor(rep(c(\"Store 1\", \"Store 2\"), each = 20)))\n\nstripchart(numcust ~ store, data = storedata,\n           method = \"jitter\", pch = 16, col = c(\"deepskyblue\", \"orange\"),\n           vertical = TRUE, main = \"Customer Counts per Store\",\n           xlab = \"Store\", ylab = \"Number of Customers\")\nmeans &lt;- tapply(storedata$numcust, storedata$store, mean)\nsegments(x0 = 1:2- 0.1, x1 = 1:2 + 0.1, y0 = means, y1 = means, lwd = 3, col = \"black\") \nmin_count &lt;- min(storedata$numcust[storedata$store == \"Store 1\"])\nmin_x &lt;- jitter(rep(1, sum(storedata$numcust == min_count))) \npoints(min_x, min_count, col = \"red\", pch = 16, cex = 1.2) \nsegments(x0 = min_x, x1 = min_x, y0 = min_count, y1 = means[\"Store 1\"], col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\n\nAnother basic example of this structure is a linear regression model:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + e_i\n\\]\nwhere:\n\n\\(Y_i\\) is the observed response,\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters representing the intercept and slope,\n\\(X_i\\) is the predictor variable,\n\\(e_i\\) is the random error term.\n\n\n\nCode\n# Generate random x values and error term\nset.seed(123)  # Ensures reproducibility\nx &lt;- rnorm(35, mean = 35, sd = 5)\nerror &lt;- rnorm(35, mean = 0, sd = 5)\n\n# Define true model parameters\nbeta0 &lt;- 2\nbeta1 &lt;- 1.5\n\n# Generate y values based on the regression model\ny &lt;- beta0 + beta1 * x + error\n\n# Fit a linear regression model\nmodel &lt;- lm(y ~ x)  # This was missing!\n\n# Select an observation to highlight\nobs_index &lt;- 20  \nx_obs &lt;- x[obs_index]\ny_obs &lt;- y[obs_index]\ny_pred &lt;- predict(model, newdata = data.frame(x = x_obs))  \n\n# Scatter plot of data points\nplot(x, y, pch = 16, col = \"darkseagreen\",\n     xlab = \"X\", ylab = \"Y\",\n     main = \"Scatter Plot with Regression Line\",\n     cex.lab = 1.5, cex.axis = 1.2, cex.main = 1.5)\n\n# Add regression line\nabline(model, col = \"black\", lwd = 2)\n\n# Highlight the observed point\npoints(x_obs, y_obs, col = \"red\", pch = 16, cex = 1.2)  \n\n# Draw a dashed vertical line from the predicted value to the observed value\nsegments(x0 = x_obs, x1 = x_obs, y0 = y_pred, y1 = y_obs, col = \"red\", lwd = 2, lty = 2)",
    "crumbs": [
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "00_ModelConcepts.html#notation",
    "href": "00_ModelConcepts.html#notation",
    "title": "Statistical Modelling",
    "section": "Notation",
    "text": "Notation\nWhen we fit the model to our data, we estimate the unknown parameters using observed data. We denote these estimates using hat notation to distinguish them from the true (but unknown) population parameters:\n\\[\n\\hat{\\beta}_0, \\quad \\hat{\\beta}_1\n\\]\nSimilarly, the fitted values (model-predicted responses) are denoted as:\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i.\n\\]\nThus, after fitting the model, the observed response can be rewritten as:\n\\[\nY_i = (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i) + e_i = \\hat{Y}_i + e_i\n\\]\nwhere:\n\n\\(\\hat{Y}_i\\) is the fitted (predicted) value, and\n\\(e_i = Y_i - \\hat{Y}_i\\) is the residual, representing the difference between the observed and predicted values.\n\n\nThese notes are largely from STA2007 notes (authored by Dr Res Altwegg and Dr Birgit Erni and some other textbooks.",
    "crumbs": [
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "00_HypoTesting.html",
    "href": "00_HypoTesting.html",
    "title": "A brief guideline to hypothesis testing",
    "section": "",
    "text": "The General Framework\nHypothesis testing is a statistical procedure of using sample data to make inferences about populations. Unlike estimation, where the goal is to quantify a parameter, hypothesis testing assesses whether an observed effect is statistically significant. More specifically, a hypothesis test evaluates two mutually exclusive statements about the population and determines which statement the data supports.\nHypothesis testing follows a structured process:\nThe basic idea of hypothesis testing is that we set up a so-called null hypothesis and then ask how likely our data are if the null hypothesis were true. If they are unlikely, we conclude that we have found evidence against the null hypothesis, i.e. the null hypothesis is probably not true.\nThe alternative hypothesis covers all the possibilities not covered by the null hypothesis. If we conclude that the null hypothesis is probably not true, that means that the alternative hypothesis is probably true. These two hypotheses are not equal in how we treat them:\nBut we never prove the alternative hypothesis outright—we only show that the null is unlikely based on the evidence. You can think of the null hypothesis as representing a baseline against which the data are compared, whereas the alternative hypothesis is what we really care about, worry about or want to demonstrate. This is an important asymmetry and will need some careful reflection.\nBelow is an example:\nNull Hypothesis (\\(H_0\\)): “The average weight of chocolate bars is 100g.”\nAlternative Hypothesis (\\(H_A\\)): “The average weight of chocolate bars is less than 100g.”\nLack of evidence against \\(H_0\\) is not the same as evidence for \\(H_0\\). We never say that we have evidence for \\(H_0\\) or that we accept \\(H_0\\) as true.\nA numerical function of the data that quantifies the strength of the observed effect, whose value determines the result of the test. Examples include the mean difference, proportion difference, or z-score.\nWe have a test statistic and to say something about how likely this test statistic (or more extreme is) under the null hypothesis, we need the null distribution of the test statistic (that is the sampling distribution of the test statistic as if the null hypothesis were true). We then compared the observed value of the test statistic to that null distribution and asked ourselves how unusual it is in light of that distribution.\nThe probability of obtaining a result as extreme as the observed one if H₀ is true. A small P-value (typically &lt;0.05) suggests strong evidence against (\\(H_0\\)).\nIn the approach you have been taught, we compare the P-value to a predefined significance level () and conclude whether to reject \\(H_0\\). Here we would like to emphasise that the p-value is a measure of evidence against \\(H_0\\) - see below!",
    "crumbs": [
      "A brief guideline to hypothesis testing"
    ]
  },
  {
    "objectID": "00_HypoTesting.html#one-sided-vs.-two-sided-tests",
    "href": "00_HypoTesting.html#one-sided-vs.-two-sided-tests",
    "title": "A brief guideline to hypothesis testing",
    "section": "One-Sided vs. Two-Sided Tests",
    "text": "One-Sided vs. Two-Sided Tests\nTwo-sided test: Tests for deviations in both directions. Example: “The average human body temperature is different from 37°C.”\nOne-sided test: Tests for deviations in a single direction. Example: “Students who study more than an hour score higher.”",
    "crumbs": [
      "A brief guideline to hypothesis testing"
    ]
  },
  {
    "objectID": "00_HypoTesting.html#decision-making-in-hypothesis-testing",
    "href": "00_HypoTesting.html#decision-making-in-hypothesis-testing",
    "title": "A brief guideline to hypothesis testing",
    "section": "Decision Making in Hypothesis Testing",
    "text": "Decision Making in Hypothesis Testing\nA small P-value constitutes evidence against \\(H_0\\). But how small is small enough? Sometimes, we want to make a firm decision about whether we can believe that the observed pattern is real or not. This requires us to choose a threshold for P. This threshold is called the significance level and denoted by \\(\\alpha\\). If we obtain a P-value that is smaller than \\(\\alpha\\), we say that we have obtained a “statistically significant result” or that “\\(H_0\\) is rejected”. If our P-value is larger than \\(\\alpha\\), we say that our result is “not significant” or that “\\(H_0\\) is not rejected”. In most situations, researchers choose a significance level of \\(\\alpha\\) = 0.05, which roughly corresponds to the probability of obtaining five heads in a row when tossing a fair coin, not a very likely event! Different values for \\(\\alpha\\) are also sometimes used; the next most common significance level is \\(\\alpha\\) = 0.01.\nBefore we go further, we want to emphasize that there is nothing magic about a specific value of \\(\\alpha\\). This threshold is an arbitrary choice and should not be taken too seriously. There is not much difference between a P-value of 0.051 and 0.049. Both constitute about the same strength of evidence against \\(H_0\\). Yet, when we apply \\(\\alpha\\) = 0.05, we would reach opposite conclusions in the two cases. It is always better to report the exact P-value rather than just state P \\(&gt;\\) 0.05 or P \\(&lt;\\) 0.05 or state that a result is “not significant” or “significant”. And it is particularly important not to imply that a “non-significant” result means that there is no effect (that would be saying \\(H_0\\) is true when we might in fact have some evidence against it)!\nAlas, dividing results into “significant” vs “not significant” is very entrenched in many fields and you will encounter these terms a lot. And used wisely, this distinction can have its merits. So we’ll stick with it.\n\nThese notes have been adapted from the STA1007 notes (authored by Dr Res Altwegg and Dr Greg Distiller and some other textbooks.",
    "crumbs": [
      "A brief guideline to hypothesis testing"
    ]
  },
  {
    "objectID": "01_ExpDesign_Why.html",
    "href": "01_ExpDesign_Why.html",
    "title": "1  Experiments and experimental design",
    "section": "",
    "text": "Key points\nThere are two fundamental ways to obtain information in research: by observation or by experimentation. In an observational study the observer watches and records information about the subject of interest. In an experiment, the experimenter actively manipulates variables hypothesized to affect the response (insert small example). Although both are important ways of understanding the world around us, only through experiments can we infer causality.\nThat is, by designing and conducting an experiment properly, if we observe a result such as a change in variable A leads to a change in our response (say variable B), we can confidently conclude that A caused this change in B. If we were to merely study variable B and observe that as variable A changes, B also changes without conducting an experiment, then we can only say that variable A and B are associated. We could not easily conclude that any change in B is due to A. It could be some other factor that is correlated with A or it could be that B caused the change in A! The key is that a well-designed experiment controls and holds constant (as best we can) all other factors that might affect the response, so we can be sure the result is caused by the variable we manipulated.\nImagine a company wants to determine whether their voluntary employee training program (the explanatory variable) increases productivity (the response). They decide to track the productivity of employees who chose to complete the training and those who did not. They note that, on average, trained employees are more productive. Can we confidently conclude that the training program caused increased productivity?\nThis is an observational study since no variable was actively manipulated, they merely observed and recorded the productivity of two groups of employees. So, we cannot conclude that completing the training program increases productivity - we cannot infer causality. It could be due to many other factors, either observed or unobserved, such as maybe employees who choose to do the training program are inherently more motivated and thus productive. Can you think of any other factors?\nIf they actively manipulate the explanatory variable, training program, by randomly assigning employees to complete the training program or not and control other factors by ensuring the employees are as similar as possible accross the groups (i.e. conducted an experiment). Any differences in productivity between the two groups could then be ascribed to the training program. If they happen to find that the employees who were assigned the training program are more productive, they can confidently say that the program caused increased productivity (and perhaps make it compulsory for all employees!).\nExperimental studies are extremely important in research and in practice. They are almost the only way in which one can control all factors to such an extent as to eliminate any other possible explanation for a change in a response other than the variable actively manipulated. In this course, we only consider experimental studies and those which aim to compare the effects of a number of treatments (comparative experiments).\nHere are some other reasons for conducting experiments:\nExperimental studies and their design are fundamental to science, allowing us to further knowledge and test theories. So lets define them more rigorously. We’ll start by introducing some terminology.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments and experimental design</span>"
    ]
  },
  {
    "objectID": "01_ExpDesign_Why.html#key-points",
    "href": "01_ExpDesign_Why.html#key-points",
    "title": "1  Experiments and experimental design",
    "section": "",
    "text": "Two ways of doing research: observation and expermentation.\nExperimentation is the path to causality.\nExperiments actively manipulate variables to isolate their effects on a response while controlling everything else.\nWe consider comparative experiments where the aim is to compare treatments.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments and experimental design</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html",
    "href": "02_ExpDesign_Term.html",
    "title": "2  Terminology",
    "section": "",
    "text": "Treatment factors, treatment levels and treatments:\nThe treatment factor is the factor or variable that the experimenter actively manipulates to measure its effect on the response. All factors/variables that are investigated, controlled, manipulated, thought to influence the response, are called the treatment factors. They become the explanatory variables (mostly categorical) in the model. For each treatment factor, we actively choose a set of levels. For example, the treatment factor “temperature” can have levels 10, 20, and 50°C. If temperature is the only treatment factor in the experiment, the treatments1 will also be 10, 20, and 50°C.\nIf we manipulate more than one factor (e.g., temperature and pressure), we have two treatment factors. When several treatment factors are manipulated, the experiment is called factorial and the treatments are all possible combinations of the factor levels. If we have pressure levels “low” and “high,” there are 6 treatments in total:\nFigure 2.1: Visualization of how treatments are formed as combinations of treatment levels.\nIn the figure above, there are two treatment factors: Temperature (on the y-axis) and Pressure (on the x-axis). The axis ticks represent the levels of each treatment factor, and the blocks within the grid represent the treatments, which are specific combinations of the levels of Temperature and Pressure. Each treatment is labeled with the corresponding combination of levels (e.g., ‘50, Low’ or ‘10, High’).\nWhen faced with a text like this, it is useful to identify the treatment factors, their levels and the treatments, as well the response. Clearly, from the question, we are interested in the effect of therapy on test anxiety. A statement like this can generally be read as the effect of the treatment factor on the response. Nowhere is another treatment factor mentioned, so we only have one in this example. What are the levels of therapy we set? The levels are 5, 10 and 15 hours of therapy and since we only have one factor these are also the treatments. Let’s summarise this as follows:",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#treatment-factors-treatment-levels-and-treatments",
    "href": "02_ExpDesign_Term.html#treatment-factors-treatment-levels-and-treatments",
    "title": "2  Terminology",
    "section": "",
    "text": "Example 1\n\n\n\nThree groups of students, 5 in each group, were receiving therapy for severe test anxiety. Group 1 received 5 hours, group 2 received 10 hours and group 3 received 15 hours. At the end of therapy each subject completed an evaluation of test anxiety. Did the amount of therapy have an effect on the level of test anxiety?\nThe three groups of students received the scores on the Test Anxiety index (TAI) at the end of treatment shown in the table below.\n\n\n\nGroup 1\nGroup 2\nGroup 3\n\n\n\n\n48\n55\n51\n\n\n50\n52\n52\n\n\n53\n53\n50\n\n\n52\n55\n53\n\n\n50\n53\n50\n\n\n\n\n\n\n\n\n\nResponse: Test Anxiety\n\nTreatment Factor: Therapy\n\nTreatment Levels: 5, 10, and 15 hours of therapy\n\nTreatments: 5, 10, and 15",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#experimental-and-observational-unit",
    "href": "02_ExpDesign_Term.html#experimental-and-observational-unit",
    "title": "2  Terminology",
    "section": "Experimental and observational unit",
    "text": "Experimental and observational unit\nThe experimental unit is the entity (e.g. material, object, or individual) to which a treatment is assigned or that receives the treatment. By contrast, the observational unit is the entity from which the response is recorded. This distinction is very important because it is the experimental units which determine how often the treatment has been replicated and therefore the precision with which we can measure the treatment effect. In the methods that we cover in this course, we require that in the end there is only one ‘observation’ (response value) per experimental unit. If several measurements have been taken on an experimental unit, we will combine these into one observation, typically by taking the mean. Very often, the experimental unit is also the observational unit.\nWhat are the experimental units? To determine this, revisit the text of Example 1 and ask yourself: what entity received the treatments or to what were treatments applied? Most of you, will probably answer the students and this is correct. Each student received the respective treatment (number of hours in therapy) assigned to their group and so there are \\(5 \\times 3 = 15\\) experimental units.\nThere is an argument to be made that it is not clear whether the students received therapy on their own or that the groups of students received therapy together. In that case, treatments were applied to groups of students and so there would be three experimental units. This will usually be clear from the text, but we’ll use this scenario to illustrate some concepts as we go.\nWe also need to know what the observational units are. The text states that at the end of therapy, each student completed an evaluation to determine their level of test anxiety. So the response, test anxiety, was measured on the student level which means students are the observational units. In the first scenario, the students are both the experimental units and observational units. But this would not be the case if groups are the experimental unit.\nWe also require that there is only one observation per experimental unit, the first scenario meets this requirement. For the second scenario, we have 5 observations per group and so we would have to take the mean of these values to end up with one response value per group.\nLet’s add to the summary assuming students are the experimental units:\n\nExperimental unit (no): Student (15)\n\nObservational unit (no): Student (15)",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#homogeneity-of-experimental-units",
    "href": "02_ExpDesign_Term.html#homogeneity-of-experimental-units",
    "title": "2  Terminology",
    "section": "Homogeneity of experimental units",
    "text": "Homogeneity of experimental units\nWhen the set of experimental units are as similar as possible such that there are no distinguishable differences between them, they are said to be homogeneous (a fancy word for saying they are of the same kind). The more homogeneous the units are, the smaller the experimental error variance (natural variation between between observations of the same treatments) will be. It is super important to have fairly homogeneous units because it allows us to detect differences between treatments more easily.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#blocking",
    "href": "02_ExpDesign_Term.html#blocking",
    "title": "2  Terminology",
    "section": "Blocking",
    "text": "Blocking\nIf the experimental units are not fairly similar but are heterogeneous (the opposite of homogeneous), we can group them into sets of similar units. This process is called blocking and the groups are considered “blocks”. We compare the treatments within each block as if each block is its own mini-experiment. This way we account for the differences between blocks and can better isolate the effect of the treatments.\n\n\n\n\n\n\nExample 2\n\n\n\nImagine you’re testing the effectiveness of two marketing strategies (A and B) to increase sales at a chain of coffee shops. The coffee shops are located in different neighborhoods, where factors like income levels might influence sales. To prevent these differences from skewing the results, you group the coffee shops into “blocks” based on neighborhood characteristics such as income level (e.g., low, medium, high).\nWithin each block, you randomly assign coffee shops to either Strategy A or Strategy B. This approach allows you to compare the strategies while controlling for variability caused by differences in neighborhood features.\nWithout blocking, would you be able to confidently attribute differences in sales to the strategies alone? Likely not, as any observed differences could be due to neighborhood-specific factors rather than the strategies themselves.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#replication-and-pseudoreplication",
    "href": "02_ExpDesign_Term.html#replication-and-pseudoreplication",
    "title": "2  Terminology",
    "section": "Replication and pseudoreplication",
    "text": "Replication and pseudoreplication\nIf a treatment is applied independently to more than one experimental unit it is said to be replicated. Treatments must be replicated! Making more than one observation on the same experimental unit is not replication, but pseudoreplication. Pseudoreplication is a common fallacy. The problem is that without true replication, we don’t have an estimate of uncertainty, of how repeatable, or how variable the result is if the same treatment were to be applied repeatedly.\nIn Example 1, if experimental units were the groups and we didn’t take the average of the observations per group, we would have pseudoreplication as each student would not be an independent replicate of a treatment - effectively, we have only applied each treatment once. You might notice that we then only have one true replicate per treatment group and this is problematic. To get an estimate of uncertainty, we would have to repeat this experiment a few more times to get more than one proper replicate.\nThe first scenario, however, did not have this problem and each treatment was replicated five times. After going through all this, we have the following summary:\n\nResponse: Test Anxiety\n\nTreatment Factor: Therapy\n\nTreatment Levels: 5, 10, and 15 hours of therapy\n\nTreatments: 5, 10, and 15\n\nExperimental unit (no): Student (15)\n\nObservational unit (no): Student (15)\n\nReplicates: 5\n\n\n\n\n\n\n\nTip\n\n\n\nCreating a summary like this, is a handy exercise for any experiment you come across, and we’ll keep doing it for every experiment in this book. As we go along, we’ll also add information about the type of experiment that was conducted.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#footnotes",
    "href": "02_ExpDesign_Term.html#footnotes",
    "title": "2  Terminology",
    "section": "",
    "text": "The terminology of treatments can be traced back to 1920’s when it was first applied by Ronald Fisher in the agricultural sciences. He is often refered to as the Founder of Statistics! Have a look at the very first application of ANOVA here and also a nice article describing the history of statistics and his contribution to the field.↩︎",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "03_ExpDesign_RRR.html",
    "href": "03_ExpDesign_RRR.html",
    "title": "3  The three R’s of experimental design",
    "section": "",
    "text": "Replication\nExperimental Design is a detailed procedure for grouping, if blocking is necessary, experimental units and for how treatments are assigned to the experimental units. There are three fundamental principles, known as the ‘three R’s of experimental design’ which are at the core of a good experiment. The following section might feel a bit repetitive, but these concepts cannot be emphasised enough.\nLet’s define it again: replication is when each treatment is applied to several experimental units. This ensures that the variation between two or more units receiving the same treatment can be estimated and valid comparisons can be made between treatments. In other words, replication allows us to separate variation due to differences between treatments from variation within treatments. For true replication, each treatment should be independently applied to several experimental units. If this is not the case, treatment effects become confounded with other factors.\nConfounding means that is not possible to separate the effects of two (or more) factors on the response, i.e. it is not possible to say which of the two factors is responsible for any changes in the response. This is what happened in the Example 1 when groups are the experimental units. With only one replicate per treatment, the effect of therapy is confounded with the experimental unit or the effect of group on test anxiety. The reason why this is a problem is that any difference between the treatments could be due to any differences between the groups and not just the number of therapy hours. The same would be true if we only had one student per group. Why? Take a moment to think about this.\nConsider the first row of the data from Example 1. It looks like the student in group 2 scored the highest, followed by group 3 and then group 1. So does longer therapy sessions lead to higher test anxiety? Likely not! With only one student per treatment, we are not able to say that any differences in the response are due to the treatments. It could be due to any differences between the individuals. Maybe the student in group 3 tends to score higher on anxiety tests regardless of the treatment, or perhaps the student in group 1 was unusually calm that day. Without replication, these individual differences could mask (or mimic) the true effects of the treatments.\nBy replicating the treatments across multiple students, we can quantify these individual differences and gain a clearer picture of whether therapy duration truly impacts test anxiety. With five students per group, we might observe that group 1 consistently scores lower than group 3. This consistency would provide stronger evidence that the treatments, and not just individual variation, are responsible for the observed differences. So by replication, we can compare within treatment variation to variation between treatments.\nReplication ensures that the variation between two or more experimental units receiving the same treatment can be estimated and valid comparisons can be made between treatments. In other words, replication allows us to separate variation due to differences between treatments from variation within treatments.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The three R's of experimental design</span>"
    ]
  },
  {
    "objectID": "03_ExpDesign_RRR.html#replication",
    "href": "03_ExpDesign_RRR.html#replication",
    "title": "3  The three R’s of experimental design",
    "section": "",
    "text": "Treatment 1\nTreatment 2\nTreatment 3\n\n\n\n\n48\n55\n51\n\n\n50\n52\n52\n\n\n53\n53\n50\n\n\n52\n55\n53\n\n\n50\n53\n50",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The three R's of experimental design</span>"
    ]
  },
  {
    "objectID": "03_ExpDesign_RRR.html#randomisation",
    "href": "03_ExpDesign_RRR.html#randomisation",
    "title": "3  The three R’s of experimental design",
    "section": "Randomisation",
    "text": "Randomisation\nRandomisation refers to the process of randomly assigning treatments to experimental units such that each experimental unit has equal chance of receiving a specific treatment. Randomisation ensures that:\n\nThere is no bias on the part of the experimenter, either conscious or unconscious, when assigning treatments to experimental units.\nNo experimental unit is favored to receive a particular treatment.\nPossible differences between units are equally distributed among treatments. If there are clear differences between units, then blocking should be performed and randomisation occurs within blocks. We’ll talk more about this when we encounter Randomised Block Designs.\nWe can assume independence between observations.\n\nRandomisation is not haphazard. In statistics (and here in the context of experimental design), randomisation has a specific meaning: namely that each experimental unit has the same chance of being allocated any of the treatments. This can be done using random number generators such as with software packages, dice or drawing number from a hat (provided the number have been shuffled adequately and have equal chance to be picked).\nLet’s have a look at randomisation in R. Suppose we have 4 treatments (A, B, C, and D) and 32 experimental units. There are no differences between the units, so we don’t have to block, and we can equally split the units across the treatments, which means we have 8 units per treatment, i.e., 8 replicates. In R, we first create a long vector of 8 As, 8 Bs, 8 Cs, and 8 Ds called all.treat. Then shuffle the vector to obtain a randomisation using the function sample.\n\n# repeat the vector A, B, C, D 8 times \nall.treats &lt;- rep(c(\"A\",\"B\",\"C\",\"D\"), times = 8)\n\n# permutation of all.treats (sample withut replacement)\nrand1 &lt;- sample(all.treats)\n\n# example output\nrand1\n\n [1] \"A\" \"D\" \"A\" \"D\" \"C\" \"C\" \"D\" \"D\" \"B\" \"B\" \"D\" \"B\" \"B\" \"C\" \"A\" \"A\" \"A\" \"C\" \"A\"\n[20] \"D\" \"A\" \"D\" \"A\" \"B\" \"C\" \"B\" \"C\" \"C\" \"B\" \"B\" \"C\" \"D\"\n\n\nExperimental unit 1 recipes the first treatment that appears as the first element in the shuffled vector, experimental unit 2 receives the second and so on.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The three R's of experimental design</span>"
    ]
  },
  {
    "objectID": "03_ExpDesign_RRR.html#reduction-of-unexplained-variation-blocking",
    "href": "03_ExpDesign_RRR.html#reduction-of-unexplained-variation-blocking",
    "title": "3  The three R’s of experimental design",
    "section": "Reduction of Unexplained Variation (Blocking)",
    "text": "Reduction of Unexplained Variation (Blocking)\nUnexplained variation (or experimental error variance or within treatment variance) is largely due to inherent differences between experimental units. The larger this unexplained variation, the more difficult it becomes to detect treatment differences (a treatment signal). To minimise experimental error variance we can control extraneous factors (i.e. keeping all else constant) and by choosing homogeneous experimental units. Otherwise, we can block experimental units to reduce the variation.\nBlocking variables are nuisance factors that might affect your response or introduce systematic variation in the response and we are typically, not interested in these. Often, they are factors that cannot be randomised, e.g. biological sex of a person, time of day, location of a warehouse etc. We control the effect of such variables on the response by blocking for them so that we can investigate the possible effect of a variable that we are interested in. Usually, in a complete block experiment, there are as many experimental units per block as there are treatments, so that each treatment is applied once in every block. Treatments are randomized to the experimental units in the blocks. We can then compare the effects of treatments on similar experimental units, and we can estimate the variation induced in the response due to the differences between blocks. This variation due to blocks can then be removed from the unexplained variation.\nBlocking also offers the opportunity to test treatments over a wider range of conditions, e.g. if I only use people of one age in my experiment (say students) I cannot generalize my results to older people. However, if I use different age blocks I will be able to tell whether the treatments have similar effects in all age groups or not.\nLastly, if blocking is not feasible, randomization will ensure that at least treatments and nuisance factors are not confounded.\n\n“Block what you can, randomize what you cannot.”\n— Box, Hunter & Hunter (1978)",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The three R's of experimental design</span>"
    ]
  },
  {
    "objectID": "04_ExpDesign_DesigningExp.html",
    "href": "04_ExpDesign_DesigningExp.html",
    "title": "4  Designing an Experiment",
    "section": "",
    "text": "When planning an experiment we need to decide on:\n\ntreatment factors and their levels\nthe response\nexperimental material / units\nblocking factors\nnumber of replicates\n\nSome of these will be determined by the research question and how experimental units are assigned to treatments are determined by the design. The design that will be chosen for a particular experiment depends on the treatment structure (determined by the research question) and the blocking structure (determined by the available experimental units).\nHere are two ways the treatments can be structured:\n\nSingle factor: the treatments are the levels of a single treatment factor.\nFactorial: when more than one factor are of interest, then the experiment is said to be a factorial experiment. The treatments are constructed by crossing the treatment factors like we did in Figure 2.1 such that the treatments are all possible combinations of the treatment levels. For example, if factor A has \\(a\\) levels and factor B has \\(b\\) levels, there are \\(a \\times b\\) treatments. Such an experiment would then be called an \\(a \\times b\\) factorial experiment.\n\nThe blocking structure is determined by the set of experimental units chosen or available for the experiment. Are there any structures/differences that need to be blocked? Do I want to include experimental units of different types to make the results more general? How many experimental units are available in each block? For the block design covered in this course, the number of experimental units in each block corresponds to the number of treatments. This is called a complete block experiment and is the simplest block design. There are several other blocking structures, such as incomplete blocks and blocks with missing values, all with specific analysis which we will not cover here.\nIn this course, we cover two basic designs: Completely Randomized Designs (CRD) and Randomized Block Designs (RBD). For both designs, the treatment structure can be single or factorial. Where they differ is in terms of the experimental units and how randomization occurs.\nCompletely Randomized Designs (CRD)\nWhen all experimental units are fairly homogeneous, a CRD is used. Treatments are randomized to all experimental units.\nRandomized Block Design\nThis design is used when all experimental units are not homogeneous or blocking is required to control a nuisance factor. The treatments are randomized to the units within blocks.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Designing an Experiment</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html",
    "href": "05_CRD_Intro.html",
    "title": "5  Introduction",
    "section": "",
    "text": "5.1 Example: The effect of social media multitasking on classroom performance.\nCompletely Randomized Designs (CRDs) are the simplest experimental designs. They are used when experimental units are uniform enough and we expect them to react similar to a given treatment. In other words, we have no reason to suspect that a group of experimental units might react differently to the treatments. We also don’t expect any effects (besides possibly a treatment effect) to cause any systematic changes in the response. So, we don’t have to block for differing experimental units or any nuisance factors.\nRemember experimental design is the procedure for how experimental units are grouped and treatments are applied. We have already said that there are no blocks in CRDs. So randomisation occurs without restriction and to all experimental units. More generally, each of the \\(a\\) treatments are randomly assigned to \\(r\\) experimental units, such that each experimental unit is equally likely to receive any of the treatments. This means that there are \\(N = r \\times a\\) experimental units in total. We only consider designs that are balanced meaning that there an equal number of experimental units per treatment, i.e. a treatment is applied to \\(r\\) units. The experiment is then said to have \\(r\\) replicates.\nThe aim when analysing CRDs is to determine whether there is an effect of the treatment factor. We accomplish this by testing for differences in the treatment means (mean of response values in each treatment) through analyses different sources of variation in the response. This will become clear as we progress.\nAs a student, I used to believe I could multitask effectively. I would scroll through my phone during lectures, study while texting friends, or listen to podcast while driving. It felt like I was paying attention to everything, but in hindsight, I can barely recall the details of those podcasts. I often had to revisit lectures or restart study sessions because my focus wasn’t truly there. This tendency extends beyond student life. In the average workplace, tasks are frequently interrupted by social media, email checks, or notifications. Many of us feel the constant pull of our phones when trying to concentrate, whether we’re working, studying, or even relaxing.\nIn an era of perceived multitasking, where devices and distractions dominate our attention, it’s worth asking: Does social media multitasking impact academic performance of students?\nThe analysis of experimental data is determined by the design. This is the first thing we need to investigate. The design dictates the terms that we will include in our statistical model and so it is crucial to be able to identify the design and all factors included (blocking and treatment). It is also important to check that randomisation has been done correctly and determine the number of replicates used. In the previous chapter we started doing this by creating a summary of the design and we do the same here. From the description of the study, it is clear that:\nStudents were randomly assigned to one of the three groups, and performance was measured for each individual. Although this may seem obvious, they only took one measurement per student, so we don’t have to worry about pseudoreplication. This setup indicates that the students are both the experimental units and the observational units in this study. With a total of 120 experimental units and three treatments, the experiment has 40 replicates. Since only one treatment factor was investigated, and no blocking was performed, this is classified as a single-factor Completely Randomized Design (CRD). Here is the study breakdown:\nBefore we continue, now is the time to note that we won’t be using the real data collected in this experiment. It wasn’t available but I have simulated data to match their results. I’ve also made some other modifications such as the original study included 122 students but to ensure a balanced design I include only 120.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html#example-the-effect-of-social-media-multitasking-on-classroom-performance.",
    "href": "05_CRD_Intro.html#example-the-effect-of-social-media-multitasking-on-classroom-performance.",
    "title": "5  Introduction",
    "section": "",
    "text": "Example 5.1\n\n\n\nTwo researchers from Turkey, Demirbilek and Talan (2018), conducted a study to try and answer this question. Specifically, they examined the impact of social media multitasking during live lectures on students’ academic performance.\nA total of 120 first-year undergraduate students from the same Turkish University were randomly assigned to one of three groups:\n\nControl Group: Students used traditional pen-and-paper note-taking.\nExperimental Group 1 (Exp 1): Students engaged in SMS texting during the lecture.\nExperimental Group 2 (Exp 2): Students used Facebook during the lecture.\n\nOver a three-week period, participants attended the same lectures on Microsoft Excel. To measure academic performance, a standardised test was administered.\n\n\n\n\nResponse Variable: Academic performance, as measured by test scores.\nTreatment Factor: Level of social media multitasking.\nTreatment Levels (Groups): Control, Exp 1, and Exp 2.\n\n\n\nResponse Variable: Academic Performance\n\nTreatment Factor: Level of Social Media Multitasking\n\nTreatment Levels: Control, Experimental 1 (SMS), Experimental 2 (Facebook)\n\nTreatments: Control, Experiment 1, Experiment 2\n\nExperimental Unit: Student (120)\n\nObservational Unit: Student (120)\n\nReplicates: 40 students per group\n\nDesign Type: Single-Factor Completely Randomized Design (CRD)",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html#exploratory-data-analysis-eda",
    "href": "05_CRD_Intro.html#exploratory-data-analysis-eda",
    "title": "5  Introduction",
    "section": "5.2 Exploratory data analysis (EDA)",
    "text": "5.2 Exploratory data analysis (EDA)\nBefore we start any analyses, we have to conduct some exploratory data analysis to get a feel for our data. We start by checking whether it has been read in correctly and then look at some descriptive statistics.\nIn R, we read in the data set and then use some commands to inspect the data set:\n\nmultitask &lt;- read.csv(\"Datasets/multitask_performance.csv\")\nnrow(multitask) # check number of rows\n\n[1] 120\n\nhead(multitask) # check first 5 rows \n\n    Group Posttest\n1    Exp1 86.39427\n2    Exp1 64.19996\n3    Exp2 52.75394\n4 Control 67.81147\n5    Exp1 52.39911\n6    Exp1 56.58150\n\ntail(multitask) # check last 5 rows \n\n      Group Posttest\n115 Control 77.94344\n116 Control 63.58444\n117    Exp1 55.17758\n118    Exp2 67.16150\n119    Exp2 32.58373\n120    Exp2 49.58119\n\nsummary(multitask)\n\n    Group              Posttest    \n Length:120         Min.   :23.38  \n Class :character   1st Qu.:52.67  \n Mode  :character   Median :65.01  \n                    Mean   :63.59  \n                    3rd Qu.:76.32  \n                    Max.   :98.78  \n\n\nThe data set consists of 120 rows (each row representing a student) and two columns (Group and Posttest). The first column, Groups, contains the treatment the student was assigned and the Posttest column contains the response measure. Using the functions head and tail, we can look at the first and last 5 rows and the function summary provides us with a description of each column. We do this to check that R has read in our data correctly (you can view the whole data set by running view(multitask) as well). The summary tells us that the Group column is of the class “character”. For our analysis, we want it to be read as a factor:\n\nmultitask$Group &lt;- as.factor(multitask$Group)\nsummary(multitask)\n\n     Group       Posttest    \n Control:40   Min.   :23.38  \n Exp1   :40   1st Qu.:52.67  \n Exp2   :40   Median :65.01  \n              Mean   :63.59  \n              3rd Qu.:76.32  \n              Max.   :98.78  \n\n\nNow, we can see that there are 40 replicates per treatment group, confirming that the experiment is balanced. I have assumed that, based on the results shown, that the Posttest scores were recorded as percentages and using the summary we can quickly check whether there are any observations that are not on the appropriate scale or might be outliers. Looks good so far!",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html#checking-assumptions",
    "href": "05_CRD_Intro.html#checking-assumptions",
    "title": "5  Introduction",
    "section": "5.3 Checking assumptions",
    "text": "5.3 Checking assumptions\nDemirbilek and Talan (2018) had several research questions, but here we only consider the following:\nAre there any differences in mean academic performance between the three groups?\nYou might think that we could perform three t-tests (Control vs Exp 1, Control vs Exp 3, Exp 1 vs Exp 2). We could, but the problem with this approach is what we call multiple testing. When conducting many tests, there is an increased risk of making a Type 1 Error (rejecting the null hypothesis when it is in fact true) 1.\nWhen we have more than two groups, we can use a one-way analysis of variance (ANOVA) which can be seen as an extension of a \\(t\\)-test and is called “one-way” because there is a single factor being considered. In the next section, we will see that ANOVA is a linear model and some of the assumptions are about the model errors (just like regression):\n\nThere are no outliers.\nThe errors are independent.\nThe errors are normally distributed.\nAll groups have equal population variances.\n\nWe need to check the validity of these assumptions. There are both formal and informal techniques. Formal techniques (i.e. hypothesis tests) are not always appropriate for several reasons such as small data sets or that testing one assumption usually requires that the other two hold, complicating the order of tests. Informal techniques are more than sufficient and in this course, we stick with them.\n\nOutliers\nOutliers are unusual observations (response values) that deviate substantially from the remaining data points. They can have a large influence on the estimates of our model. Think of statistics such as means and variances, outlying observations will shift the mean towards them and distort the variability of the data.\nIf we’re lucky, outliers are artefacts of data recording or entering issues, such as a missing decimal points or incorrect scaling (called error outliers). These types of outliers can be corrected and the analysis can be done as usual. If, however, there are freak observations that are not clearly due to anything like data inputting, then they are likely genuine unusual responses (called interesting outliers) and should not be discarded. There are many ways of identifying and dealing with outliers (Aguinis, Gottfredson, and Joo (2013) found 29 different ways in the literature). Here, it is recommended that the analysis should be run with and without the outliers to see whether the conclusion depends on their inclusion. When dealing with outliers, it is best to be transparent and clear about how they were handled. Simply removing outliers with no explanation is questionable research practice.\nA good way to check for outliers, is to inspect the data visually with a box-plot of your data grouped by treatment.\n\nboxplot(Posttest ~ Group, data = multitask, col = c(\"skyblue\", \"lightgreen\", \"pink\"), \n        main = \"Posttest Scores by Group\", \n        xlab = \"Group\", \n        ylab = \"Posttest Scores\")\n\nstripchart(Posttest~Group, data = multitask, vertical = TRUE, add = TRUE, method = \"jitter\")\n\n\n\n\n\n\n\nFigure 5.1: Box-plots of Post treatment scores by group.\n\n\n\n\n\nThe first line of code plots the box-plot and by inputting Posttest~Groups as the first argument we are say plot the values of Posttest by Groups. There are extra graphical parameters specified to make the plot look a bit nicer. The function stripchart is used to overlay the data points. Based on these plots, there aren’t any obvious outlying observations.\n\n\nEqual population variance\nThe model assumes that population variances in different levels of the treatment factor are equal. That is, it is assumed in ANOVA that the variance of the response within each treatment is a separate estimate of the same population variance.\nSince we only have sample data, we would not expect that the sample variances to be exactly the same. If they are different it does not mean the assumption is not met. We expect them to differ a bit due to chance simply because we are sampling. Every time we sample from a population, the data set will be different and so will it’s variability. The sample variances need to be similar enough so that our assumption of equal population variance is reasonable.\nTo check this assumption, we can inspect the box-plots again and compare the heights. More specifically, we look at the interquartile ranges (IQR). From looking at the plot, the IQRs do not vary widely. If you prefer to look at the actual values, we can use R to obtain them:\n\nsort(tapply(multitask$Posttest,multitask$Group,IQR))\n\n Control     Exp2     Exp1 \n14.01068 20.94529 21.97001 \n\n\nAnother measure of variability we can look at, are the standard deviations (sd’s). With the same line of code but just replacing the function we want to apply, we obtain the sd of each group:\n\nsort(tapply(multitask$Posttest,multitask$Group,sd))\n\n Control     Exp1     Exp2 \n10.82887 14.60601 16.42678 \n\n\nThe rule of thumb is to use the ratio of the smallest to largest standard deviation and check whether it is smaller than five. In our case, the smallest sd (of the Control group) is about 1.5 times smaller than the largest sd (of the Exp 2 group) which is acceptable.\n\n\nNormally distributed errors\nWe can check this assumption by looking at the residuals after model fitting. A common misconception is to think that the response needs to be normally distributed. However, it is only the unexplained variation, i.e. the errors or residuals (estimates of errors), that we assume to be normally distributed. Of course, if the response has a clearly non-normal distribution (e.g. Binomial), then the residuals are likely to be non-normal as well. So, we can check our response values before hand for obvious deviation from normality, but we have to check this assumption again after fitting our model. Things to look for are asymmetric box-plots which indicate skew distributions. We also want to check that the data points tend to cluster around the median. In Figure 5.1, there are no signs of any clear deviation from normality. Other graphs we could look at are histograms or Quantile-Quantile (Q-Q) plots. Q-Q plots show the theoretical quantiles of the standard normal distribution against the actual quantiles of our data. We want our data to be as close to the xy line as possible (deviations in the tails are expected).\n\npar(mfrow = c(1,3))\n\n# First we subset the data for each group\ncontrol &lt;- multitask$Posttest[multitask$Group == \"Control\"]\nexp1 &lt;- multitask$Posttest[multitask$Group == \"Exp1\"]\nexp2 &lt;- multitask$Posttest[multitask$Group == \"Exp2\"]\n\n\nqqnorm(control, pty = 4, col =\"blue\", main = \"Control\")\nqqline(control, col = \"red\")\n\nqqnorm(exp1, pty = 4, col =\"blue\", main = \"Exp 1\")\nqqline(exp1, col = \"red\")\n\nqqnorm(exp2, pty = 4, col =\"blue\", main = \"Exp 2\")\nqqline(exp2, col = \"red\")\n\n\n\n\n\n\n\nFigure 5.2: Q-Q plots of response per treatment group.\n\n\n\n\n\nThe qqnorm function plots the theoretical quantiles on the x-axis and the sample quantile son the y-axis. So each point on the plot corresponds to a quantile from the sample plotted against the expected quantile from the standard normal distribution. As a reference we add a straight 45-degree line (in red) using the qqline function to indicate what perfect normality would look like.\n\n\nIndependent errors\nThe assumption is that the errors are independent. While we can check for certain types of dependence in the residuals after fitting the ANOVA (as we will see later), dependence among observations generally results in dependent residuals. Therefore, before fitting any models, we examine the observations and the experimental design to identify potential violations of independence.\nIn statistics, if one observation influences another in some way or another, they are said to be dependent. For the type of data considered here, there are two types of independence we require. Firstly, observations within treatments should be independent and second, observations between samples should be independent. Another way of saying this, is there should be independence within and among treatments. Depending on the direction of any violations, the within treatment variance or among treatment variance can either be deflated or inflated and treatment effects can be biased. This has considerable impact on the test statistic (F-ratio for ANOVA, more on this later) which could lead to misleading results. 2\nViolations of independence typically occur when the experimental units within or among treatments are connected in some way. Dependence within a sample can occurs when they are taken in a non-random sequence. Doing so typically allows some other variable to introduce dependence between successive observations. For example, measurement drift (when a tool’s reading gradually changes over time), physical effects (e.g. temperature) of the location of experimental units or the experimenter might become better (or worse) at taking the measurement as they move along. If these variables are not taken into account (by including them as factors in the model), it leads to a lack of independence in the errors of our model. Specifically, they lead to auto-correlated residuals; observations made closer together in time or space are more similar to each other than expected (this is what we check after model fitting).\nAn informal check we could do, is to plot the data in the order in which they were collected (if this information is available) whether that is temporally or spatially to see if any patterns emerge. To do this in R, we can create a Cleveland dot plot.\n\ndotchart(multitask$Posttest, ylab = \"Order of observation\", xlab =\"Post treatment test score\")\n\n\n\n\n\n\n\nFigure 5.3: Cleveland dot chart of response values in the order in which they appear in the data set.\n\n\n\n\n\nWe have assumed that the order in which the observations appear in the data set are the order in which they were recorded. If there were any factors that caused systematic trends, (i.e. dependence) in the observations, then there would be some kind of pattern in the dot chart. For our example, there is no clear pattern. After fitting the model, we can also plot the residuals against spatial coordinate or against order to check for obvious patterns. This method, however, only detects violations of independence if observations are related to time or space.\nDependence between treatments can occur if we apply the treatments to the same group of experimental units or if experimental units from different treatments are able to interact in some way during the experiment. These types of violations including those mentioned above, are ones that we can mostly prevent or control by properly designing the experiment. When we control for factors that might induce dependence, we can include them in our model.\nOther reasons for dependence may not be as obvious or easy to eliminate as we will see below. In the end, they may not have a strong impact on our estimates but it is important to carefully scrutinize your design and the system you are studying to identify possible sources of dependence so that these can be addressed and dealt with properly.\nIn our example, within and among group dependence could be caused by the students interacting or influencing each other in some way (by sharing notes for example). During the lectures, this can be controlled by careful monitoring and randomising their position in the lecture theater, but outside of lectures, it is less easy to control. Here we can argue that if students interacted outside of lectures the impact on their academic performance (as measured by the test) would likely be negligible. The integrity of the students is at play. It is not really possible to diagnose this type of dependence after the fact, only with careful design and implementation can these be avoided.\nIt is the onus of the experimenter to design and conduct experiments that ensure independence. With more thought (and if we’re lucky, funding) all well-designed experiments should lead to independent data. If violations are found after the fact, they cannot typically be corrected and then methods that deal specifically with dependent data (if appropriate) should be used3.\n\n\nA quick note on the robustness of ANOVA\nA statistical procedure is said to be robust to departures from a model assumption if the results remain unbiased even when the assumption is not met. The robustness of ANOVA is as follows:\n\nThe assumption of normality is not super crucial. Only severe departures from normality such as long-tailed distributions or skewed distributions when sample sizes are unequal and/or small are particularly problematic.\nIndependence within and among groups is extremely important. ANOVA does not handle dependent data and other analyses should be attempted if there is dependence.\nANOVA is relatively robust to violations of the equal variance assumption as long as there are no outliers, sample sizes are large and fairly equal (in the case of unbalanced designs which we do not cover here), and the sample variances are relatively equal.\nANOVA is not very resistant to severely outlying observations either.\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn this course, you will always encounter data that has already been collected and the description of the experiment will likely not be very exhaustive. You might be task then with thinking about how the assumption of independence could have been violated, but for the most part we will assume the data are independent, both within and among samples (unless otherwise stated or you are asked if the assumption holds).\nNo real data set ever meets all assumptions of a model perfectly. As the famous (at least in the world of statistics) quote by George Box goes: “All models are wrong but some are useful.” Judging whether a particular data set meets our assumptions reasonably well is therefore a bit of an art. You will likely read and hear that being able to identify violations comes from experience. The best way to get experience is to look at lots of data sets where you know how well they meet the assumptions. That’s best done via simulation. We therefore encourage you to use the attached R code to simulate data where various assumptions are violated. Run the code a number of times to get a feeling for how variable your actual sample can be even if the data generating mechanism doesn’t change. You may also want to play around with the sample sizes and you can change the degree to which the assumptions are violated to get a feeling for how these violations show up in the plots.\n\n R code",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html#summary",
    "href": "05_CRD_Intro.html#summary",
    "title": "5  Introduction",
    "section": "5.4 Summary",
    "text": "5.4 Summary\nCompletely Randomized Designs (CRDs) are the simplest experimental designs, used when experimental units are uniform and expected to react similarly to treatments. Since no nuisance factors are controlled, randomization occurs without restriction, and treatments are evenly assigned across experimental units (balanced design).\nThe social media multitasking study served as an example, where 120 students were randomly assigned to three groups (Control, SMS, Facebook) to measure their academic performance. This setup represents a single-factor CRD, where students are both the experimental and observational units with 40 replicates per group.\nBefore conducting ANOVA, we:\n\nChecked the data set for correct structure (120 observations, treatment groups as factors).\nInspected summary statistics and visualized distributions (box-plots, histograms, Q-Q plots).\n\nFor ANOVA, the following assumptions were examined:\n\nOutliers: Check via box-plots.\nEqual variance: Assess using interquartile ranges and ratio of sample standard deviations.\nNormality of errors: Verified using Q-Q plots.\nIndependence within and between treatment groups: Considered through study design.\n\nProper experimental design ensures valid conclusions. Identifying violations of assumptions early helps prevent biased results.\n\n\n\n\nAguinis, Herman, Ryan K Gottfredson, and Harry Joo. 2013. “Best-Practice Recommendations for Defining, Identifying, and Handling Outliers.” Organizational Research Methods 16 (2): 270–301.\n\n\nChen, Ashley, Suchita E Kumar, Rhea Varkhedi, and Dillon H Murphy. 2024. “The Effect of Playback Speed and Distractions on the Comprehension of Audio and Audio-Visual Materials.” Educational Psychology Review 36 (3): 79.\n\n\nDemirbilek, Muhammet, and Tarik Talan. 2018. “The Effect of Social Media Multitasking on Classroom Performance.” Active Learning in Higher Education 19 (2): 117–29.\n\n\nUnderwood, A. J. 1996. “Analysis of Variance.” In Experiments in Ecology: Their Logical Design and Interpretation Using Analysis of Variance, 140–97. Cambridge University Press.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html#footnotes",
    "href": "05_CRD_Intro.html#footnotes",
    "title": "5  Introduction",
    "section": "",
    "text": "Can’t remember what a \\(t\\)-test is and/or need a refresher on hypothesis testing? Have a look this video on t-tests and document for a brief reminder. Also, a quick (and cool) sidenote: This study by Chen et al. (2024) used a Completely Randomized Design (CRD), randomly assigning undergraduate students to playback speed groups (1x, 1.5x, 2x, and 2.5x) to measure the effect on comprehension of recorded lectures. Using ANOVA they found that comprehension was preserved up to 2x speed. I personally like to increase the playback speed to 1.5px if I just need to revise something quickly.↩︎\nUnderwood (1996) has a very detailed explanation of the independence assumption (and the others) in the context of ANOVA. The book is for ecological experiments, but much of it pertains to all types of experiments.↩︎\nA few of these methods are repeated measures ANOVA, mixed-models or hierarchical models.↩︎",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html",
    "href": "06_CRD_Model.html",
    "title": "6  A Simple Model for a CRD",
    "section": "",
    "text": "6.1 The model\nTo analyse data collected from a Completely Randomised Design we could use \\(t\\)-tests and compare the samples two at a time. This approach is problematic for two reasons. Firstly, the test statistic of a \\(t\\)-test is calculated with a standard deviation based only on the two samples it considers. We want our test statistic to consider the variability in all samples collected. Second, when we conduct multiple tests the overall Type 1 Error rate increases. That is, when doing many tests, the chance of making at least one wrong conclusion increases with the number of tests (if you want to know more see the box below). To avoid this, we will use the ANOVA method which was specifically developed for comparing multiple means.\nWhen we collect samples, we usually want to learn something about the populations from which they were drawn. To do this, we can develop a model for the observations that reflects the different sources of variation believed to be at play.\nFor Completely Randomised Designs, we have \\(a\\) treatments which implies \\(a\\) population means \\(\\mu_1, \\mu_2, \\mu_3, \\ldots, \\mu_a\\). We are interested in modelling the means of the treatments and the differences between them. Ultimately we want to test whether they are equal which we’ll get to in the next section. First, we construct a simple model for each observation \\(Y_{ij}\\):\n\\[\nY_{ij} = \\mu_{i} + e_{ij},\n\\]\nwhere\n\\[\n\\begin{aligned}\ni & = 1, \\dots, a \\quad (a = \\text{number of treatments}) \\\\\nj & = 1, \\dots, r \\quad (r = \\text{number of replicates}) \\\\\nY_{ij} & = \\text{observation of the } j^{th} \\text{ unit receiving treatment } i \\\\\n\\mu_i & = \\text{mean of treatment } i \\\\\ne_{ij} & = \\text{random error with } e_{ij} \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\nThat is, each observation is modeled as the sum of its population mean and some random variation, \\(e_{ij}\\). This random variation represents unexplained differences between individual observations within the same group and we assume that these differences follow a normal distribution with mean 0 and constant variance across all treatment groups. 1\nWe can change the notation slightly by arbitrarily dividing each mean into a sum of two components: the overall mean \\(\\mu\\) (the mean of the entire data set, which is the same as the mean of the \\(a\\) means2) and the difference between the population mean and the overall mean. In symbols, this translates to:\n\\[\n\\begin{aligned}\n\\mu_1 &= \\mu + (\\mu_1 - \\mu) \\\\\n\\mu_2 &= \\mu + (\\mu_2 - \\mu) \\\\\n&\\;\\;\\vdots \\notag \\\\\n\\mu_a &= \\mu + (\\mu_a - \\mu)\n\\end{aligned}\n\\]\nThe difference \\((\\mu_i - \\mu)\\) is the effect of treatment \\(i\\), denoted by \\(A_i\\). So each population mean is the sum of the overall mean and the part that we attribute to the particular treatment (\\(A_i\\)):\n\\[\n\\mu_i = \\mu + A_i, \\quad i = 1, 2, \\dots, a,\n\\]\nwhere \\(\\sum A_i = 0\\).\nReplacing \\(\\mu_i\\) in the model above leads to the common parameterisation of a single-factor ANOVA model3:\n\\[\nY_{ij} = \\mu + A_{i} + e_{ij}\n\\]\nwhere\n\\[\n\\begin{aligned}\ni & = 1, \\dots, a \\quad (a = \\text{number of treatments}) \\\\\nj & = 1, \\dots, r \\quad (r = \\text{number of replicates}) \\\\\nY_{ij} & = \\text{observation of the } j^{th} \\text{ unit receiving treatment } i \\\\\n\\mu & = \\text{overall or general mean} \\\\\nA_i & = \\text{effect of the } i^{th} \\text{ level of treatment factor A} \\\\\ne_{ij} & = \\text{random error with } e_{ij} \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\nThe model can be interpreted as follows:\nEach observation, \\(Y_{ij}\\), is the sum of the overall mean (\\(\\mu\\)), plus the effect of the treatment it belongs to (\\(A_i\\)), and some random error (\\(e_{ij}\\)). We use two subscripts on the \\(Y\\). One to identify the group (treatment) and the other to identify the subject (experimental unit) within the group:\n\\[\n\\begin{aligned}\nY_{1j} &= \\mu + A_1 + e_{1j} \\\\\nY_{2j} &= \\mu + A_2 + e_{2j} \\\\\nY_{3j} &= \\mu + A_3 + e_{3j} \\\\\n&\\;\\;\\vdots \\notag \\\\\nY_{aj} &= \\mu + A_a + e_{aj} \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#the-model",
    "href": "06_CRD_Model.html#the-model",
    "title": "6  A Simple Model for a CRD",
    "section": "",
    "text": "Why the \\(\\sum A_i = 0\\) constraint?\n\n\n\n\n\nThis constraint ensures that the treatment effects are expressed as deviations from the overall mean. To see why this holds, take the sum of both sides of the equation:\n\\[\n\\sum_{i=1}^{a} \\mu_i = \\sum_{i=1}^{a} (\\mu + A_i).\n\\]\nExpanding the right-hand side:\n\\[\n\\sum_{i=1}^{a} \\mu_i = a\\mu + \\sum_{i=1}^{a} A_i.\n\\]\nBy definition, the overall mean \\(\\mu\\) is the mean of the treatment means:\n\\[\n\\mu = \\frac{1}{a} \\sum_{i=1}^{a} \\mu_i.\n\\]\nMultiplying both sides by \\(a\\) gives:\n\\[\n\\sum_{i=1}^{a} \\mu_i = a\\mu.\n\\]\nComparing this with our earlier equation:\n\\[\na\\mu = a\\mu + \\sum_{i=1}^{a} A_i.\n\\]\nSubtracting \\(a\\mu\\) from both sides, we get:\n\\[\n\\sum_{i=1}^{a} A_i = 0.\n\\]\nThis constraint is standard in ANOVA models to ensure that the treatment effects are relative to the overall mean rather than being arbitrarily defined. It is not an additional assumption; any \\(a\\) means can be written in this way.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison to regression\n\n\n\n\n\nIf you wanted to, you could rewrite this with the regression notation you’ve encountered before as a regression model with a single categorical explanatory variable:\n\\[ Y_i = \\beta_0 + \\beta_1 T2_i + \\beta_2 T3_i + e_i \\]\nwhere \\(T2\\) and \\(T3\\) are indicator variables (i.e. \\(T2 = 1\\) if observation \\(i\\) is from treatment 2 and 0 otherwise). The intercept estimates the mean of the baseline category, here it is \\(T1\\).\nThese two models are equivalent. The data are exactly the same: in both situations we have \\(a\\) groups and we are interested in the mean response of these groups and the difference between them. The model notation is just slightly different. In the ANOVA model we use \\(\\mu\\) and \\(A_i\\) instead of \\(\\beta_0\\) and \\(\\beta_i\\) which have different meanings.\n\n\n\n\n\n\n\nRegression\nANOVA\n\n\n\n\n\\(\\beta_0\\) is the mean of the baseline category\n\\(\\mu\\) is the overall mean\n\n\n\\(\\beta_1\\) is the difference between the means of category 2 and the baseline category.\n\\(A_i\\) is the effect of treatment \\(i\\), i.e. change in mean response relative to the overall mean.\n\n\n\nWhen all the explanatory variables are categorical, which is mostly the case in comparative experimental data, it is more convenient to write the model in the ANOVA form, for two reasons:\n\nThe \\(A_i\\) notation is more concise, because we don’t have to add all the dummy variables. This makes it easier to read and understand because there is only one term per factor.\nMathematically it is more convenient. In this format all terms are deviations from a mean. This leads directly to sums of squares4 (squared deviations from a mean) and analysis of variance. We will see later that we can partition the total sum of squares into one part for every factor in the model. This allows us to investigate the variability in the response contributed by every model term (or factor).",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#estimation",
    "href": "06_CRD_Model.html#estimation",
    "title": "6  A Simple Model for a CRD",
    "section": "6.2 Estimation",
    "text": "6.2 Estimation\nOkay, so we have a model which we now need to fit to our data. When we do this, we estimate the model parameters using our data. The parameters we want to estimate are \\(\\mu\\) (the overall mean), the treatment effects (\\(A_i\\)) and \\(\\sigma^2\\) (the error variance). As for regression, we find least squares estimates for the parameters which minimise the residual or error sum of squares5:\n\\[ \\text{SSE} = \\sum_i\\sum_j e_{ij}^2 = \\sum_i\\sum_j (Y_{ij} - \\hat{Y}_{ij})^2 = \\sum_i\\sum_j (Y_{ij} - \\mu - A_i)^2\\]\nIt turns out when we solve for the estimates that minimise the SSE6, we obtain the following estimators:\n\\[\n\\begin{aligned}\n\\hat{\\mu} = \\bar{Y}_{..} \\\\\n\\hat{\\mu}_i = \\bar{Y}_{i.}\n\\end{aligned}\n\\]\nand\n\\[\\hat{A}_i =  \\bar{Y}_{i.} - \\bar{Y}_{..}\\]\nFrom linear model theory we know that the above are unbiased estimates7 of \\(\\mu\\) and the \\(A_i\\)’s. What does this tell you? It tells you that we can use the sample means as estimates for the true means. The estimated mean response for treatment \\(i\\) is the observed sample mean of treatment \\(i\\) and the observed overall mean is the estimated grand mean.\nFor the last parameter, the error variance, an unbiased estimator is found by dividing the minimised SSE (i.e. calculated with the least squares estimates) by its degrees of freedom:\n\\[ s^2 = \\frac{1}{N-a}\\sum_{ij}(Y_{ij} - \\bar{Y}_{i.})^2 \\]\nThis quantity is called the Mean Squares for Error (MSE) or residual mean square. It has \\((N-a)\\) degrees of freedom since we have \\(N\\) observations and have estimated \\(a\\) means. If you look at the formula you’ll notice that it is an average of the observed variability from the different treatment groups.\n\n\n\n\n\n\nCompare this with regression\n\n\n\n\n\nCompare this with the equations you saw in the regression section. Barring the extra subscript, the only difference is the equation for calculating the fitted/predicted value.\nIn regression, the fitted value is:\n\\[ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i \\]\nand here it is:\n\\[ \\hat{Y}_{ij} = \\bar{Y}_{i.} = \\hat{\\mu} + \\hat{A}_i \\]",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#in-context-of-the-social-media-multitasking-example",
    "href": "06_CRD_Model.html#in-context-of-the-social-media-multitasking-example",
    "title": "6  A Simple Model for a CRD",
    "section": "6.3 In context of the social media multitasking example",
    "text": "6.3 In context of the social media multitasking example\nLet’s take what we’ve learned so far and apply it to our example. We had \\(a = 3\\) treatments each with \\(r=40\\) replicates. The model equation is:\n\\[ Y_{ij} = \\mu + A_{i} + e_{ij}  \\]\nwhere\n\\[\n\\begin{aligned}\ni & = 1, \\dots, 3  \\\\\nj & = 1, \\dots, 40 \\\\\n\\end{aligned}\n\\]\nIf we write the model out for each treatment, we get:\n\\[\n\\begin{aligned}\nY_{Cj} &= \\mu + A_C + e_{Cj} \\\\\nY_{E1j} &= \\mu + A_{E1} + e_{E1j} \\\\\nY_{E2j} &= \\mu + A_{E2} + e_{E2j} \\\\\n\\end{aligned}\n\\]\nand when we fit the model to the data, the predicted means for the treatments are:\n\\[\n\\begin{aligned}\n\\hat{Y}_{C} &= \\hat{\\mu} + \\hat{A}_C = \\bar{Y}_{C.}\\\\\n\\hat{Y}_{E1} &= \\hat{\\mu} + \\hat{A}_{E1} = \\bar{Y}_{E1.}\\\\\n\\hat{Y}_{E2} &= \\hat{\\mu} + \\hat{A}_{E2} = \\bar{Y}_{E2.}\n\\end{aligned}\n\\]\nTo fit this model in R, we use the aov function and then use another function to extract the estimated parameters. By specifying type = “effects”, the function returns the \\(\\hat{A_i}\\)’s\n\nm1 &lt;- aov(Posttest~Group, data = multitask)\n\nmodel.tables(m1, type = \"effects\")\n\nTables of effects\n\n Group \nGroup\nControl    Exp1    Exp2 \n 12.049  -0.703 -11.345 \n\n\nThis tells us that the average score for students in the control group is roughly 12% higher than the overall average8. Both experimental groups performed worse, with students in the second group scoring, on average, about 11% less than the mean across all groups. We can also extract the overall mean and the treatment means by specifying type = “means”:\n\nmodel.tables(m1, type = \"means\")\n\nTables of means\nGrand mean\n         \n63.58527 \n\n Group \nGroup\nControl    Exp1    Exp2 \n  75.63   62.88   52.24 \n\n\nThe grand mean (i.e. average of all test scores) was 64% in this experiment. The control group scored on average 76% which is 12% higher than the overall mean and so on. So we have the estimates for the effects, grand mean and treatment means.\nThe last parameter we need to estimate is the error variance \\(\\sigma^2\\). Have a look at the formula again:\n\\[ s^2 = \\frac{1}{N-a}\\sum_{ij}(Y_{ij} - \\bar{Y}_{i.})^2 \\]\nIf we focus on the sum and break into sums of squares for each treatment \\(i\\), we get for the first treatment (let’s say that is the control group):\n\\[ \\sum_{j}(Y_{1j} - \\bar{Y}_{1.})^2 \\] Which is the sum of the squared differences between the observations in the control group and the mean score of the control group. We can easily calculated that in R:\n\ncontrol_scores &lt;- multitask$Posttest[multitask$Group == \"Control\"] # extract all scores for control group\nmean_control_scores &lt;- mean(control_scores) # calculate bar Y_1. - that is the mean score for control group\n\ncontrol_sum_squares &lt;- sum((control_scores - mean_control_scores)^2) # calculate sum of squares (not square of sum!)\n\nFirst, we subset the data set for the scores in the control group. Then we find the mean and calculate the squared differences, which is all summed together to give the sums of squares for treatment group 1. We can repeat this for the remaining treatments and sum the three sum of squares together and divide by \\(N-a\\) to get the MSE.\n\n# Expermiment 1 \nexp1_scores &lt;- multitask$Posttest[multitask$Group == \"Exp1\"]\nmean_exp1_scores &lt;- mean(exp1_scores)\nexp1_sum_squares &lt;- sum((exp1_scores - mean_exp1_scores)^2)\n\n# Expermiment 2 \nexp2_scores &lt;- multitask$Posttest[multitask$Group == \"Exp2\"]\nmean_exp2_scores &lt;- mean(exp2_scores)\nexp2_sum_squares &lt;- sum((exp2_scores - mean_exp2_scores)^2)\n\n# Total sums of sqaures\nsum_squares &lt;- sum(control_sum_squares + exp1_sum_squares + exp2_sum_squares)\n\nN &lt;- nrow(multitask) # number of observations overall\na &lt;- 3 # number of treatment groups \n\n\nsum_squares/(N-a) # MSE \n\n[1] 200.1463\n\n\nLater we will see that we can extract this quantity easily from the ANOVA table. But for now, this is a useful exercise to make sure you understand the formula. So, \\(\\hat{\\sigma^2} = s^2 = 200\\) (rounded off to the nearest integer) and \\(\\hat{\\sigma} = s = 14\\). This is the estimate of variance we will use to conduct an hypothesis to determine if there are any difference in the treatment means. Now you can see that it takes into account the variability of all our samples.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#standard-errors-and-confidence-intervals",
    "href": "06_CRD_Model.html#standard-errors-and-confidence-intervals",
    "title": "6  A Simple Model for a CRD",
    "section": "6.4 Standard errors and confidence intervals",
    "text": "6.4 Standard errors and confidence intervals\nIn the previous section we saw how the parameters of the ANOVA model are estimated. We also need a measure of uncertainty for each of these estimates (in the form of a standard error, variance, or confidence interval). Let’s start with the variance of a treatment mean estimate:\n\n\nVariance, Standard Deviation and Standard Error: what’s all this again? The variance (Var) is a good way of measuring variability. The Standard Deviation (SD) is the square root of the variance of a sample or population. The Standard Error (SE) is the SD of an estimate (read that again).\n\\[Var(\\hat{\\mu}_i) = \\frac{\\sigma^2}{n_i} \\]\nRemember that the sampling distribution of the mean is \\(N(\\mu,\\frac{\\sigma^2}{n})\\) and here we assumed that the groups have equal population variances.\nIf we assume that two treatment means are independent, the variance of the difference between two means is:\n\\[\nVar(\\hat{\\mu}_i - \\hat{\\mu}_j) = Var(\\hat{\\mu}_i) + Var(\\hat{\\mu}_j) = \\frac{\\sigma^2}{n_i} + \\frac{\\sigma^2}{n_j}\n\\]\nTo estimate these variances we substitute the MSE for \\(\\sigma^2\\) as it is an unbiased estimate of the error variance (the variability within each group). The standard errors of the estimates are found by taking the square root of the variances. The standard error is the standard deviation of an estimated quantity, and is a measure of its precision (uncertainty); how much it would vary in repeated sampling.\nWe can assume normal distributions for our estimates because we have assumed a normal linear model and because they are means (or differences between means). This means that confidence intervals for the population treatment means are of the form:\n\\[ \\text{estimate} \\pm t^{\\alpha/2}_v \\times \\text{SE}(\\text{estimate})\\]\nwhere \\(t^{\\alpha/2}_v\\) is the \\({\\alpha/2}^{th}\\) percentile of the Student’s \\(t\\) distribution with \\(v\\) degrees of freedom. The degrees of freedom are the error degrees of freedom, \\(N-a\\) for CRD.\nWhat are the standard errors associated with the parameter estimates in the social media example? We can easily extract this by specifying an extra argument to the model.tables function.\nStandard error of the effects:\n\nmodel.tables(m1, type = \"effects\", se = TRUE)\n\nTables of effects\n\n Group \nGroup\nControl    Exp1    Exp2 \n 12.049  -0.703 -11.345 \n\nStandard errors of effects\n        Group\n        2.237\nreplic.    40\n\n\nand for the treatment means:\n\nmodel.tables(m1, type = \"means\", se = TRUE)\n\nTables of means\nGrand mean\n         \n63.58527 \n\n Group \nGroup\nControl    Exp1    Exp2 \n  75.63   62.88   52.24 \n\nStandard errors for differences of means\n        Group\n        3.163\nreplic.    40\n\n\nSo, now we have parameter estimates and their standard errors. Equipped with these, we are closer to answering the original question: Does social media multitasking impact academic performance of students? Based on the model we fitted and the parameters we estimated, how do we test this? The answer is with an ANOVA table.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#summary",
    "href": "06_CRD_Model.html#summary",
    "title": "6  A Simple Model for a CRD",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nThis chapter introduces the Completely Randomized Design (CRD) model and explains why ANOVA is preferred over multiple t-tests, which inflate the Type 1 Error rate.\nIn ANOVA, each observation is modeled as:\n\\[\nY_{ij} = \\mu + A_{i} + e_{ij}\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(A_{i}\\) is the treatment effect (difference between treatment mean \\(\\mu_i\\) and the overall mean), and \\(e_{ij}\\) is random error which normally distributed with mean 0 and variance (\\(\\sigma^2\\)).\nParameters are estimated using least squares, with the mean squares error (MSE) providing an estimate of variance (\\(\\sigma^2\\)).\nApplying ANOVA to the social media multitasking study, we estimated treatment means and effects together with their standard errors, setting the stage for hypothesis testing using an ANOVA table.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#footnotes",
    "href": "06_CRD_Model.html#footnotes",
    "title": "6  A Simple Model for a CRD",
    "section": "",
    "text": "As opposed to non-constant variance across all treatment groups: \\(e_{ij} \\sim N(0, \\sigma^2_{i})\\) where the \\(\\sigma_i^2\\)’s are different.↩︎\n\\(\\mu = \\frac{\\sum\\mu_i}{a}\\)↩︎\nOften called Model I.↩︎\nIn statistics, sums of squares is a measure of variability and refers to squared deviations from a mean or expected value. For example, the residual sums of squares (sum of squared deviations of the observations from the fitted values).↩︎\nerror = observed - fitted.↩︎\nAnother name for this is the residual sums of squares (RSS).↩︎\nUnbiased means that the expected value of these statistics equals the parameter being estimated. In other words, the statistic equals the true parameter on average.↩︎\nRemember: \\(\\mu_i = \\mu + A_i\\)↩︎",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html",
    "href": "07_CRD_ANOVA.html",
    "title": "7  Analysis of Variance",
    "section": "",
    "text": "7.1 An Intuitive Explanation\nThe ANOVA model we have introduced is identical to a regression model with categorical variables, it is just parameterised differently. So why the different names and emphasis on variance - ANalysis Of VAriance? A well designed experiment allows us to estimate the within-treatment variability and between treatment variability. More specifically, it enables the partitioning of the total sum of squares into independent parts, one for each factor in the model (treatment and/or blocking factors). This allows us unambiguously to estimate the variability in the response contributed by each factor and the experimental error variance! We can then use this partitioning to perform hypothesis tests. In other words: by looking at the variation we can find out if the response differs due to the treatments.\nAn ANOVA applied to a single factor CRD is called a one-way ANOVA or between-subjects ANOVA or an independent factor ANOVA. It is a generalization of the ‘two-sample t-test assuming equal variances’ to the case of more than two populations.\nBefore we consider real data, we first want to look at a constructed example to explain the main ideas behind ANOVA. Assume that we carried out two experiments on plants removing nitrate (NO\\(_3\\)) from storm water. In both experiments, we consider three plant species (un-creatively called ‘A’, ‘B’, and ‘C’). In both experiments, we have three replicates per treatment. We are only interested in comparing the species so there is no control treatment. We obtained the following data:\nIf you look at these data sets carefully, you will see that each of the three species had the same mean in the two experiments. However, the measurements were much more variable in Experiment 2 than in Experiment 1.\nThe basic idea of ANOVA relies on the ratio of the among-treatment-means variation to the within-treatment variation. This is the F-ratio. The F-ratio can be thought of as a signal-to-noise ratio:",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#an-intuitive-explanation",
    "href": "07_CRD_ANOVA.html#an-intuitive-explanation",
    "title": "7  Analysis of Variance",
    "section": "",
    "text": "Table 7.1: Hypothetical Experiment\n\n\n\n\n\n\n\n(a) Experiment 1\n\n\n\n\n\nSpecies\nA\nB\nC\n\n\n\n\n\n40\n48\n58\n\n\n\n42\n50\n62\n\n\n\n38\n52\n60\n\n\nAverage\n40\n50\n60\n\n\n\n\n\n\n\n\n\n\n\n(b) Experiment 2\n\n\n\n\n\nSpecies\nA\nB\nC\n\n\n\n\n\n40\n65\n45\n\n\n\n25\n35\n75\n\n\n\n55\n50\n60\n\n\nAverage\n40\n50\n60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich experiment has better evidence that the true mean NO\\(_3\\) removal rate differs between species? Pause and think about this before reading on.\n\n\n\n\n\nIntuitively, we would say that Experiment 1 shows much stronger evidence for a true effect than Experiment 2. Why? Both experiments show the same differences among the treatment (species) means. So the variability in the treatment means is the same. However, the variability among the observations within treatments differs between the two experiments. In Experiment 1, the variability within treatments is much less than the variability among treatments. In Experiment 2, the variability within treatments is about the same as the variability among treatments.\n\n\n\n\n\nLarge ratios imply the signal (difference among the means) is large relative to the noise (variation within groups), providing evidence of a difference in the means.\nSmall ratios imply the signal (difference among the means) is small relative to the noise, indicating no evidence that the means differ.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#the-f-test",
    "href": "07_CRD_ANOVA.html#the-f-test",
    "title": "7  Analysis of Variance",
    "section": "7.2 The F-test",
    "text": "7.2 The F-test\nWhen we take the ratio of two variances, it can be shown that the ratio follows an F-distribution with degrees of freedom equal to those of the two variances.\nSo, for example, say we want to compare the variability between two independent groups, each with normally distributed observations. We define the test statistic as the ratio of the two sample variances:\n\\[\nF = \\frac{s_1^2}{s_2^2}\n\\]\nwhere \\(s_1^2\\) and \\(s_2^2\\) are the sample variances of the two groups. The resulting statistic follows an F-distribution with degrees of freedom:\n\n\\(df_1 = n_1 - 1\\) for the numerator (corresponding to variance \\(s_1^2\\))\n\\(df_2 = n_2 - 1\\) for the denominator (corresponding to variance \\(s_2^2\\))\n\nThe F-distribution is a probability distribution that arises frequently, particularly in ANOVA and regression analysis.\n\n\nCode\n# Define the range of F-values\nx &lt;- seq(0, 5, length.out = 500)\n\n# Define degrees of freedom pairs\ndf_pairs &lt;- list(\n  c(1, 10),\n  c(5, 10),\n  c(10, 10),\n  c(20, 20)\n)\n\n# Define colors for different lines\ncolors &lt;- c(\"red\", \"blue\", \"green\", \"purple\")\n\n# Create an empty plot\nplot(x, df(x, df_pairs[[1]][1], df_pairs[[1]][2]), type=\"n\",\n     xlab=\"F value\", ylab=\"Density\",\n     main=\"F-distribution for Varying Degrees of Freedom\")\n\n# Loop through df pairs and add lines\nfor (i in seq_along(df_pairs)) {\n  lines(x, df(x, df_pairs[[i]][1], df_pairs[[i]][2]), col=colors[i], lwd=2)\n}\n\n# Add a legend\nlegend(\"topright\", legend=paste(\"df1 =\", sapply(df_pairs, `[[`, 1), \", df2 =\", sapply(df_pairs, `[[`, 2)), \n       col=colors, lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\n\nKey properties of the F-distribution:\n\nIt is always non-negative: \\(F \\geq 0\\).\nIt is asymmetric and skewed to the right, particularly for small degrees of freedom.\nAs the degrees of freedom increase, the F-distribution approaches a normal shape.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#analysis-of-variance-for-crd",
    "href": "07_CRD_ANOVA.html#analysis-of-variance-for-crd",
    "title": "7  Analysis of Variance",
    "section": "7.3 Analysis of Variance for CRD",
    "text": "7.3 Analysis of Variance for CRD\nLet’s go back to the linear model for the single-factor CRD that we examined earlier:\n\\[\nY_{ij} = \\mu + A_i + e_{ij}\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(A_i\\) are the treatment effects (that is the difference between treatment means and the overall mean), and \\(e_{ij}\\) are the error terms (the differences between the observation and the fitted value, i.e. treatment mean). Remember that the estimated values for these parameters are the observed values:\n\\[\n\\begin{aligned}\n\\hat{\\mu} &= \\bar{Y}_{..} \\\\\n\\hat{A}_i &= \\bar{Y}_{i.} - \\bar{Y}_{..}\\\\\n\\hat{e}_{ij} &= Y_{ij} -  \\bar{Y}_{i.}\n\\end{aligned}\n\\]\nBy taking \\(\\mu\\) over to the left-hand-side in the equation, and substituting the above observed values we obtain:\n\\[\n\\begin{aligned}\nY_{ij} - \\mu &= (\\mu_i - \\mu) + (Y_{ij} - \\mu)\\\\\nY_{ij} - \\bar{Y} &= (\\bar{Y}_i - \\bar{Y}) + (Y_{ij} - \\bar{Y}_i) \\\\\n\\end{aligned}\n\\]\nSquaring and summing both sides gives the decomposition:\n\\[\n\\sum_i \\sum_j (Y_{ij} - \\bar{Y})^2 = \\sum_i \\sum_j (\\bar{Y}_i - \\bar{Y})^2 + \\sum_i \\sum_j (Y_{ij} - \\bar{Y}_i)^2\n\\]\nEach term represents squared deviations:\n\nThe first term is of observations around the overall mean representing the total variation in the response.\nThe second is of the group means around the overall mean representing the explained variation or variation between treatments and,\nThe last term represents the deviations of observations from their treatment means (unexplained or within treatment variation).\n\nWe could also call these:\n\\[\nSS_{\\text{total}} = SS_{\\text{between groups}} + SS_{\\text{within groups}}\n\\] or\n\\[\nSS_{\\text{total}} = SS_{\\text{treatment}} + SS_{\\text{error}}\n\\]\nThe analysis of variance is based on this identity1. The total sums of squares equals the sum of squares between groups plus the sum of squares within groups.\nBack to our constructed example. What are the different sums of squares? For Experiment 1, we get: \\(SS_{\\text{total}} = 624; SS_{\\text{between groups}} = 600; SS_{\\text{within groups}} = 24\\). Verify these numbers and do the same for Experiment 2.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#anova-table",
    "href": "07_CRD_ANOVA.html#anova-table",
    "title": "7  Analysis of Variance",
    "section": "7.4 ANOVA Table",
    "text": "7.4 ANOVA Table\nThis division of the total sums of squares is typically summarised in an analysis of variance table. The first column contains the “source” of the variability with the first entry (the order is not important, although this is the typical order) representing the between-treatment variability (explained variation), second is the error (unexplained variation, variation of experimental units within treatments) and lastly the total variation. Here we have used the notation \\(SS_A\\) to represent the sums of squares for treatment factor A. The second column gives the sums of squares of each source. The third column contains the degrees of freedom.\n\n\n\n\n\n\n\n\n\n\nSource\nSums of Squares (SS)\ndf\nMeans Squares (MS)\nF\n\n\n\n\nTreatment\n\\(\\sum_i n(\\bar{Y}_i - \\bar{Y})^2\\)\n\\(a-1\\)\n\\(MS_A = SS_A / (a-1)\\)\n\\(MS_A / MSE\\)\n\n\nResiduals (Error)\n\\(\\sum_i \\sum_j (Y_{ij} - \\bar{Y}_i)^2\\)\n\\(N-a\\)\n\\(MSE = SSE / (N-a)\\)\n\n\n\nTotal\n\\(\\sum_i \\sum_j (Y_{ij} - \\bar{Y})^2\\)\n\\(N-1\\)\n\n\n\n\n\nThe fourth column contains the Mean squares. This is what we get when we divide sums of squares by the appropriate degrees of freedom.\n\\[ \\text{MS} = \\frac{SS}{df}\\]\nThis is simply an average and may be seen as an estimate of variance. So when we divide the treatment SS by its degrees of freedom, we get an estimate of the variation due to treatments and similarly, for the the residual SS, we get an estimate of the error variance. You’ve seen this before!\n\\[\\text{MSE} = \\hat{\\sigma}^2 = \\frac{1}{N-a}\\sum_i\\sum_j(Y_{ij} - Y_{i.})^2\\]\n\n7.4.1 What Are Degrees of Freedom?\nDegrees of freedom (df) represent the number of independent pieces of information available for estimating a parameter. When making statistical calculations, we typically lose one degree of freedom for every estimated parameter before the current calculation.\nFor example, when estimating the standard deviation of a data set, we first estimate the mean, thereby reducing the number of independent observations available to calculate variability. This is why the denominator in the variance formula is \\(N-1\\):\n\\[ s^2 = \\frac{\\sum(Y_i - \\bar{Y})^2}{N -1} \\]\nYou can think of degrees of freedom as the number of independent deviations around a mean. If we have \\(n\\) observations and their mean, once we know \\(n-1\\) of the values, the last one is fixed—it must take on a specific value to satisfy the mean equation. Therefore, only \\(n-1\\) observations are truly free to vary.\nExample: Three Numbers Summing to a Fixed Mean\nSay we have three (\\(n=3\\)) numbers: (4, 6, 8). The mean of these three numbers is 6. If we only knew the first two numbers (4,6) and the mean, the third number must be 8:\n\\[\n\\begin{aligned}\n\\bar{x} &= \\frac{\\sum x_i}{n}\\\\\n6 &= \\frac{4+6+x_3}{3}\\\\\n18 &= 10 + x_3 \\\\\nx_3 &= 8\n\\end{aligned}\n\\]\nSince the third number is uniquely determined by the first two and the mean, we only have \\(n-1\\) (i.e., 2) degrees of freedom.\nAnother Intuitive Analogy\nImagine you are distributing a fixed amount of money among friends. If you have R100 and four friends, you can freely allocate money to three friends, but whatever is left must go to the fourth friend to ensure the total remains R100. Similarly, once the first \\(n-1\\) values are chosen, the last value is determined, limiting the degrees of freedom.\nIn ANOVA\nIf you look at the treatment sums of squares: \\(\\sum_i n_i (\\bar{Y}_{i.} - \\bar{Y}_{..})^2\\). We have \\(a\\) deviations around the grand mean. But once we know \\(a-1\\) of the treatment means and the grand mean2, the last mean is fixed. So we have \\(a-1\\) independent deviations around the overall mean.\nIf you look at the treatment sums of squares: \\(\\sum_i \\sum_j (Y_{ij} - \\bar{Y}_{..})^2\\). We are using \\(N\\) observations and calculating the deviations of these observations around the overall mean. So, only \\(N-1\\) observations are free to vary, the last observation is fixed for the calculated mean to hold true.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#back-to-the-constructed-example",
    "href": "07_CRD_ANOVA.html#back-to-the-constructed-example",
    "title": "7  Analysis of Variance",
    "section": "7.5 Back to the constructed example",
    "text": "7.5 Back to the constructed example\nWhat does the ANOVA table look like for our constructed example? You’ve already worked out the sums of squares. What are the df’s and Mean squares?\nLet’s have a look at Experiment 1 first.\n\n# Experiment 1 data \nexp1data &lt;- data.frame(species = rep(c(\"A\",\"B\",\"C\"), each = 3),\n                       response = c(40,42,38,48,50,52,58,62,60))\n\nexp1_anova &lt;- aov(response~species, data = exp1data)\nsummary(exp1_anova)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nspecies      2    600     300      75 5.69e-05 ***\nResiduals    6     24       4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd then Experiment 2:\n\n# Experiment 2 data \nexp2data &lt;- data.frame(species = rep(c(\"A\",\"B\",\"C\"), each = 3),\n                       response = c(40,25,55,65,35,50,45,75,60))\n\nexp2_anova &lt;- aov(response~species, data = exp2data)\nsummary(exp2_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nspecies      2    600     300   1.333  0.332\nResiduals    6   1350     225               \n\n\nSince the overall mean and the treatment means were the same in both experiment, we expected the \\(SS_{\\text{treatment}}\\) to be the same in both experiments. This was indeed the case – they are 600 in both experiments. The sample sizes were also the same in both experiments, so we would expect the df to be the same. With 9 observations, we have 8 df in total. Three treatments (Species) leads to 2 treatment df and 6 df remain for the residuals. The difference between the two experiments is that the observations were much more variable in Experiment 2 than in Experiment 1. Accordingly, we find that \\(SS_{\\text{error}}\\) was much larger in Experiment 2, and this led to larger MSE in Experiment 2. How does this affect the conclusions we draw from each of the experiments? This is where the F-ratio comes in.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#the-f-test-in-anova",
    "href": "07_CRD_ANOVA.html#the-f-test-in-anova",
    "title": "7  Analysis of Variance",
    "section": "7.6 The F-test in ANOVA",
    "text": "7.6 The F-test in ANOVA\nWe first set up the null and alternate hypothesis. The null hypothesis is that all treatments have the same mean, or equivalently, that all treatment effects are zero.\n\\[\n\\begin{aligned}\nH_0&: \\mu_1 = \\mu_2 = \\ldots = \\mu_a \\\\\nH_0&: A_1 = A_2 = \\ldots = A_a = 0\\\n\\end{aligned}\n\\]\nAnd the alternative hypothesis is the opposite of that:\n\\[\n\\begin{aligned}\nH_A&: \\text{At least one } \\mu_i \\text{ is different.} \\\\\nH_A&: \\text{At least one } A_i \\neq 0\n\\end{aligned}\n\\]\n\n\nRead that again. The alternative is that at least one treatment is different, there is a difference somewhere. It is not that all treatment means are different.\nIf \\(H_0\\) is true, the among-treatment-means variation should equal the within-treatment variation. We can use the F-ratio to test \\(H_0\\):\n\\[ F^* = \\frac{MS_A}{MSE} \\]\nThis ratio has an F-distribution with \\(a-1\\) numerator degrees of freedom and \\(N-a\\) denominator degrees of freedom.\nYou can think of the F-ratio as a signal-to-noise ratio. If \\(H_0\\) is true, \\(F\\) is expected to be close to 1. If \\(H_0\\) is false, \\(F\\) is expected to be much larger than 1. This means that the F-test we conduct is a one-sided upper tailed test. If \\(H_0\\) is false, the means squares for treatment will be much larger than the MSE, resulting in large F-values. We are only interested in this one side of possible outcomes therefore, a one-sided test.\nIn Experiment 1, \\(F = \\frac{300}{4} = 75\\), which leads to a very small \\(p\\)-value (\\(&lt; 0.001\\)). The signal was much larger than the noise, and our data are very unlikely if \\(H_0\\) were true. So we have good evidence that the treatments differ.\nIn Experiment 2, \\(F = \\frac{300}{225} = 1.33\\), which leads to a large \\(p\\)-value (\\(0.33\\)). Signal and noise were of similar magnitude, and our data are not unlikely if \\(H_0\\) were true. So we have no evidence against \\(H_0\\), i.e., no evidence that nitrate extraction differs between species.\nHow did we get these p-values? This is the same as in any hypothesis test. We have a test statistic and to say something about how likely this test statistic (or more extreme is) under the null hypothesis, we need the null distribution of the test statistic (that is the sampling distribution of the test statistic as if the null hypothesis were true). We then compared the observed value of the test statistic to that null distribution and asked ourselves how unusual it is in light of that distribution. Does our test statistic belong to this null distribution?\nThe \\(F\\) test statistic follows an F distribution as specified above.\n\\[\\text{F}^* \\sim \\text{F}_{(a-1),\\;(N-a)}\\]\nFor both experiment, this equates to an F distribution with 2 numerator and 6 denominator degrees of freedom which looks like this:\n\n\nCode\n# Define the range of F-values\nx &lt;- seq(0, 100, length.out = 500)\ny &lt;- df(x, df1 = 2, df2 = 6)\nplot(x, y, type=\"l\",\n     xlab=\"F value\", ylab=\"Density\",\n     main=\"\")\n\n\n\n\n\n\n\n\n\nWe can plot the test statistics on the graph as well and highlight the area under the curve to the right of each of these test statistics:\n\n\nCode\n# Define x values\nx &lt;- seq(0, 100, length.out = 500)\ny &lt;- df(x, df1 = 2, df2 = 6)\n\n# Define test_stats\ntest_stats &lt;- c(75, 1.33)\n\n# Plot the F-distribution density curve\nplot(x, y, type = \"l\", col = \"black\", lwd = 2,\n     xlab = \"F value\", ylab = \"Density\",\n     main = \"\")\n\n# Add vertical lines at test_stats\nabline(v = test_stats, col = \"red\", lty = 2, lwd = 2)\n\n# Shade the areas to the right of the test_stats\npolygon(c(test_stats[1], x[x &gt;= test_stats[1]], max(x)), \n        c(0, y[x &gt;= test_stats[1]], 0), col = rgb(0, 0, 1, 0.3), border = NA)\n\npolygon(c(test_stats[2], x[x &gt;= test_stats[2]], max(x)), \n        c(0, y[x &gt;= test_stats[2]], 0), col = rgb(1, 0, 0, 0.3), border = NA)\n\n# Add points at the critical values\npoints(test_stats, df(test_stats, df1 = 2, df2 = 6), pch = 19, col = \"black\")\n\n\n\n\n\n\n\n\n\nRemember sampling distributions are probability distributions. For continuous random variables, the area under the curve represents probability. Specifically, the probability of a random variable taking on a specific value or larger, is the area under the curve to the right of that value. For test statistics and their probability distribution, that probability is the p-value. The p-value is the probability of observing a test statistic at least as extreme as we did if the null hypothesis was in fact true. The smaller the p-value, the stronger the evidence against \\(H_0\\).\nWe can obtain the p-value in two ways (you will need to be able to do both):\n\nUsing Software.\n\nIn R, there are several built-in functions for certain probability distributions. These functions typically follow a naming convention:\n\nd&lt;dist&gt;() for density functions\np&lt;dist&gt;() for cumulative probability functions\nq&lt;dist&gt;() for quantile functions\nr&lt;dist&gt;() for random sampling\n\nFor example, when working with the F-distribution, we use:\n\ndf(x, df1, df2) for the probability density function (PDF)\npf(x, df1, df2) for the cumulative distribution function (CDF)\nqf(p, df1, df2) for quantiles\nrf(n, df1, df2) for random sampling\n\nTo obtain a p-value, we often use the cumulative probability functions (p&lt;dist&gt;()) with returns \\(Pr[X&lt;x]\\) so \\(Pr[X&gt;x] = 1 - Pr[X&lt;x]\\). Below is how to obtain the p-value for the second experiment:\n\nf_statistic &lt;- 1.33\ndf1 &lt;- 2  # Numerator degrees of freedom\ndf2 &lt;- 6  # Denominator degrees of freedom\n\n# Upper-tail probability (right-tailed test)\np_value &lt;- 1 - pf(f_statistic, df1, df2)\np_value\n\n[1] 0.332583\n\n\nThis value is quite large and corresponds to the area to the right of an F value of 1.33 for the distribution above. We interpret this p-value as the test statistic is quite likely to have come from this null distribution, there is a 33% chance of observing this test statistic or more extreme if the null hypothesis is true. We do not have strong evidence against the null hypothesis of equal means.\n\n\n\n\n\n\nCaution\n\n\n\nA large p-value does not mean that \\(H_0\\) is true!\n\nThe p-value is not the probability that the null hypothesis is true.\nThe p-value is not the probability that the alternative hypothesis is false.\nThe p-value is a statement about the relation of the data to the null hypothesis.\nThe p-value does not indicate the size or biological importance of the observed pattern.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can round the p-value if you need to enter the value to a certain number of decimals in a quiz or test using the function round.\n\n\n\nUsing tables.\n\nBefore the days of widespread programming, statisticians used tables to find critical values and p-values for various probability distributions. These tables were pre-computed for different significance levels (e.g., 0.05, 0.01) and degrees of freedom. In modern statistical analysis, we no longer rely on static tables, as software like R can compute exact probabilities. But since we have written examinations, we have to learn how to do this and it is a useful exercise to make sure you understand what you are doing and not just spitting out a value.\nF-tables look like this:\n\n\nRead it carefully. The table says: “Entries in the table give \\(F_\\alpha\\) values, where \\(\\alpha\\) is the area or probability in the upper tail of the F distribution. For example, with four numerator degrees of freedom, eight denominator degrees of freedom, and 0.05 area in the upper tail, F.05 = 3.84.” This is important, not all tables look like this. See if you can find the F-value mentioned.\nThe numerator df is in the column and the denominator df is in the row. In the row dimension are different \\(\\alpha\\) values as well. To find an F-value, locate the df in the column and row. Can you find the following:\n\n\\(F_{4,4}^{0.05} = 6.39\\)\n\\(F_{10,2}^{0.1} = 9.39\\)\n\\(F_{1,1}^{0.025} = 647.79\\)\n\\(F_{7,3}^{0.01} = 27.67\\)\n\nThis is how we find critical values of F-distributions. If you are asked to compare a test statistic with a critical value at a specific significance level, you will find the value with a table like this. To find the critical values in R, we use the fq function:\n\n# F4,4 0.05\n\nqf(p = 0.05, df1 = 4, df2 = 4, lower.tail = FALSE) # if lower.tail = TRUE which is the default, the critical value with probability 0.05 to the left would be return. \n\n[1] 6.388233\n\n# F10,2 0.11\nqf(p = 0.1,   df1 = 10, df2 = 2, lower.tail = FALSE)\n\n[1] 9.391573\n\n# F1,1 0.025\nqf(p = 0.025, df1 = 1,  df2 = 1, lower.tail = FALSE)\n\n[1] 647.789\n\n# F7,3 0.01\nqf(p = 0.01,  df1 = 7,  df2 = 3, lower.tail = FALSE)\n\n[1] 27.6717\n\n\nNow, how do we use the tables to obtain p-values? The test statistic for the first Experiment was 1.33 and the df’s were 2 (num) and 6 (denom). If we look at the table above, it only goes to 4 denominator degrees of freedom, so we need the continuation of the table.\nNow we locate the F-values with 2 and 6 degrees of freedom and compare the test statistic of the second experiment (1.33) to them. The smallest value is 3.46 where the probability to the right of that value is 0.1. Our test statistic is much smaller than this, so lies further to the right and so logically, the right-hand-side probability of this value with be greater than 0.1. So we conclude that the p-value that our p-value is &gt; 0.1 (which it is, we calculated it to be 0.32). With tables we cannot get exact probabilities, but we can say something about the magnitude of the p-value. Try it for the first experiment which had an F-value of 75.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#conclusion-does-social-media-multitasking-impact-academic-performance-of-students",
    "href": "07_CRD_ANOVA.html#conclusion-does-social-media-multitasking-impact-academic-performance-of-students",
    "title": "7  Analysis of Variance",
    "section": "7.7 Conclusion: Does social media multitasking impact academic performance of students?",
    "text": "7.7 Conclusion: Does social media multitasking impact academic performance of students?\nLet’s revisit the real experiment we started this section with. I repeat the experiment description below.\n\n\n\n\n\n\nExample 5.1\n\n\n\nTwo researchers from Turkey, Demirbilek and Talan (2018), conducted a study to try and answer this question. Specifically, they examined the impact of social media multitasking during live lectures on students’ academic performance.\nA total of 120 undergraduate students were randomly assigned to one of three groups:\n\nControl Group: Students used traditional pen-and-paper note-taking.\nExperimental Group 1 (Exp 1): Students engaged in SMS texting during the lecture.\nExperimental Group 2 (Exp 2): Students used Facebook during the lecture.\n\nOver a three-week period, participants attended the same lectures on Microsoft Excel. To measure academic performance, a standardised test was administered.\n\n\nIn the previous sections we introduced this study, checked the model assumptions and obtained estimates of the model parameters. Now equipped with that information and all that you have learnt, we are ready to fit to conduct the ANOVA hypothesis test to finally answer our question:\nDoes social media multitasking impact academic performance of students?\nWe start with the hypotheses:\n\\[H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_a\\]\nIn words we say that the average academic performance of students did not differ across the treatments (levels of social media multitasking).\nAnd the alternative hypothesis is the opposite of that:\n\\[H_A: \\text{At least one } \\mu_i \\text{ is different.}\\]\nAt least one of the social media multitasking treatments resulted in a different mean academic performance, they are not all equal.\nWe have fit the model already (called m1) and call the summary function to obtain the ANOVA table:\n\n# m1 &lt;- aov(Posttest ~ Group, data = multitask)\n\nsummary(m1)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup         2  10975    5488   27.42 1.72e-10 ***\nResiduals   117  23417     200                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nViolà! We have our ANOVA table. Inspect the results and make sure you understand how each value is obtained and what they represent. By just looking at the table, you should be able to answer the following questions:\n\nHow many treatments were there?\nHow many observations in total?\nIs there evidence for a treatment effect?\n\nThe first two you can answer with the degrees of freedom and the third is answered by conducting the hypothesis test. With three treatments, we have 2 treatment degrees of freedom. We had 40 students per group, the sample size is then 120 which means there are 117 degrees of freedom for the residuals. The treatment MS (5488) was much larger that the MSE (200). This leads to an F-ratio of 27.42 with a p-value of \\(1.72\\times e^{-10}\\) (that’s extremely small). We have strong evidence that the treatments did result different academic performances across students At least one treatment resulted in a different mean academic performance. In a report, you would write:\n“The manipulation of social media multitasking affected the academic performance of students in this experiment (\\(F_{2,117} = 27.42\\), \\(p=1.72\\times e^{-10}\\)).”\nBut which treatments differed? We cannot answer that question with this hypothesis. It only tells us that there is a difference, there is a treatment effect. It does not tell us where the difference or possible differences lie. To determine this, we need to use treatment contrasts. Before we do this or present any results, we need to do one last thing.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#model-checking",
    "href": "07_CRD_ANOVA.html#model-checking",
    "title": "7  Analysis of Variance",
    "section": "7.8 Model Checking",
    "text": "7.8 Model Checking\nRemember that we said some of our assumptions need to be checked after the model is fitted. Our model specifies the error terms are (1) normally distributed, (2) all with the same variance (homoscedastic), and (3) that they are independent. The residuals are estimates of these error terms and we can therefore use them to check the model assumptions. Normally distributed, equal variance and independent really means that there is no discernible pattern or structure left in the residuals. If there is, then the model has failed to pick up an important structure in the data.3\nWe call the function plot on our model object. For our purposes we are only going to look at two of the plots and we inspect them one by one by specifying the plot number with the argument which:\n\nplot(m1, which = 1)\n\n\n\n\n\n\n\n\nThis is a plot of the residuals (obs - fitted) against the fitted values and we are hoping to see no patterns. We have three lines, one for each treatment group and we want to check that our residuals are centered around zero and have constant variance across the groups. 4\n\nplot(m1, which = 2)\n\n\n\n\n\n\n\n\nThe second plot is a Q-Q plot which we have seen before when we checked the assumption of normality before model fitting. Now, we plot the standardised residuals against the theoretical quantiles of a standard normal distribution. We are looking for the same pattern as before, that the points fall close to the dotted line. As usual, there many be some deviations at the tails but for the most part, there are no serious problems with this plot. If there is some doubt, we can also look at a histogram of the residuals:\n\nhist(resid(m1))\n\n\n\n\n\n\n\n\nThe assumption of independent errors is mostly checked before model fitting and by consideration of the experimental design. If we suspected auto-correlated residuals, we could plot the residuals against order:\n\nplot(resid(m1) ~ seq_along(resid(m1)), \n     xlab = \"Order of Observations\", \n     ylab = \"Residuals\", \n     main = \"Residuals vs. Order\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\n\nThere are no patterns at all, the residuals appear randomly distributed. So no indications of dependence.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#summary",
    "href": "07_CRD_ANOVA.html#summary",
    "title": "7  Analysis of Variance",
    "section": "7.9 Summary",
    "text": "7.9 Summary\nThat’s a lot. So let’s summarise what we did in this chapter:\nWe introduced Analysis of Variance (ANOVA), which is fundamentally the same as a regression model with categorical variables but parameterised differently. ANOVA allows us to partition total variance into between-treatment and within-treatment variability, helping us determine whether observed differences in the response variable are due to the treatments and not just sampling error.\nWe explored ANOVA through a constructed experiment on nitrate removal by plants, demonstrating that variation within treatments influences our ability to detect true treatment effects. The F-ratio, a measure of the signal-to-noise ratio, is central to ANOVA. A large F-ratio suggests that between-group variability is greater than within-group variability, providing evidence that at least one treatment differs.\nThe F-test determines statistical significance, and its p-value is derived from the F-distribution. A small p-value suggests strong evidence against the null hypothesis (\\(H_0\\)), indicating at least one group mean differs. The ANOVA table summarises the calculations of the hypothesis test, including sums of squares (SS), degrees of freedom (df), mean squares (MS), and the F-statistic.\nApplying ANOVA to real experimental data, we analysed the impact of social media multitasking on student performance. With three treatment groups (control, SMS, Facebook), we found a statistically significant effect (\\(F_{2,117} = 27.42\\), \\(p=1.72\\times10^{-10}\\)), confirming that at least one treatment influenced academic performance. However, ANOVA does not specify which groups differ and how they differ — this requires post-hoc tests.\nFinally, we validated model assumptions:\n\nNormality: Checked via a Q-Q plot and histogram of residuals.\nHomoscedasticity (equal variance): Examined using a residuals vs. fitted plot.\nIndependence: Considered in the experimental design and checked by plotting residuals against observation order.\n\n\n\n\n\nDemirbilek, Muhammet, and Tarik Talan. 2018. “The Effect of Social Media Multitasking on Classroom Performance.” Active Learning in Higher Education 19 (2): 117–29.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#footnotes",
    "href": "07_CRD_ANOVA.html#footnotes",
    "title": "7  Analysis of Variance",
    "section": "",
    "text": "In mathematics, an identity is an equation that is always true, regardless of the values of it’s variables. In other words, the identity is true for all observations.↩︎\nRemember, \\(\\mu = \\frac{\\sum \\mu_i}{a}\\).↩︎\nThe same concepts apply to linear regression models.↩︎\nRemember we assumed \\(e_{ij} \\sim N(0,\\sigma^2)\\) and residuals are estimates of the errors.↩︎",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "08_CRD_Contrasts.html",
    "href": "08_CRD_Contrasts.html",
    "title": "7  Contrasts",
    "section": "",
    "text": "Contrasting pairs of treatment means\nThe aim in many experiments is to compare treatments. To do this we contrast one group of means with another, i.e. we compare means, or groups of means, to see if treatments differ, and by how much they differ. A comparison of treatments (or groups of treatments) is called a contrast. If the experiment has been conducted as a result of specific research hypotheses, these will already define the contrasts we should construct first.\nFor a single-factor CRD with only two treatments, we could conduct a t-test to compare the two means or construct a confidence interval to estimate the difference. But we know that with more than two treatments, we encounter problems of multiple testing. How do we contrast treatments when we have a factor with more than two levels?\nWe wrote the ANOVA model as:\n\\[Y_{ij} = \\mu + A_i + e_{ij}\\]\nwith overall mean and the treatment effects as the parameters (as well as the error variance). Because the effects are constrained to sum to zero, i.e. \\(\\sum_i^a A_i = 0\\) we call this ANOVA model the sum-to-zero parameterisation.\nThe above parameterisation is useful for constructing ANOVA tables. For estimating differences between treatments, however, a different parameterisation is more useful:\n\\[Y_{ij} = A_i + e_{ij}\\] In this version, we no longer have the overall mean as a parameter but use only the treatment effects \\(A_i\\). Remember that any model ultimately needs to describe the treatment means. There are a number of different ways in which to do this. One is the so-called treatment contrast parameterisation, which R uses as default for regression models. In this parameterisation, \\(A_1\\) estimates the mean of the baseline treatment (by default, R orders the treatments alphabetically and takes the first one as baseline). The other parameters then estimate the difference between each treatment and the baseline treatment: \\(A_2\\) estimates the difference between the second and the first treatment, \\(A_3\\) estimates the difference between the third and the first, etc.\nTo get a better understanding of this, let’s fit the model to the social media data with this parameterisation. In R, this is done by using the lm function.\nm1.tc &lt;- lm(Posttest ~ Group, data = multitask)\nsummary(m1.tc)\n\n\nCall:\nlm(formula = Posttest ~ Group, data = multitask)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.964 -10.175   0.583   8.550  37.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   75.634      2.237  33.812  &lt; 2e-16 ***\nGroupExp1    -12.752      3.163  -4.031 9.92e-05 ***\nGroupExp2    -23.394      3.163  -7.395 2.32e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.15 on 117 degrees of freedom\nMultiple R-squared:  0.3191,    Adjusted R-squared:  0.3075 \nF-statistic: 27.42 on 2 and 117 DF,  p-value: 1.717e-10\nThis is exactly the same output as you have seen before in the regression section! The intercept measures the mean of the baseline treatment (here it is the Control group). The next estimate GroupExp1 is the difference between the mean of Experiment 1 and the mean of the Control group. Similarly, the last one is the difference between the mean of the Control Group and that of Experiment 2. You can verify this by using the mean estimates we obtain when we fit the model previously:\nmodel.tables(m1, type = \"means\")\n\nTables of means\nGrand mean\n         \n63.58527 \n\n Group \nGroup\nControl    Exp1    Exp2 \n  75.63   62.88   52.24 \n\n62.88 - 75.63 #GroupExp1\n\n[1] -12.75\n\n52.24 - 75.63 #GroupExp2 \n\n[1] -23.39\nWhy is this useful? Now, we can formally test whether these differences are statistically significant using a hypothesis test!\nThink back to regression—what was the null hypothesis for the coefficients in the output?\nIt was:\n\\[\\beta_i = 0\\].\nThe same principle applies here. We test whether the treatment effects (\\(A_i\\)) are equal to zero:\n\\[H_0: A_i = 0\\]\nSince we are interested in testing differences between groups, and the control group serves as the baseline, we are specifically testing:\n\\[\n\\begin{aligned}\nH_0: A_2 &= 0 \\\\\nH_0: A_3 &= 0\n\\end{aligned}\n\\]\nThis is the test that R conducts in the output above. It tests, for the last two parameters, the hypothesis that the difference between Experiment 1 and the Control is zero an that the difference between Experiment 2 and the Control is zero. In both cases, the p-values are extremely small which suggest that there are differences (the effects are not equal to zero).\nWhat about the intercept? This is testing that the mean of the Control group is zero. So, it tests whether the students in the control group scored zero on average. This doesn’t really make sense and it is not a useful test. So not all tests that R carries out are necessarily useful or informative! Very often testing whether the intercept is different from zero is not interesting.\nWhat if we aren’t interested in the contrast R perform by default? We wanted to know whether there is a difference between the other two groups? We simply need to change the baseline treatment that R uses and we can do this easily using the relevel command:\nm1.tc &lt;- lm(Posttest ~ relevel(Group, ref =\"Exp1\"), data = multitask)\nsummary(m1.tc)\n\n\nCall:\nlm(formula = Posttest ~ relevel(Group, ref = \"Exp1\"), data = multitask)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.964 -10.175   0.583   8.550  37.408 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           62.882      2.237  28.111  &lt; 2e-16 ***\nrelevel(Group, ref = \"Exp1\")Control   12.752      3.163   4.031 9.92e-05 ***\nrelevel(Group, ref = \"Exp1\")Exp2     -10.642      3.163  -3.364  0.00104 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.15 on 117 degrees of freedom\nMultiple R-squared:  0.3191,    Adjusted R-squared:  0.3075 \nF-statistic: 27.42 on 2 and 117 DF,  p-value: 1.717e-10\nNotice that the residual standard error, F-value and other statistics at the end of the output are exactly the same as for Model m1.tc above. The two models are equivalent and provide the same fit to the data. The only difference is that the parameters have different interpretations.\nTo conclude this section, we present the final results of the social media multitasking experiment. The ANOVA revealed a significant treatment effect on academic performance (\\(F = 27.42\\), \\(p = 1.72\\times e^{-10}\\)). Specifically, students in both experimental conditions performed worse than those in the control group. On average, students in Experiment 1 scored 12% lower (\\(t = -4.031\\), \\(p = 9.92 \\times 10^{-5}\\)), while those in Experiment 2 scored 24% lower (\\(t = -7.395\\), \\(p = 2.32 \\times 10^{-11}\\)), with a standard error of 3.163. Students in Experiment 2 scored on average 10% less than those in Experiment 1 (\\(t=-3.364\\), \\(p = 0.001\\)). This confirms that multitasking with social media during lectures negatively impacted academic performance in this experiment.\nI think the message is clear, going on social media during lectures is probably not going to help you learn. In general reducing the time you spend on social media will probably help you. You certainly don’t have to delete all social media apps, but taking intentional breaks and trying to give your full attention when it is required, will certainly make a difference. Here are some videos that have motivated me to improve my focu and decrease my time spent on social media!",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrasts</span>"
    ]
  },
  {
    "objectID": "08_CRD_Contrasts.html#contrasting-pairs-of-treatment-means",
    "href": "08_CRD_Contrasts.html#contrasting-pairs-of-treatment-means",
    "title": "7  Contrasts",
    "section": "",
    "text": "Construction of treatmenat means under the treatment contrast parameterisation:\n\\[\n\\begin{aligned}\n\\mu_1 &= A_1 \\\\\n\\mu_2 &= A_1 + A_2 \\\\\n\\vdots& \\\\\n\\mu_a &= A_1 + A_a\n\\end{aligned}\n\\]\nand under the sum-to-zero parameterisation:\n\\[\n\\begin{aligned}\n\\mu_1 &= \\mu + A_1 \\\\\n\\mu_2 &= \\mu + A_2 \\\\\n\\vdots& \\\\\n\\mu_a &= \\mu + A_a\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we can’t focus https://www.youtube.com/watch?v=6QltxZ-vPMc\nQuit social media https://www.youtube.com/watch?v=3E7hkPZ-HTk",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Contrasts</span>"
    ]
  },
  {
    "objectID": "09_RCBD_Intro.html",
    "href": "09_RCBD_Intro.html",
    "title": "8  Introduction",
    "section": "",
    "text": "8.1 Treatments vs. Blocks\nSo far, we have examined completely randomised designs where randomisation of experimental units to treatments was completely unrestricted. With complete randomisation, all other variables (the environment that we can never control completely) that might affect the response are, on average, equal in all treatment groups. This allows us to be confident that differences in group means are due to the treatments.\nHowever, there is often important variation in additional variables that we are not directly interested in. If we can group our experimental units with respect to these variables to make them more similar, we achieve a more powerful design. This is the idea of blocking.\nIf blocks are used effectively, we can separate variability due to treatments, blocks, and errors, reducing unexplained variability. That is, variability between blocks can be estimated and removed from the residual error. Essentially, we compare treatments over more similar experimental units than in a completely randomised design. With reduced error variance, our test becomes more powerful.\nBlocking is also useful when we want to demonstrate that treatment differences hold over a wider range of conditions. For example, in the social media multitasking example, the experiment was conducted on first year students. Strictly speaking, the results then only apply to first year students and extrapolation to students in different years of their degree is limited. Alternatively, we could choose students from first, second and third year (for example) and apply one replicate of each treatment within year. In this case, year of study would be the blocking factor.\nMore generally, we often want to show that our results hold for different species, age groups, or biological sexes. In such cases, we could use species, age, or sex as blocks. While blocks are typically used to control for variation in variables we are not directly interested in, sometimes these factors may also be of interest in their own right.\nWhen is a factor a treatment, and when is it a block?\nA good way to distinguish between them is by asking whether we can manipulate the factor and randomly assign experimental units to its levels.\nAlthough we can always estimate differences between blocks, we need to be much more cautious when inferring causality from block-level differences or from any factor that we cannot randomise (as is the case in observational studies).\nSuppose we study whether different teaching methods (interactive vs. traditional) affect student performance, conducted in public and private schools.\nIf private school students perform better, we cannot conclude school type caused the difference due to potential confounders such as socioeconomic background or teacher quality.\nEven though we control for school type, observed differences may be due to these external factors, not just the school itself. We cannot be sure that the observed differences are really only due to school type.\nSometimes, however, blocking variables can also be randomised. Suppose a study is testing two medications (A vs. B) for blood pressure, experiments are conducted in two labs (Lab 1 & Lab 2).\nPatients could have been randomly assigned to labs, but if logistical constraints prevent this, lab is used as a block. Since we only care about medication effects, lab differences are treated as a nuisance variable. The real difference is interest. We are not interested block effects on the response, only treatment effects. Blocking factors are used to control for known sources of variation that might obscure the treatment effect.",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "09_RCBD_Intro.html#treatments-vs.-blocks",
    "href": "09_RCBD_Intro.html#treatments-vs.-blocks",
    "title": "8  Introduction",
    "section": "",
    "text": "We generally cannot manipulate the age or sex of an individual, but we can manipulate, for example, the food they receive. So, age and sex are blocking factors, whereas food type is a treatment.\nWe can manipulate the level of social media multitasking, but we cannot manipulate the year of study of students. So, level of social media multitasking is a treatment, while year of study is a block.\n\n\n\n\n\n\n\n\nExample of observational study\n\n\n\n\n\nSuppose we are studying whether different music streaming platforms (e.g., Spotify, Apple Music, YouTube Music) influence a song’s popularity. We cannot randomly assign a song to a particular streaming platform because artists typically release their music on multiple platforms simultaneously. However, platform choice is still the main factor of interest.\nWe would analyze differences in song popularity (e.g., number of streams, chart position) across platforms as we would for any treatment factor. However, we must be cautious when attributing differences solely to the platform itself because other factors could also play a role. For instance:\n\nArtist popularity: A well-known artist might naturally attract more streams, regardless of the platform.\nMarketing strategies: Some platforms might promote certain songs more aggressively.\nRelease timing: Songs released during peak listening hours or days may perform better.\nPlatform demographics: Different platforms cater to different audiences, which might influence engagement.\n\nSince we cannot randomly assign songs to platforms, we cannot be certain that observed differences in popularity are only due to the platform. Instead, they may be influenced by a combination of these external factors. This is an observational study.\n\n\n\n\n\nTeaching method is the treatment (assigned to students).\n\nSchool type is a blocking factor (cannot be randomly assigned).\n\n\n\n\n\nMedication is the treatment (randomly assigned).\n\nLab is a blocking factor (controls lab-related variability).",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "09_RCBD_Intro.html#choosing-blocking-factors",
    "href": "09_RCBD_Intro.html#choosing-blocking-factors",
    "title": "8  Introduction",
    "section": "8.2 Choosing Blocking Factors",
    "text": "8.2 Choosing Blocking Factors\nAny variable that might affect the response besides treatment factor should be considered for blocking. Common blocking factors include:\n\nGeographic location: field, site, regions or cities that share similar economic conditions.\nTime: experimental replication over different days or weeks. Blocking for economic cycles or seasonal effects.\nSubject: person, plant, businesses, phenotype.\nDemographic groups: age, gender, income or education level, consumer behavior segments.\n\nEquipment: container types, growth chambers.\n\nFor example, if we are testing the effectiveness of a new advertising campaign, it would be useful to block by city or region to control for differences in local economies, purchasing behavior, or media consumption. Similarly, if an experiment measures the impact of dynamic pricing on sales, it is a good practice to replicate the price changes across multiple days, blocking for daily or weekly variations in consumer spending habits. This way time accounts for these differences rather inflating the error variance.\nLikewise, if we are studying the effect of sports training programs on player performance, and athletes train in different facilities with varying equipment, we could assign a block to each training center to ensure that facility-related differences are accounted for. This prevents training location from being mistaken as a treatment effect, allowing a clearer evaluation of the actual program’s impact.\nThe key takeaway is that reducing error variance increases the power of the experiment. Thoughtful blocking design helps achieve this by accounting for known sources of variation.",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "09_RCBD_Intro.html#randomised-complete-block-design",
    "href": "09_RCBD_Intro.html#randomised-complete-block-design",
    "title": "8  Introduction",
    "section": "8.3 Randomised Complete Block Design",
    "text": "8.3 Randomised Complete Block Design\nThere are a few different types of randomised block designs depending on the availability of experimental units and size of the blocks. Here we will consider the best case scenario, where blocks are big enough to contain an equal amount of experimental units such that each treatment occcurs exactly once within a block. If we have a single treatment factor with \\(a\\) levels, then we have \\(a\\) experimental units per block. This design is said to be balanced, each block is the same with respect to treatments. In balanced block designs, the treatment and block effects can be completely separated (are independent) . This greatly simplifies the interpretation of results.\nAs in CRD, randomisation is still a crucial component of the design. The difference is that now \\(a\\) treatments are assigned randomly to the \\(a\\) experimental units within a block, i.e. randomisation is not complete over ALL experimental units but restricted within each block. Within each block, the experimental units are equally likely to receive any of the \\(a\\) treatments. You can see this as CRD within each block!\nLet’s see how we could randomise treatment within blocks using R. Imagine we had four treatments (A,B,C and D). We randomise the treatments to the units within one block like this:\n\nunits &lt;- 1:4\nrbind(sample(units,4), rep(c(\"A\",\"B\",\"C\",\"D\")))\n\n     [,1] [,2] [,3] [,4]\n[1,] \"3\"  \"2\"  \"4\"  \"1\" \n[2,] \"A\"  \"B\"  \"C\"  \"D\" \n\n\nThe third unit receive treatment A, the second receives B and so on. We then repeat this for every block.",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "09_RCBD_Intro.html#the-pygmalion-effect",
    "href": "09_RCBD_Intro.html#the-pygmalion-effect",
    "title": "8  Introduction",
    "section": "8.4 The Pygmalion Effect",
    "text": "8.4 The Pygmalion Effect\nThe Pygmalion effect is a psychological phenomenon that suggests when people are held to high expectations, they tend to perform better. This applies to, for example, teachers and students, managers and employees or coaches and athletes. It is named after a mythological king of Cyprus, Pygmalion, who fell in love with a sculpture he created of his ideal woman.\n\n\nThis article explains the concept nicely and also briefly discusses the study we will use later. https://thedecisionlab.com/biases/the-pygmalion-effect\nMany experiments have found results to support this type of self-fulfilling prophecy. Typically, they involve putting someone in charge of a group of people, then privately telling the leader that say a few of these people are exceptional (these people were randomly selected though). Then, later the performance of the group is measured and if the Pygmalion effect is present, the individuals who were marked as exceptional should have performed better.\nBack in 1990, one researcher in this field, noticed that experiments like these might involve something that is called interpersonal contrasts. When some individuals are singled out for high expectations, others might feel neglected. This could potentially skew the results by making the others look good even though it was just the rest that performed poorly. The researcher wanted to conduct an experiment to test the Pygmalion effect without interpersonal contrasts.\nThey achieved this by applying the high expectation to an entire group and not selected individuals within a group. Let’s have a look the exact experiment!\n\n\n\n\n\n\nExample: The Pygmalion Effect in Military Training\n\n\n\nA study conducted by Eden (1990) examined whether raising leaders’ expectations of their trainees would enhance performance, without creating interpersonal contrast effects.\nA total of 10 army companies consisting of 2 platoons each were used in the study. Within each company, one randomly assigned platoon received the Pygmalion treatment, while the other two served as controls. The idea is that the assignment of the Pygmalion treatment to an entire platoon prevents interpersonal contrasts.\n\nPygmalion Group: Platoon leaders were informed that their trainees had exceptionally high command potential based on pre-existing evaluations.\n\nControl Group: Platoon leaders received no expectation-enhancing information.\n\nOver the training period, leaders in both conditions met biweekly with a psychologist to reinforce expectations. At the end of the program, soldiers took multiple tests which measured their performance in four areas:\n\nTheoretical specialty knowledge (taught by platoon leaders)\n\nPractical specialty skills (taught by platoon leaders)\n\nPhysical fitness (assessed independently)\n\nTarget shooting (assessed independently)\n\n\n\n\n\nA platoon is a military unit typically consisting of 30 to 50 soldiers, led by a platoon leader (usually a lieutenant). Several platoons form a company, which is a larger military unit consisting of three to five platoons, commanded by a company leader (usually a captain).\nFirst things first! We need to determine the design so we can use the appropriate analysis. The researcher was interested in determining the effect of the Pygmalion effect on performance. This indicates to use that there is a single treatment factor and that it is whether or not the Pygmalion effect was applied (we’ll call this the Pygmalion Treatment) and the response is some measure of performance. The text gives four possible responses! The four areas in which the performance was tested. We’ll start with the first one as our response. So far we know:\n\n\nThe name of the treatment factor is not always obvious. It is usually something that describes the collection of similar treatments created in response to some research hypothesis or what has been manipulated. In biological or ecological studies, it can be quite clear. For example, if we had treatments high, medium and low rainfall, “Rainfall” is the variable we manipulated.\nAlso, as before, I’ve modified the example slightly. In the original study, there three platoons per company with two serving as control and one company only had two. So we have simplified the design so that it is balanced.\n\nResponse Variable: Theoretical specialty knowledge.\nTreatment Factor: Pygmalion Treatment.\nTreatment Levels (Groups): Control, Pygmalion\nTreatments: Control, Pygmalion\n\nNow, the treatments were randomly assigned to platoons within a company. This gives away two things, (1) the experimental unit is an entire platoon and (2) treatments were assigned randomly within a company, i.e. a block! They were not interested in the effect of company on performance but merely wanted to account for possible differences between platoons in companies. So here Company is a blocking variable and the 10 companies are the blocks. Finally, on what was the response measured? The soldiers! They are then the observational units. The paper doesn’t state how many soldiers were in each platoon and it doesn’t really matter since the scores have to be combine to have one measurement per experimental unit.\nHere is the final summary of the design:\n\nResponse Variable: Theoretical specialty knowledge.\nTreatment Factor: Pygmalion Treatment.\nTreatment Levels (Groups): Control, Pygmalion\nTreatments: Control, Experiment 1, Experiment 2\n\nExperimental Unit: Platoon (20)\n\nObservational Unit: Soldier\n\nReplicates: 10 platoons received each treatment\n\nRandomisation: To platoons within companies, i.e. restricted to within blocks.\nDesign Type: Randomised Complete Block Design (CRD)\n\nYou will typically have access to the data as well to help you identify some aspects of the design. There are two ways the data might be represented, in long format:\n\n\n\n\n\n\nCompany\nTreat\nScore\n\n\n\n\nC1\nPygmalion\n80.0\n\n\nC1\nControl\n63.2\n\n\nC2\nPygmalion\n83.9\n\n\nC2\nControl\n63.1\n\n\nC3\nPygmalion\n68.2\n\n\nC3\nControl\n76.2\n\n\nC4\nPygmalion\n76.5\n\n\nC4\nControl\n59.5\n\n\nC5\nPygmalion\n87.8\n\n\nC5\nControl\n73.9\n\n\nC6\nPygmalion\n89.8\n\n\nC6\nControl\n78.9\n\n\nC7\nPygmalion\n76.1\n\n\nC7\nControl\n60.6\n\n\nC8\nPygmalion\n71.5\n\n\nC8\nControl\n67.8\n\n\nC9\nPygmalion\n69.5\n\n\nC9\nControl\n72.3\n\n\nC10\nPygmalion\n83.7\n\n\nC10\nControl\n63.7\n\n\n\n\n\n\n\nor in wide format:\n\n\n\n\n\nCompany\nPygmalion\nControl\n\n\n\n\nC1\n80.0\n63.2\n\n\nC2\n83.9\n63.1\n\n\nC3\n68.2\n76.2\n\n\nC4\n76.5\n59.5\n\n\nC5\n87.8\n73.9\n\n\nC6\n89.8\n78.9\n\n\nC7\n76.1\n60.6\n\n\nC8\n71.5\n67.8\n\n\nC9\n69.5\n72.3\n\n\nC10\n83.7\n63.7\n\n\n\n\n\n\n\nThe long format represents each observation as a separate row, with treatments recorded in a single column. This format is useful for statistical modeling and visualization since it keeps data structured for comparisons across treatments. You might have noticed that so far all the data sets we have used when fitting models using aov have been in long format. You will struggle to fit the model with a data set in wide format!\nThe wide format organizes data so that each unit (e.g., a company) appears in a single row, with treatments as separate columns. This format is often preferred for paired comparisons and summary tables.\nBoth formats contain the same information but serve different purposes depending on the type of analysis being performed.",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "11_RCBD_Assump.html",
    "href": "11_RCBD_Assump.html",
    "title": "9  Assumptions",
    "section": "",
    "text": "Data from randomised block designs are analysed with two-way ANOVAs. The assumptions of a two-way ANOVA are the same as a one-way. That is,\n\nEqual population variance.\nNormal errors.\nIndependent errors.\nNo outliers.\n\nWith the addition of a block variable comes a new assumption:\n\nThe effects of the blocks and treatments are additive.\n\nSimply put, it means that the we assume the treatment effects are similar in all blocks. That if a treatment is applied in one block, the effect is the same as in another block. For example, if we applied had a treatment factor: marketing strategy with two treatments A and B, and we want to apply it to stores in different economic regions (blocks), the effect of, for example, marketing strategy A should be the same in both regions. We will check this assumption visually as well. One way, is to plot the response against the block for each treatment. But first! Some exploratory data analysis.\n\n# read in data \npyg_data &lt;- read.csv(\"Datasets/pygmalion_data.csv\")\n\n# look at first and last few rows \nhead(pyg_data); tail(pyg_data)\n\n  Company     Treat Score\n1      C1 Pygmalion  80.0\n2      C1   Control  63.2\n3      C2 Pygmalion  83.9\n4      C2   Control  63.1\n5      C3 Pygmalion  68.2\n6      C3   Control  76.2\n\n\n   Company     Treat Score\n15      C8 Pygmalion  71.5\n16      C8   Control  67.8\n17      C9 Pygmalion  69.5\n18      C9   Control  72.3\n19     C10 Pygmalion  83.7\n20     C10   Control  63.7\n\nsummary(pyg_data)\n\n   Company             Treat               Score      \n Length:20          Length:20          Min.   :59.50  \n Class :character   Class :character   1st Qu.:66.78  \n Mode  :character   Mode  :character   Median :73.10  \n                                       Mean   :73.31  \n                                       3rd Qu.:79.17  \n                                       Max.   :89.80  \n\n\nAh! We need to convert both the company and Treat variable to factors.\n\npyg_data$Company &lt;- as.factor(pyg_data$Company)\npyg_data$Treat &lt;- as.factor(pyg_data$Treat)\n\nsummary(pyg_data)\n\n    Company        Treat        Score      \n C1     :2   Control  :10   Min.   :59.50  \n C10    :2   Pygmalion:10   1st Qu.:66.78  \n C2     :2                  Median :73.10  \n C3     :2                  Mean   :73.31  \n C4     :2                  3rd Qu.:79.17  \n C5     :2                  Max.   :89.80  \n (Other):8                                 \n\n\nNice, now we can see that we had ten replicates per treatment, two observations per block which means 20 observations in total. Let’s go ahead and check the the first four assumptions.\n\nboxplot(Score~Treat, data = pyg_data)\n\nstripchart(Score~Treat, data = pyg_data, add = TRUE, vertical = TRUE, method = \"jitter\", jitter = 0.1)\n\n\n\n\n\n\n\n\nOkay, the boxplots look relatively symmetric, there are no clear signs of non-normality. They also look very similar in terms of height, the assumption of homogeneity seems reasonable as well. Let’s have a look at the sample standard deviations.\n\nsd(pyg_data$Score[pyg_data$Treat == \"Pygmalion\"])\n\n[1] 7.587124\n\nsd(pyg_data$Score[pyg_data$Treat == \"Control\"])\n\n[1] 6.927209\n\n\nThen, we need to check the independence assumption. This is often the hardest assumption to verify because it requires knowledge about how the data were collected. In practice, you will need to assess independence in one of two ways:\n\nBefore conducting an experiment – Ideally, you would discuss the study design with the researchers before data collection to ensure that independence is maintained.\nWhen analyzing existing data – If you are reviewing a published study, you must rely on the authors’ description of the experimental setup to determine whether independence is reasonable.\n\nIn this study, the researchers assumed platoons operated independently and took steps to prevent treatment contamination:\n\nRandomization ensured that each platoon was independently assigned to the Pygmalion or control condition, reducing bias.\nLeaders were instructed not to discuss their treatment condition, preventing expectation spillover.\nEach platoon was analyzed separately, ensuring observations within treatments were treated as independent.\n\nAfter fitting the model, and assuming the order in which the response was measured is the order in which it appears in the data set, we can check for any pattern in the residuals that may indicate dependence.\nNow, let’s check the new assumption of additivity. We can plot the response against treatment and add colour-coded lines connecting the experimental units from the same block. This is a bit tedious to do with base R (don’t worry, we won’t expect you to code this manually) but you have to understand and interpret the plot it produces.\n\n\nCode\n# Ensure Treat is a factor with proper order\npyg_data$Treat &lt;- factor(pyg_data$Treat, levels = c(\"Control\", \"Pygmalion\"))\n\n# Convert Treat to numeric for plotting (1 = Control, 2 = Pygmalion)\npyg_data$Treat_numeric &lt;- as.numeric(pyg_data$Treat)\n\n# Define colors for companies\ncompany_colors &lt;- rainbow(length(unique(pyg_data$Company)))\nnames(company_colors) &lt;- unique(pyg_data$Company)\n\n# Create base plot\nplot(pyg_data$Treat_numeric, pyg_data$Score, \n     xlab = \"Treatment\", ylab = \"Score\", \n     main = \"Pygmalion Effect by Company\",\n     pch = 16, col = company_colors[pyg_data$Company], xaxt = \"n\")\n\n# Add custom x-axis labels\naxis(1, at = c(1, 2), labels = c(\"Control\", \"Pygmalion\"))\n\n# Add lines connecting observations from the same company\nfor(block in unique(pyg_data$Company)){\n  temp &lt;- pyg_data[pyg_data$Company == block, ]\n  temp &lt;- temp[order(temp$Treat_numeric), ]  # Order by treatment for correct line drawing\n  lines(temp$Treat_numeric, temp$Score, col = company_colors[block], lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nIf the assumption of additivity is met, we would expect relatively few lines to cross, i.e. we would expect mostly parallel lines. In the plot above, it seems that for most blocks, a low value in the control treatment is associated with a high value in the Pygmalion treatment. There are three lines (i.e. companies) that don’t conform to this pattern, where it seems that Pygmalion treatment did not alter the scores or maybe even caused a reduction. It’s important to remember that sampling variability prevents us from observing perfectly parallel lines in practice. The observed treatment means are always subject to random variation, which can introduce some deviations from the expected pattern.\nWhat happens if this assumption is wrong, i.e. the blocking and treatment factors do interact? That is the treatment effect depends on which block it is in. Consider the following plots depicting an experiment with three treatments and three blocks. The first panel shows an example where treatments and blocks are additive – the lines connecting the same treatment in all blocks are parallel. Due to variability, we would of course never actually observe such parallel lines. In reality, the observed treatment means would be subject to random deviations from the true population means, and with lots of variability, the lines could cross and look more like the second panel, which is showing an example where the additivity assumption is violated.\n\n\n\n\n\n\n\n\n\nWith only one experimental unit per treatment in each block, as in a typical randomised complete block design, it is difficult to know which situation we have: the additivity assumption is violated, or there is simply a lot of random error. The interaction effect between treatment and block is confounded with the random error term. That is, \\(e_{ij}\\) in the model equation is actually the sum of the interaction effect and the random error. So if the additivity assumption is violated, \\(e_{ij}\\) is inflated and it will be harder to find differences between treatments.\n\n\n\n\n\nWith some replication of treatments within blocks, as in generalised randomised complete block designs (which we don’t cover here), we can separately estimate the interaction effects. This is similar to what we will see when we talk about Factorial Experiment sin the next section.",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assumptions</span>"
    ]
  },
  {
    "objectID": "10_RCBD_Model.html",
    "href": "10_RCBD_Model.html",
    "title": "10  Linear model & ANOVA",
    "section": "",
    "text": "Linear model\nWe wish to compare \\(a\\) treatments and have \\(N\\) experimental units arranged in \\(b\\) blocks each containing \\(a\\) homogeneous experimental units: \\(N = ab\\). The \\(a\\) treatments are assigned to the units in the \\(j^{th}\\) block at random. The design (blocking and treatment factors and the randomisation) determine the structural part of the model.\nA linear model for the RBD is:\n\\[Y_{ij} = \\mu + A_i + B_j  +e_{ij}\\]\nwhere,\n\\[\n\\begin{aligned}\n    Y_{ij} &= \\text{observation on treatment } i \\text{ in block } j \\\\\n    i &= 1, \\dots, a \\text{ and } j = 1, \\dots, b \\\\\n    \\mu &= \\text{general/overall mean} \\\\\n    A_i &= \\text{effect of the } i^{th} \\text{ treatment} \\\\\n    B_j &= \\text{effect of the } j^{th} \\text{ block} \\\\\n    e_{ij} &= \\text{random error with } e_{ij} \\sim N(0, \\sigma^2) \\\\\n    \\sum_{i=1}^{a} A_i &= \\sum_{j=1}^{b} B_j = 0\n\\end{aligned}\n\\]\nThis model says that each observation is made up of an overall mean, a treatment effect, a block effect, and an error part. The block effect is interpreted in the same way as the treatment effect, it is the difference between block mean \\(j\\) and the overall mean \\(\\mu\\).\nIt also says that these effects are additive. Additivity means that the effect of the \\(i^{th}\\) treatment on the response (\\(A_i\\)) is the same regardless of the block in which the treatment is used. Similarly, the effect of the \\(j^{th}\\) block is the same (\\(B_j\\)) regardless of the treatment. The additional constraint of \\(\\sum_{j=1}^{b} \\beta_j = 0\\) follows the same logic as explained before.\nLet’s fit this model to the Pygmalion data. For the Pygmalion experiment, the researchers compared control to Pygmalion treatment so we have \\(a=2\\) treatments. The number of blocks, \\(b\\), was 10. In R, on the right-hand-side of the formula, we have the treatment factor + blocking factor. The code looks exactly the same as before, except we add the Company (blocking) variable.\npyg_model &lt;- aov(Score ~ Treat + Company, data = pyg_data)\nWe can again extract the model estimates with model.table:\nmodel.tables(pyg_model, type = \"means\", se = TRUE)\n\nTables of means\nGrand mean\n      \n73.31 \n\n Treat \nTreat\n  Control Pygmalion \n    67.92     78.70 \n\n Company \nCompany\n   C1   C10    C2    C3    C4    C5    C6    C7    C8    C9 \n71.60 73.70 73.50 72.20 68.00 80.85 84.35 68.35 69.65 70.90 \n\nStandard errors for differences of means\n        Treat Company\n        3.126   6.990\nreplic.    10       2\nFirst we see the grand mean of 73.31 followed by the treatment means. Then, we have ten block means, these are the mean scores within each block. Lastly, we see the standard errors for the differences of means.",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear model & ANOVA</span>"
    ]
  },
  {
    "objectID": "10_RCBD_Model.html#sums-of-squares-and-analysis-of-variance",
    "href": "10_RCBD_Model.html#sums-of-squares-and-analysis-of-variance",
    "title": "10  Linear model & ANOVA",
    "section": "Sums of squares and Analysis of variance",
    "text": "Sums of squares and Analysis of variance\nNow we have three sources of variability: differences between treatments, differences between blocks and experimental error. The total sum of squares can be split into three sums of squares: for treatments, blocks, and error respectively.\n\\[\nSS_{total} = SS_A + SS_B + SSE\n\\]\nwith degrees of freedom\n\\[\n(ab - 1) = (a - 1) + (b - 1) + (a - 1)(b - 1)\n\\]\nThe advantage of blocking becomes apparent here. If we had not blocked, i.e. used a completely randomised design, for example, the \\(SS_A\\) (sums of squares for treatment) would be the same. However, in the completely randomised design, we would not be able to separate \\(SS_B\\) from \\(SSE\\) and the combined \\(SSE\\) would therefore be larger.\nWhen using a RBD, part of the unexplained variation is now explained and can be captured in the block sum of squares, \\(SS_B\\). A small \\(SSE\\) has the advantage of smaller standard errors, i.e. more precise estimates (for treatment effects and treatment means) and thus it is easier to detect differences between treatments.\n\n\nYou can think of the SSE as the variability among experimental units that cannot be accounted for by blocks or treatments.\nThe sums of squares are summarised in an ANOVA table.\n\n\n\n\n\n\n\n\n\n\nSource\nSS\ndf\nMS\nF\n\n\n\n\nTreatments A\n\\(SS_A = b \\sum_i (\\bar{Y}_i - \\bar{Y}_{..})^2\\)\n\\((a - 1)\\)\n\\(\\frac{SS_A}{(a-1)}\\)\n\\(\\frac{MS_A}{MSE}\\)\n\n\nBlocks B\n\\(SS_B = a \\sum_j (\\bar{Y}_j - \\bar{Y}_{..})^2\\)\n\\((b - 1)\\)\n\\(\\frac{SS_B}{(b-1)}\\)\n\n\n\nError\n\\(SSE = \\sum_{ij} (Y_{ij} - \\bar{Y}_{i.} - \\bar{Y}_{.j} + \\bar{Y}_{..})^2\\)\n\\((a - 1)(b - 1)\\)\n\\(\\frac{SSE}{(a-1)(b-1)}\\)\n\n\n\nTotal\n\\(SS_{total} = \\sum (Y_{ij} - \\bar{Y}_{..})^2\\)\n\\(ab - 1\\)\n\n\n\n\n\nMuch of the table remains the same as in a one-way ANOVA, but now it includes an additional row for the blocking variable. The sum of squares for the blocking factor is calculated similarly to that of the treatment factor—by summing the squared deviations of observations within each block from the block’s mean response. The residual sum of squares is also slightly different, but you don’t need to worry too much about that1. Since the total SS is simply the sum of the treatment, block, and residual SS, you can always compute SSE by subtraction.\nFrom this ANOVA table, we can test the hypothesis of no differences between the treatment means as before.\n\\[H_0: \\mu_! = \\mu_2 = \\ldots =\\mu_a\\]\nWhich is equivalent to testing:\n\\[\nH_0 : \\alpha_1 = \\alpha_2 = \\dots = \\alpha_a = 0\n\\]\nusing the F-test which compares the mean square for treatments with the mean square for error:\n\\[\nF = \\frac{MS_A}{MSE} \\sim F_{a-1, (a-1)(b-1)}\n\\]\nNotice the degrees of freedom!\nWhat does the ANOVA table look like for the Pygmalion data? Again, we use the summary function on the model object to obtain the table.\n\nsummary(pyg_model)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nTreat        1  581.0   581.0   11.89 0.00729 **\nCompany      9  510.2    56.7    1.16 0.41433   \nResiduals    9  439.8    48.9                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s first look at the degrees of freedom:\n\nWe had \\(a \\times b = 2 \\times 10 = 20\\) experimental units, so we should have \\(a \\times b - 1\\) total degrees of freedom.\nThe treatment degrees of freedom are \\(a-1 = 2-1 = 1\\) and similarity, there are \\(b-1 = 10 -1=9\\) degrees of freedom of the block effect.\nThat leaves \\((a-1)(b-1) = 1 \\times 9\\) degrees of freedom for the error term.\n\nIt is always a good idea to check that the df’s match with what you expected them to be. One serious error that happens easily is one of the factor is fitted as continuous covariate because the levels were labelled using numbers. Hence why we converted the categorical variables (Treat and Company) to factors.\nThe next column lists the sums of squares for the three components. The mean squares are calculated as \\(\\frac{SS}{df}\\), e.g. \\(\\frac{581}{1} = 581\\) for the treatment. The \\(F\\)-value for the treatment variable is the ratio of \\(MS_{treat}\\) to the \\(MSE\\). This is the same as in the one-way ANOVA. R then looks up the corresponding p-value, which is \\(0.00729\\).\nSo we have an very small p-value which means that we have strong evidence against our null hypothesis of equal treatment means. We cannot conclude that the effects are equal to zero. There is evidence to suggest that at least one treatment resulted in a different mean score. Here, because we have two treatments, the results indicate that the data are not compatible with a null hypothesis of equal means. We make the following conclusion:\n“There is evidence to suggest that the two treatment means are different (\\(F = 11.89\\), \\(p = 0.0073\\)).”\nIf we had more than two treatment means as is usually the case, we would conclude:\n“There is evidence to suggest that at least one treatment resulted in a different mean response, there is evidence for a treatment effect (\\(F = 11.89\\), \\(p = 0.0073\\)).”\nThere are many different ways to say that there is a difference somewhere. For example:\n\nOne or more treatments had a mean response that differed from the others.\nNot all treatment means are the same; at least one is significantly different.\nThe results indicate that not all treatment means are equal.\n\nYou get the idea! As long as it is clear that a ‘significant’ result indicates that there is a difference somewhere, we don’t know where, but there is evidence for a treatment effect.",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear model & ANOVA</span>"
    ]
  },
  {
    "objectID": "10_RCBD_Model.html#what-about-the-f-test-for-the-blocking-variable",
    "href": "10_RCBD_Model.html#what-about-the-f-test-for-the-blocking-variable",
    "title": "10  Linear model & ANOVA",
    "section": "What about the F-test for the blocking variable?",
    "text": "What about the F-test for the blocking variable?\nWe see that the blocks accounted for a similar fraction of the sums of squares as the other two components (just over a third). If we did not block, this variation would be part of the SSE but then the error degrees of freedom would also be larger (the 9 degrees of freedom would be part of the error degrees of freedom). In fact, here, the blocking did not significantly reduce the unexplained variability, since the F-value is close to one. The variability explained by the blocks is close to what would be expected due to random noise.\nRemember, we aren’t particularity interested in formal inference about block effects (we knew or suspected that they were different) and we should always be careful about interpreting the F-test for the blocking variable (as blocks typically cannot be randomised to experimental units - see the previous chapter). We might, however, be interested in whether blocking increased the efficiency of the design by reducing the unexplained variation (SSE). There exists a more thorough method of assessing the relative efficiency of blocking - that is, relative to if a simpler design (i.e. CRD) was used instead2. Here, however, we focus on a simple and quick check of block efficiency using the F-ratio.\nWe would like the block factor to explain a lot of variation. If the mean square of the blocking variable is larger than the error mean square we conclude that blocking was effective (compared to a CRD).\n\nIf \\(F &gt; 1\\) then blocking did reduce unexplained error variance.\nIf \\(F \\approx 1\\) then the blocks did not improve the power of the experiment and you would have been equally well off with a CRD.\nIf \\(F&lt;1\\) which happens rarely, it means that blocking did not account for much of the variability because experimental units within blocks are more heterogeneous than between blocks (or there are strong interactions between blocks and treatments). Blocks actually reduced the power of the experiment but this should really not happen if you choose your blocks sensibly.\n\nIf blocking was not efficient, we would still leave the block factor in the model (design dictates analysis), but we might decide not to use blocking in a similar experiment in the future because it didn’t assist in reducing experimental error variance and only cost us degrees of freedom.",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear model & ANOVA</span>"
    ]
  },
  {
    "objectID": "10_RCBD_Model.html#estimation",
    "href": "10_RCBD_Model.html#estimation",
    "title": "10  Linear model & ANOVA",
    "section": "Estimation",
    "text": "Estimation\nTo obtain estimates for the treatment and block effects, we minimize the error sum of squares (method of least squares).\n\\[\nSSE = \\sum_i \\sum_j (Y_{ij} - \\mu - A_i - B_j)^2\n\\]\n\\(Y_{ij} - \\mu - \\alpha_i - \\beta_j\\) is the observed value minus the expected value (the structural part of the model). This difference is just the error \\(e_{ij}\\). If we minimise the error sum of squares we obtain the following estimates:\n\\[\n\\hat{\\mu} = \\bar{Y}_{..}\n\\]\n\\[\n\\hat{A_i} = \\bar{Y}_{i.} - \\bar{Y}_{..}, \\quad i = 1 \\dots a\n\\]\n\\[\n\\hat{B_j} = \\bar{Y}_{.j} - \\bar{Y}_{..}, \\quad j = 1 \\dots b\n\\]\nTo estimate the \\(i^{th}\\) treatment effect we take the observed treatment mean minus the overall mean, similarly to obtain block effect estimates.\n\n\n\n\nKuehl, Robert O. 2000. “Design of Experiments: Statistical Principles of Research Design and Analysis.” (No Title).",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear model & ANOVA</span>"
    ]
  },
  {
    "objectID": "10_RCBD_Model.html#footnotes",
    "href": "10_RCBD_Model.html#footnotes",
    "title": "10  Linear model & ANOVA",
    "section": "",
    "text": "You can easily get there by rearranging the model equation so that \\(e_{ij}\\) is on the right-hand-side, replacing the effects with the difference in terms of means and simplifying.↩︎\nKuehl (2000) wrote a great textbook (freely available) that explains the relative efficiency check in detail.↩︎",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear model & ANOVA</span>"
    ]
  },
  {
    "objectID": "12_RCBD_Contrasts.html",
    "href": "12_RCBD_Contrasts.html",
    "title": "11  Contrasts",
    "section": "",
    "text": "After finding that there is evidence to suggest that at least two of the treatments differed from each other (another way to say there is a treatment effect!), we want to find out which ones differed and estimate these differences. In the Pygmalion experiment, there were only two treatments so we know the difference is between them. In fact, we could have used a paired t-test to analyse the data and we would get the same results. Generally though, there will be more than two treatments and then after concluding that there are differences, we want to know where the differences lie.\n\n\nWhen we have blocks in RCBD, the observations are paired and data from two treatments can be analysed using a paired t-test. When we do not have blocks, the observations are not and data from two treatments can be analysed using a standard t-test.\nTo do that, we use the coefficients from fitting a linear regression model to estimate the difference between the two treatments (as we did for CRD experiments as well). The null hypothesis is again:\n\\[H_0: \\mu_C - \\mu_P = 0 \\]\nwhere C stands for Control and P for Pygmalion. We use the lm function as in regression.\n\n\nWe use the lm function when we are primarily interested in the coefficient estimates and difference. We use aov() when we want a breakdown of how much each factor can explain of the overall variation in the response, and when we want a general test for ‘are there any difference between the treatments’.\n\npyg_model_reg &lt;- lm(Score ~ Treat + Company, data = pyg_data)\nsummary(pyg_model_reg)\n\n\nCall:\nlm(formula = Score ~ Treat + Company, data = pyg_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.390 -3.217  0.000  3.217  9.390 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      66.210      5.184  12.771 4.52e-07 ***\nTreatPygmalion   10.780      3.126   3.448  0.00729 ** \nCompanyC10        2.100      6.990   0.300  0.77069    \nCompanyC2         1.900      6.990   0.272  0.79191    \nCompanyC3         0.600      6.990   0.086  0.93348    \nCompanyC4        -3.600      6.990  -0.515  0.61897    \nCompanyC5         9.250      6.990   1.323  0.21839    \nCompanyC6        12.750      6.990   1.824  0.10147    \nCompanyC7        -3.250      6.990  -0.465  0.65303    \nCompanyC8        -1.950      6.990  -0.279  0.78659    \nCompanyC9        -0.700      6.990  -0.100  0.92243    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.99 on 9 degrees of freedom\nMultiple R-squared:  0.7127,    Adjusted R-squared:  0.3936 \nF-statistic: 2.233 on 10 and 9 DF,  p-value: 0.1211\n\n\nA few things to note here.\n\nWe are only interested in the second line starting with TreatPygmalion. R pasted the name of the factor and the name of the treatment level together.\nThe Control treatment was taken as the baseline (it comes first in the alphabet).\nThe remaining lines are not of interest to us. It estimates the differences between each block and the first and tests whether these effects are different from zero. R doesn’t know we aren’t interested in these, so it computes the effects and hypothesis test as if we are. We ignore this part.\nIf we didn’t know that this code was for analysing a RCBD we would probably think that it is linear regression with two categorical variables. Think back to the regression module, what does this intercept represent? It represents the mean score for some treatment level and company. Since ‘C’ comes before ‘P’ in the alphabet and C1 is before everything else, the intercept is the average score for the control group in the first block. This is because R uses the treatment contrast parameterisation by default for all the factors. We can change this by letting R know that the block effects sum to zero.\n\n\npyg_model_reg2 &lt;- lm(Score ~ Treat + C(as.factor(Company), contr.sum), data = pyg_data)\nsummary(pyg_model_reg2)\n\n\nCall:\nlm(formula = Score ~ Treat + C(as.factor(Company), contr.sum), \n    data = pyg_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.390 -3.217  0.000  3.217  9.390 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         67.920      2.211  30.725 2.01e-10 ***\nTreatPygmalion                      10.780      3.126   3.448  0.00729 ** \nC(as.factor(Company), contr.sum)1   -1.710      4.689  -0.365  0.72379    \nC(as.factor(Company), contr.sum)2    0.390      4.689   0.083  0.93554    \nC(as.factor(Company), contr.sum)3    0.190      4.689   0.041  0.96857    \nC(as.factor(Company), contr.sum)4   -1.110      4.689  -0.237  0.81818    \nC(as.factor(Company), contr.sum)5   -5.310      4.689  -1.132  0.28675    \nC(as.factor(Company), contr.sum)6    7.540      4.689   1.608  0.14232    \nC(as.factor(Company), contr.sum)7   11.040      4.689   2.354  0.04300 *  \nC(as.factor(Company), contr.sum)8   -4.960      4.689  -1.058  0.31775    \nC(as.factor(Company), contr.sum)9   -3.660      4.689  -0.780  0.45514    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.99 on 9 degrees of freedom\nMultiple R-squared:  0.7127,    Adjusted R-squared:  0.3936 \nF-statistic: 2.233 on 10 and 9 DF,  p-value: 0.1211\n\n\nNow, the intercept is what we expect, the mean score for the control treatment across all blocks.\n\nmean(pyg_data$Score[pyg_data$Treat == \"Control\"])\n\n[1] 67.92\n\n\nLet’s interpret the hypothesis test for the difference between the treatment means. The estimated difference is 10.78 with a standard error of 3.126. The test statistic is 3.448 which has a p-value of 0.00729. Look familiar? It’s the exact same p-value we found in the ANOVA table! That is because the ANOVA is an extension of the t-test to more than two groups and when we only have two treatments, they are equivalent. In fact, the test statistics have the following relationship:\n\\[ t^2 = F\\]\nTest the result to confirm that it holds. Now, let’s return to the interpretation. The test shows that the difference between the control and Pygmalion treatment is statistically significant, as indicated by the extremely small p-value. This provides strong evidence against the null hypothesis of equal means.\nTo recall the experiment’s design: The researchers aimed to test the Pygmalion effect while eliminating interpersonal contrasts by assigning treatments to entire groups. Specifically, platoons within companies were used as treatment units, and since there were 10 companies, each with 2 platoons, companies served as blocks. The response variable, theoretical specialty knowledge, was measured through test scores.\nThe results of a two-way ANOVA provide evidence of a treatment effect (\\(F = 11.89\\), \\(p = 0.0073\\)). More precisely, the estimated difference between the control and Pygmalion treatment was 10.78 (s.e. = \\(3.13\\), \\(t = 3.45\\), \\(p = 0.0073\\)). This suggests that the Pygmalion effect was successful, as soldiers in the Pygmalion group scored higher on average than those in the control group.\n\n\nIn an actual analysis, we would not report both the ANOVA and t-test since they are equivalent when we have two treatments.\nNice! We’re done. Before we move on, I’ll summarise the results of the actual study. The researcher had four different responses:\n\nTheoretical specialty knowledge (taught by platoon leaders)\nPractical specialty skills (taught by platoon leaders)\nPhysical fitness (assessed independently)\nTarget shooting (assessed independently)\n\nSignificant treatment effects were found for theoretical and practical specialty scores (\\(F = 13.74\\), \\(p &lt; 0.01\\) and \\(F = 6.37\\), \\(p &lt; 0.05\\), respectively). No significant difference was found for physical fitness or target shooting, confirming that the Pygmalion effect was specific to areas influenced by leader expectations! This suggests that high expectations from others can enhance performance, particularly in areas where they have direct influence. With this in mind, I want you to know that I believe in your potential to excel in this course and expect nothing less. ;) On to the next section!",
    "crumbs": [
      "Randomised Block Designs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Contrasts</span>"
    ]
  },
  {
    "objectID": "13_FE_Intro.html",
    "href": "13_FE_Intro.html",
    "title": "13  Introduction",
    "section": "",
    "text": "13.1 Factorial Structure vs. Experimental Design\nSo far, we have explored experiments with a single treatment factor. However, in many cases, analyzing factors one at a time does not fully explain the behavior of the response variable. This is particularly true when factors interact, meaning that the effect of one factor depends on the level or setting of another factor.\nA factorial experiment involves more than one treatment factor, allowing us to study how factors interact. In a complete factorial experiment, every possible combination of factor levels is tested. The total number of treatments is the product of the number of levels for each factor. In other words, each treatment is a combination of one level from each factor.\nIt is important to note that a factorial experiment is not a design by itself—it is a treatment structure. The underlying design can be:\nIn the social media multitasking example, suppose the researchers wanted to know whether the effect of social media multitasking on academic performance is mitigated by lecture format? We would ask:\nThe experiment would still follow a Completely Randomized Design (CRD) but now with two treatment factors instead of one.\nSimilarly, if we extended the Pygmalion experiment to include an additional factor, we would have an RCBD with two treatment factors.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "13_FE_Intro.html#factorial-structure-vs.-experimental-design",
    "href": "13_FE_Intro.html#factorial-structure-vs.-experimental-design",
    "title": "13  Introduction",
    "section": "",
    "text": "A Completely Randomized Design (CRD)\nA Randomized Complete Block Design (RCBD)\n\n\n\nDoes the effect of social media multitasking on academic performance depend on lecture format?",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "13_FE_Intro.html#notation-and-structure-of-factorial-experiments",
    "href": "13_FE_Intro.html#notation-and-structure-of-factorial-experiments",
    "title": "13  Introduction",
    "section": "13.2 Notation and Structure of Factorial Experiments",
    "text": "13.2 Notation and Structure of Factorial Experiments\nIn general, if an experiment has two treatment factors with \\(a\\) and \\(c\\) levels, then there are \\(a \\times c\\) treatments. This is called an \\(a \\times c\\) factorial treatment structure.\nTo clarify the terminology:\n\nA treatment factor has different levels (e.g., social media multitasking: none, texting, Facebook).\nTreatments are the combinations of factor levels (e.g., no multitasking + lecture format A, texting + lecture format B).\n\nIn factorial experiments, the treatment factors are said to be crossed, meaning that all levels of one factor appear at all levels of the other factor.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "13_FE_Intro.html#randomisation-in-factorial-experiments",
    "href": "13_FE_Intro.html#randomisation-in-factorial-experiments",
    "title": "13  Introduction",
    "section": "13.3 Randomisation in Factorial Experiments",
    "text": "13.3 Randomisation in Factorial Experiments\nRandomization in factorial experiments depends on the chosen design and is carried out similarly to single-factor experiments. In R, it is helpful to number or name the treatments systematically.\nSuppose we have two factors:\n\nMarketing Strategy (2 levels: \\(m_0, m_1\\))\nProduct Promotion (2 levels: \\(p_0, p_1\\))\n\nThis creates four treatments:\n\\(m_0p_0\\), \\(m_0p_1\\), \\(m_1p_0\\), \\(m_1p_1\\)\nIf we have 12 experimental units and no need for blocking, we conduct a Completely Randomized Design (CRD) as follows:\n\ntreats &lt;- c(\"m0p0\", \"m0p1\", \"m1p0\", \"m1p1\")\ntreats &lt;- rep(treats, each = 3)  # Repeat each treatment 3 times\ntreats \n\n [1] \"m0p0\" \"m0p0\" \"m0p0\" \"m0p1\" \"m0p1\" \"m0p1\" \"m1p0\" \"m1p0\" \"m1p0\" \"m1p1\"\n[11] \"m1p1\" \"m1p1\"\n\nr1 &lt;- sample(treats)  # Randomly assign treatments\n\ncbind(1:12, r1)  # Display the assignments\n\n           r1    \n [1,] \"1\"  \"m1p0\"\n [2,] \"2\"  \"m0p0\"\n [3,] \"3\"  \"m0p0\"\n [4,] \"4\"  \"m0p1\"\n [5,] \"5\"  \"m1p1\"\n [6,] \"6\"  \"m1p0\"\n [7,] \"7\"  \"m1p0\"\n [8,] \"8\"  \"m1p1\"\n [9,] \"9\"  \"m0p0\"\n[10,] \"10\" \"m1p1\"\n[11,] \"11\" \"m0p1\"\n[12,] \"12\" \"m0p1\"\n\n\nThis code assigns treatments randomly and prints the experimental unit number alongside its assigned treatment. If we had blocking, we would repeat the randomization separately for each block.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "13_FE_Intro.html#is-comprehension-affect-by-playback-speed-and-lecture-modality",
    "href": "13_FE_Intro.html#is-comprehension-affect-by-playback-speed-and-lecture-modality",
    "title": "12  Introduction",
    "section": "12.4 Is comprehension affect by playback speed and lecture modality?",
    "text": "12.4 Is comprehension affect by playback speed and lecture modality?\nIn keeping with the theme of students, learning and teaching. Have you ever wondered whether playback speed affects your comprehension of a lecture? Or whether your comprehension is better with audio-only lectures such as podcast versus recorded lectures with visuals? What about if you listen to a podcast at double speed versus a recorded lecture at double speed, is there difference in comprehension? to answer this question, researchers from the University of California conducted a \\(2\\times2\\) factorial experiment.\n\n\n\n\n\n\nLecture modality and playback speed\n\n\n\nChen et al. (2024) conducted an experiment to find out whether visual information improves comprehension when lectures are played at faster speeds. Specifically, they wanted to investigate the effect of lecture modality (audio-only or audio-visual) and playback speed (1x or 2x) on comprehension of students and whether these factors interact. We can summarise the research questions as follows:\n\nDoes lecture modality have an effect on comprehension?\nDoes playback speed have an effect on comprehension?\nIs there an interaction effect of modality and playback speed on comprehension?\n\nA total of 200 undergraduate students were randomly assigned to one of four groups:\n\nAudio-only at normal speed (1x)\nAudio-visual (with slides) at normal speed (1x)\nAudio-only at double speed (2x)\nAudio-visual (with slides) at double speed (2x)\n\nThe researchers chose two lectures: one about about real estate appraisals and another bout the history of the Roman Empire. The lectures were either presented as audio-visual clips which consisted of presentation slides and instructor images, and no subtitles or captions were provided. All the graphics (maps, figures) in the slides were static. For lectures presented as audio-only clips, only the instructor’s audio was made available.\nEach student was presented both lectures in the modality and speed they were assigned. Afterwards, they completed a comprehension test consisting of 25 multiple choice questions on each topic. The average of the scores was taken as the final measure of comprehension.\n\n\n\n\nWe will be using the actual data collected but we will only be using a subset of the information they recorded. The authors conducted a different analysis which incorporates this extra information. We will not be doing this as the method they used is outside th scope of this course.\nRight! Let’s begin with identifying the design. It should be clear that we have two treatment factors: lecture modality and playback speed each with the treatment levels. this means that we have a total of \\(2 \\times 2 = 4\\) treatments which are the combinations of the treatment levels. They investigated the effect of these factors on the comprehension of students - that means, comprehension is the response.\n\nResponse Variable: Comprehension\n\nTreatment Factors: Lecture modality and playback speed\n\nTreatment Levels: Lecture modality: Audio-only or Audio-visual; Playback speed: 1x or 2x\n\nTreatments: Audio-only at normal speed (1x); Audio-visual at normal speed (1x); Audio-only at double speed (2x); Audio-visual at double speed (2x)\n\nEach student was assigned to one the treatments indicating that students were the experimental units. The response was also measured on each student, they are the observational units as well. Therefore, since we had 200 students and 4 treatment groups, there was 50 students per group, the experiment had 50 replicates.\n\nExperimental Unit: Student (200)\n\nObservational Unit: Student (200)\n\nReplicates: 50 students per group\n\n\nLastly, we need to determine how randomisation was conducted. There is no indication of any blocking and treatments were randomised to the whole group of experimental units. So this is a Completely Randomised Design, specifically it is a \\(2\\times2\\) factorial CRD.\n\nDesign Type: \\(2\\times2\\) factorial Completely Randomized Design (CRD)\n\nBefore we do any further analysis, we need to talk a bit about effects!\n\n\n\n\nChen, Ashley, Suchita E Kumar, Rhea Varkhedi, and Dillon H Murphy. 2024. “The Effect of Playback Speed and Distractions on the Comprehension of Audio and Audio-Visual Materials.” Educational Psychology Review 36 (3): 79.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "15_FE_Inter.html",
    "href": "15_FE_Inter.html",
    "title": "14  Interactions",
    "section": "",
    "text": "Interactions between treatment factors are an important reason for conducting factorial experiments. If the effect of a factor would always be the same, no matter which other factors are present, and at what levels, it would be enough to investigate this factor on its own in a single-factor experiment. However, many factors interact with other factors, which means that the effects change, depending on the levels of the other factors.\nUp until now, we have spoken rather loosely about ‘effects’. But at this point, we need to define more clearly what we mean by the effect of a treatment or the effect of an explanatory variable. By the effect of a treatment, we mean the change in response relative to either a reference or baseline treatment, or often in experiments, to an overall mean response.\nIn regression, the effect of a continuous explanatory variable is measured by the slope, which is the change in response for a one-unit increase in the explanatory variable, i.e., relative to one unit less. The effect of categorical or factor variables in regression is the change in response relative to a reference category.\nIn experiments, when using an ANOVA model, the effect of a treatment is mostly measured as the change in response relative to an overall mean response.\nThere are different kinds of effects: main effects and interaction effects.\nThe main effect of a treatment measures the average change in response, averaged over all levels of the other factors, relative to the overall mean. When there is only a single factor in an experiment, we only have main effects.\nIf the effect of a factor depends on the level of another factor that is present, then the two factors interact. The interaction effect represents the change in response relative to the main effects with a particular treatment.\nIf there are multiple factors in an experiment, and the effects of one factor depend on the level of the other factor, i.e., the two factors interact, the (average) effects might not give a good idea of changes in the response, or of how the factors affect the response. In such cases, we need to study the individual treatments more closely. We look at the combinations of factor levels with large interaction effects.\nThe figure below illustrates a number of possible interaction situations in a \\(2 \\times 2\\) factorial experiment, with treatment factor A having levels a1 and a2, and factor B having levels b1 and b2. To determine whether main effects of A are present, we must ask whether the average response changes when moving from a1 to a2, and similarly for main effects of B.\nTo determine whether interaction effects are present, we must ask whether the change in response when moving from a1 to a2 depends on the level of B. Main effects and interaction effects can both be present simultaneously.\nBefore we do anything, orient yourself. What is represented by each axis, how is the graph differentiating between treatment factors? In the figure below, the response is on the y-axis and we have the levels of treatment factor A on the x-axis, the levels of B are denoted by the colour of the line.\nLet’s start with (a). The main effect of A is the average change in response - averaged over all levels of the other factors. Essentially, we need to determine what happens to the response when we ignore the levels of B. To do this, we have to calculate the average of the points at a1 and separately, at a2. When points in a line are eveny distributed, the average is the mid-point. If you do this for both levels of A and connect the dots, you will have drawn a flat horizontal line. Now, going from a1 to a2, what happens to the response? In other words, does the average response change? No, there is n change which means there is no main effect of factor A.\nGoing through the same motions for factor B, reveals that going from b1 to b2 increases the mean response. There is a main effect of factor B. What about an interaction effect? We ask: does the effect of A on the response change when we consider the levels of the other factor? now we do not avearge over the other factor, we take it into account. Looking at the plot, if we focus on the red line and go from a1 to a2, nothing happens to the response. If we focus on the black line, the same (i.e. nothing) happens to the response as well. If we reverse this and focus on the points at a1, going from b1 (red) to b2 (black) increases the response. At a2, the response increases as well by the same amount. So nothing changed when we considered the other factors, there is no interaction.\nNow, consider plot (e).\n\nAveraging the response at a1 and a2 results in a horizontal line again. No main effect of factor A.\nAveraging the response at b1 and b2, leads to the same conclusion as before. Going from b1 to b2, increases the response. There is a main effect of factor B.\nIf we focus on the red line (b1), going from a1 to a2 increases the response. However, at b2, going from a1 to a2 decreases the response! The effect of treatment factor A depends on the level of B, they interact with each other.\n\nSo does this mean that A and B have no effect on the response? No, they both affect the response, their effects, however, depend on the level of the other factor present.\nPlot (e) demonstrates clearly why sometimes main effects cannot be understood or interpreted when interactions are present. In such a case, the interaction plot is very helpful to illustrate the effects. Try deciding for the other plots whether there are main effects for factor A and B and whether A and B interact. The answers are given in the figure caption.\n\n\n\n\n\nInteraction plots for six hypothetical \\(2 \\times 2\\) factorial experiments. (a) only main effect of B, (b) no main effects and no interaction effects, (c) only main effect of A, (d) main effect of A and main effect of B, (e) interaction between A and B, but both main effects are 0, (f) main effect of B, small main effect of A, A and B interact.\n\n\n\n\nInteraction effects are calculated as the difference between the treatment mean and the sum of the main effects. To express this more precisely, it is useful to write down the model.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "15_FE_Inter.html#can-going-for-a-brief-walk-help-with-memory-performance",
    "href": "15_FE_Inter.html#can-going-for-a-brief-walk-help-with-memory-performance",
    "title": "13  Interactions",
    "section": "",
    "text": "Warning\n\n\n\nSalas, Minakata, and Kelemen (2011) conducted a study to examine whether a brief bout of aerobic exercise influences memory performance and judgements of learning (JOLs).\nA total of 80 college students participated in a 2 × 2 factorial between-subjects design where they were randomly assigned to one of four conditions:\nWalking-Walking: Participants walked before both encoding and retrieval. Walking-Sitting: Participants walked before encoding but sat before retrieval. Sitting-Walking: Participants sat before encoding but walked before retrieval. Sitting-Sitting (Control): Participants sat before both encoding and retrieval.\nAfter the activity, all students studied 30 English nouns, provided immediate JOLs, and then took a free recall test\n\n\n\n\n\n\nSalas, Carlos R, Katsumi Minakata, and William L Kelemen. 2011. “Walking Before Study Enhances Free Recall but Not Judgement-of-Learning Magnitude.” Journal of Cognitive Psychology 23 (4): 507–13.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "14_FE_Model.html",
    "href": "14_FE_Model.html",
    "title": "15  Model for Factorial Experiments",
    "section": "",
    "text": "15.1 Replication\nAssuming we have a continuous response variable for which we assume a normal distribution, no blocking factors and a factorial experiment with two treatment factors, the following model is plausible:\n\\[\nY_{ijk} = \\mu + A_i + C_j + (AC)_{ij} + e_{ijk}\n\\]\nwhere \\(Y_{ijk}\\) is the \\(k^{th}\\) observation on the \\((ij)^{th}\\) treatment combination and\n\\[\n\\begin{aligned}\n    Y_{ij} &= \\text{observation on treatment } i \\text{ in block } j \\\\\n    \\mu &= \\text{general/overall mean} \\\\\n    A_i &= \\text{main effect of the } i^{th} \\text{ level of A} \\\\\n    A_i &= \\text{main effect of the } j^{th} \\text{ level of C} \\\\\n    (AC)_{ij} &= \\text{interaction between the }i^{th}\\text{ level of A and the }j^{th}\\text{ level of C.} \\\\\n    e_{ijk} &= \\text{random error with } e_{ijk} \\sim N(0, \\sigma^2) \\\\\n\\end{aligned}\n\\]\nNote that (AC) is a single symbol and does not mean the interpaction is the product of the two main effects.\n\\(\\mu + A_i + C_j + (AC)_{ij}\\) is the structural part of the model which describes the mean or expected response with treatment \\(ij\\), i.e. at the \\(i^{th}\\) level of factor A and \\(j^{th}\\) level of factor C. Depending on the estimates for the main effects, each treatment will have a different estimated mean response. For every level of A there is a main effect, the \\(A_i\\)’s. For every level of factor B there is a main effect, \\(B_i\\). For every combination of A and B levels there is an interaction effect, \\((AC)_{ij}\\). So the model implies that each treatment mean is made up of an overall mean, two main effects and and interaction term.\nReplication is crucial in any experiment! Without replication, we cannot estimate the experimental error variance (\\(\\sigma^2\\)), which is essential for assessing variability and conducting hypothesis tests.\nIf we only have one observation per treatment, that observation becomes the treatment mean. Since we cannot compute deviations from the treatment mean, there is no estimate of error variance. This means that while we can technically estimate the model parameters, the model itself is practically useless—we cannot perform hypothesis tests without an estimate of error variance. And if we can’t test anything, what’s the point?\nIn factorial experiments, the situation gets even worse when we don’t replicate treatments. Specifically, we can’t calculate the interaction effect. In general, the interaction effect is calculated as the difference between the treatment mean and the sum of the main effects and the overall mean:\n\\[(AC)_{ij} = \\bar{Y}_{ij} - (\\mu + A_i + C_j) \\]\nNow consider the first plot in the series below. There is only one observation in the hypothetical treat \\(i = 1\\) and \\(j = 3\\). That means that the treatment mean \\(\\bar{Y}_{ij}\\) is just the mean of this single observation. We can’t calculate any deviations from this mean with only one observation as we usually would for the error variance. We always need to calculate an error term and this is always calculated as the deviation of the observation to the next closest mean. With only one observation per treatment, the next closest mean to that observation is the sum of the main effects: \\(\\mu + A_i + B_j\\). But wait, that means the error term is also the interaction term since the treatment mean and the observation are the same? Jup! Now you see the problem. With no replication, the error term and the interaction effect are confounded.\nHow is the interaction effect calculated? (a) only one observation, interaction effect confounded with error term, cannot estimate interaction effect; (b) large interaction present; (c) interaction statistically not discernable (very small).\nHave a look at the second plot. Now we have five observations within the treatment. We can calculate the 5 error terms:\n\\[r_{13k} = Y_{13k} - \\bar{Y}_{13}\\]\nand we can calculate the interaction effect:\n\\[ (\\hat{AB})_{13} = \\bar{Y}_{13}- (\\hat{\\mu} + \\hat{A}_1 + \\hat{B}_3)\\]\nThey are no longer the same thing, they are separable! The interaction effect for this treatment is quite big if you look at the difference visually. For the last plot, there are also five observations, but now the deviation of the treatment mean from the sum f the main effects is almost zero; it’s just due to random variation. The interaction effect is too small to detect statistically.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model for Factorial Experiments</span>"
    ]
  },
  {
    "objectID": "14_FE_Model.html#parameter-estimation",
    "href": "14_FE_Model.html#parameter-estimation",
    "title": "15  Model for Factorial Experiments",
    "section": "15.2 Parameter estimation",
    "text": "15.2 Parameter estimation\nThe maximum likelihood/least squares estimates are found by minimizing the error or residual sum of squares:\n\\[\nS = \\sum_{ijk} (Y_{ijk} - \\mu - \\alpha_i - \\beta_j - (\\alpha\\beta)_{ij})^2\n\\]\nThe solutions to these equations are the least squares estimates:\n\\[\n\\hat{\\mu} = \\bar{Y}_{...}\n\\]\n\\[\n\\hat{A}_i = \\bar{Y}_{i..} - \\bar{Y}_{...}, \\quad i = 1, \\dots, a\n\\]\n\\[\n\\hat{C}_j = \\bar{Y}_{.j.} - \\bar{Y}_{...}, \\quad j = 1, \\dots, c\n\\]\n\\[\n(\\hat{AC})_{ij} = \\bar{Y}_{ij.} - \\bar{Y}_{i..} - \\bar{Y}_{.j.} + \\bar{Y}_{...}, \\quad i = 1, \\dots, a, \\quad j = 1, \\dots,c\n\\]\nThe main effects are as before, except that now they refer to differences between row or column means ([Figure 6.3]) and the overall mean. The interaction effects are estimated as the differences between treatment means and the sum of the main effects.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model for Factorial Experiments</span>"
    ]
  },
  {
    "objectID": "14_FE_Model.html#back-to-the-example",
    "href": "14_FE_Model.html#back-to-the-example",
    "title": "15  Model for Factorial Experiments",
    "section": "15.3 Back to the example",
    "text": "15.3 Back to the example\nLet’s fit this model to the data of the playback and lecture modality experiment. This time, we have access to the actual data collected! Let’s explore the data and check our assumptions. The assumptions are the same as in a one-way ANOVA. That is normality of errors, equal population variances, independent errors and no outliers.\nAs always we start by reading in our data, checking that it has been read in correctly and looking at some descriptive statistics.\n\ndata &lt;- read.csv(\"Datasets/Exp2DataPlayback.csv\")\n\nhead(data)\n\n  Participant.ID       Condition Speed Content.Type Accuracy\n1     945445adf5 1x Audio-Visual     2 Audio-Visual       42\n2     23afb88ef3        1x Audio     1   Audio-Only       56\n3     1bc24e0480        1x Audio     1   Audio-Only       62\n4     4fbdbd41a5        1x Audio     1   Audio-Only       44\n5     442adf227a        1x Audio     1   Audio-Only       56\n6     3ca9d09e2e 1x Audio-Visual     2 Audio-Visual       48\n\n\nThe data set contains 5 columns:\n\nParticipant.ID – This column contains a unique identification code for each participant in the study.\n\nCondition – Indicates the experimental condition or treatment, which includes both playback speed (1x or 2x) and content type (Audio-Only or Audio-Visual).\n\nSpeed – A numeric column that explicitly represents the playback speed, with 1 for normal speed (1x) and 2 for double speed (2x).\n\nContent.Type – Specifies whether the participant received Audio-Only or Audio-Visual content.\n\nAccuracy– The participant’s performance score, representing comprehension accuracy.\n\n\nsummary(data)\n\n Participant.ID      Condition             Speed     Content.Type      \n Length:200         Length:200         Min.   :1.0   Length:200        \n Class :character   Class :character   1st Qu.:1.0   Class :character  \n Mode  :character   Mode  :character   Median :1.5   Mode  :character  \n                                       Mean   :1.5                     \n                                       3rd Qu.:2.0                     \n                                       Max.   :2.0                     \n    Accuracy    \n Min.   :14.00  \n 1st Qu.:42.00  \n Median :54.00  \n Mean   :52.78  \n 3rd Qu.:64.00  \n Max.   :90.00  \n\n\nFrom the summary, you should notice a few things:\n\nAll the columns are read in as character values except Speed and Accuracy. We need the relevant columns to factors if we want to use them in our analysis.\nSpeed and Accuracy seem to be read in as numeric values. This makes sense for Accuracy but not Speed! Speed is a categorical variable with levels 1x and 2x, we need to correct this.\n\n\ndata$Condition &lt;- factor(data$Condition)\ndata$Content.Type &lt;- factor(data$Content.Type)\ndata$Speed &lt;- factor(data$Speed)\n\nsummary(data)\n\n Participant.ID               Condition  Speed         Content.Type\n Length:200         1x Audio       :50   1:100   Audio-Only  :100  \n Class :character   1x Audio-Visual:50   2:100   Audio-Visual:100  \n Mode  :character   2x Audio       :50                             \n                    2x Audio-Visual:50                             \n                                                                   \n                                                                   \n    Accuracy    \n Min.   :14.00  \n 1st Qu.:42.00  \n Median :54.00  \n Mean   :52.78  \n 3rd Qu.:64.00  \n Max.   :90.00  \n\n\nGreat, now we can see that each treatment was replicated 50 times as we expected. To check our assumptions we start as always by plotting the response against treatments.\n\nboxplot(Accuracy ~ Condition, data = data, \n        ylab = \"\", main = \"\", las = 1) \n\n# we could also have specified the first argument as Accuracy ~ Content.Type * Speed\n\nstripchart(Accuracy ~ Condition, data = data, vertical = TRUE, add = TRUE, method = \"jitter\")\n\n\n\n\n\n\n\n\nThere are no clear signs of deviation from normality, the box-plots look relatively symmetric. We could plot Q-Q plots for each treatment as well. Let’s do that for two of the treatments.\n\npar(mfrow=c(1,2)) # splitting the plotting window into 1 row with 2 columns\n\nqqnorm(data$Accuracy[data$Condition == \"1x Audio\"], pty = 4, col =\"blue\", main = \"1x Audio\")\nqqline(data$Accuracy[data$Condition == \"1x Audio\"], col = \"red\")\n\nqqnorm(data$Accuracy[data$Condition == \"1x Audio-Visual\"], pty = 4, col =\"blue\", main = \"1x Audio-Visual\")\nqqline(data$Accuracy[data$Condition == \"1x Audio-Visual\"], col = \"red\")\n\n\n\n\n\n\n\n\nNo worrisome patterns! Next, the box-plots also suggest that there are no outliers and there are no clear indications that the assumption of equal population variance is not reasonable. Let’s have a look a the standard deviations.\n\nsort(tapply(data$Accuracy, data$Condition, sd))\n\n1x Audio-Visual        1x Audio        2x Audio 2x Audio-Visual \n       13.71690        14.01084        14.93064        16.76386 \n\n\nThe ratio of the smallest to largest is roughly 1.22 which is much smaller than five. Lastly, we need to check the assumption of independence. We start by assuming that the order in which the data are in the data set is the order in which the measurements were taken and we construct a Cleveland dot chart.\n\ndotchart(data$Accuracy, ylab = \"Order of observation\", xlab =\"Post treatment test score\")\n\n\n\n\n\n\n\n\nNo clear patterns in the measurements, so no reason to suspect any dependence between successive measurements. The students were randomly assigned to each group and there are no other reasons to believe that independence was violated based on the description of the experiment.\nWith all the assumptions checked, we can move onto fitting the linear model to our data and inspecting the model estimates. Here is the model equation:\n\\[ \\text{Accuracy}_{ijk} = \\mu + \\text{Speed}_i + \\text{Content.Type}_j + \\text{(Speed:Content.Type)}_{ij} + e_{ijk} \\]\nwhere,\n\\[\n\\begin{aligned}\n    i &= 1,2\\text{ and }j = 1,2\\\\\n    e_{ijk} &= \\text{random error with } e_{ijk} \\sim N(0, \\sigma^2) \\\\\n\\end{aligned}\n\\]\nIn R, we fit the model like this:\n\nmodel &lt;- aov(Accuracy ~ Speed + Content.Type + Speed:Content.Type, data = data)\nmodel.tables(model, type = \"means\", se = TRUE)\n\nTables of means\nGrand mean\n      \n52.78 \n\n Speed \nSpeed\n    1     2 \n54.94 50.62 \n\n Content.Type \nContent.Type\n  Audio-Only Audio-Visual \n       49.10        56.46 \n\n Speed:Content.Type \n     Content.Type\nSpeed Audio-Only Audio-Visual\n    1 50.32      59.56       \n    2 47.88      53.36       \n\nStandard errors for differences of means\n        Speed Content.Type Speed:Content.Type\n        2.108        2.108              2.981\nreplic.   100          100                 50\n\n\nR allows a convenient shorthand for this type of model. Instead of typing out all three terms, you can shorten the right hand side of the formula to Speed*Content.Type. The * operator indicates to R that we want main effects and interaction effects. Try it yourself to see that you get the same result.\nWe extract the treatment means as before. The grand mean is shown first. Now with a factorial treatment structure, we get the mean values for each level of the treatment factors included and the treatment means. In the output below, we see the means of the 1x and 2x speed followed by the means for the levels of content type. Lastly, the treatment means are presented in a 2 by 2 matrix format. The treatment “1x Audio Only” had a mean accuracy of 50.32, “2x Audio-Only” mean response is 47.8, and so on. And then lastly, the standard errors for differences between means.\nWe can also extract the estimated effects as before.\n\nmodel.tables(model, type = \"effects\", se = TRUE)\n\nTables of effects\n\n Speed \nSpeed\n    1     2 \n 2.16 -2.16 \n\n Content.Type \nContent.Type\n  Audio-Only Audio-Visual \n       -3.68         3.68 \n\n Speed:Content.Type \n     Content.Type\nSpeed Audio-Only Audio-Visual\n    1 -0.94       0.94       \n    2  0.94      -0.94       \n\nStandard errors of effects\n        Speed Content.Type Speed:Content.Type\n        1.490        1.490              2.108\nreplic.   100          100                 50\n\n\nFirst we get the main effects for Speed and Content.Type. Then we get the interaction effects and standard errors. Let’s check that we understand how these interaction effects are calculated. Remember:\n\\[(AB)_{ij} = \\bar{Y}_{ij} - (\\mu + A_i + B_j) \\]\nSo for treatment \\(i = 1\\) and \\(j=1\\), the equation becomes:\n\\[\\hat{(AB)}_{11} = \\bar{Y}_{11} - (\\hat{\\mu} + \\hat{A}_1 + \\hat{B}_1) \\]\nWe go by the dimensions of the matrix returned by R, so then treatment \\(i = 1\\) and \\(j=1\\) is “1x Audio-Only”. Substituting the estimated values:\n\\[\n\\begin{aligned}\n(AB)_{11} &= 50.32 - (52.78 + 2.16 -3.68) \\\\\n&= -0.94\n\\end{aligned}\n\\]\nWhich is what R outputs as well. Now, we want to ask is there evidence for an interaction effect? To do this we need to construct the ANOVA table.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Model for Factorial Experiments</span>"
    ]
  },
  {
    "objectID": "16_FE_ANOVA.html",
    "href": "16_FE_ANOVA.html",
    "title": "15  ANOVA",
    "section": "",
    "text": "ANOVA Table\nThe model for a factorial experiment with two treatment factors was:\n\\[\nY_{ijk} = \\mu + A_i + C_j + (AC)_{ij} + e_{ijk}\n\\]\nIf we move \\(\\mu\\) to the left-hand side of the equation, we get:\n\\[\nY_{ijk} - \\mu = A_i + C_j + (AC)_{ij} + e_{ijk}\n\\]\nNow, each of the terms on the RHS is a deviation from a mean.\nWe can square and sum the corresponding observed deviations and obtain sums of squares. For a balanced factorial experiment, the total sum of squares on the LHS can be split into four parts, corresponding to:\n\\[\nSS_{total} = SS_A + SS_C + SS_{AC} + SS_E\n\\]\nThe degrees of freedom for these sums of squares are:\n\\[\nabn - 1 = (a - 1) + (b - 1) + (a - 1)(b - 1) + ab(n - 1)\n\\]\nwhere \\(n\\) is the number of replicates per treatment. The degrees of freedom on the right-hand side add up to the total degrees of freedom. Once again, we summarise all this in a table.\nThe following table summarizes the partitioning of variation:\nThere are three F-tests in this ANOVA table.\nThe alternative hypothesis is, in each case, that at least one of the parameters considered is non-zero.\nWhile discussing interactions, we saw that sometimes, with strong interaction effects, the main effects of a factor may disappear (be close to zero). But this does not mean that the factor has no effect. On the contrary, it has an effect on the response; the effects just differ over the levels of the other factor and may average out.\nTherefore, we usually start by testing the interaction effects. If there is evidence for the presence of interactions, we have to examine the main effects with this in mind, i.e., be careful with the interpretation of the main effects. Some people say that it becomes meaningless to test for main effects if there is evidence of interactions. However, this depends on what we want to know. The main effects still tell us whether or not the average response changes with changing levels of the factor.\nThe F-ratio always has the mean square for error in the denominator. As before, it is a ratio of two variance estimates, and in each case, it can be seen as a signal-to-noise ratio: how large are the effects relative to the experimental error variance?",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "16_FE_ANOVA.html#back-to-the-example",
    "href": "16_FE_ANOVA.html#back-to-the-example",
    "title": "15  ANOVA",
    "section": "15.1 Back to the example",
    "text": "15.1 Back to the example\nBefore we inspect the ANOVA table for the working example, we need to check the assumptions about the errors after model fitting. We do this by inspecting the residuals.\n\npar(mfrow = c(1,2))\nplot(model, which = 1)\nplot(model, which = 2)\n\n\n\n\n\n\n\n\nThere are no clear violations, in the first plot, the residuals appear to be centered around zero and the spread is reasonably equal across groups. The second plot is a Q-Q plot of the residuals which shows nothing worrisome. Remember we can also plot a histogram of the residuals to check normality.\nFor the independence assumption, we construct the dot chart once again but with the residuals.\n\ndotchart(model$residuals) # note the different way of extracting residuals!\n\n\n\n\n\n\n\n\nThe y-axis is messy but we can ignore that, it shows the index of each observation and there are 200 hence why it overlaps so much. The residuals look uniform, there are no systematic patterns or trends in the plot.\nLet’s see what the ANOVA table looks like for our working example.\n\nsummary(model)\n\n                    Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nSpeed                1    933   933.1   4.201 0.041724 *  \nContent.Type         1   2708  2708.5  12.195 0.000592 ***\nSpeed:Content.Type   1    177   176.7   0.796 0.373485    \nResiduals          196  43532   222.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nVerify that the degrees of freedom are what you expected! First, we look at the interaction. The F-value is quite small which leads to a large p-value of 0.37. This means that we really have no evidence against the null hypothesis that the factors interact. There is some evidence for a main effect of Speed but there is much stronger evidence as indicated by the small p-value for a main effect of lecture modality.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "17_FE_Contrasts.html",
    "href": "17_FE_Contrasts.html",
    "title": "16  Contrasts",
    "section": "",
    "text": "16.1 Conclusion\nThere are two approaches to analysing data from experiments. The first is to construct a set of a-priori contrasts, test these, and perhaps afterwards use unplanned comparisons to see if there are any other interesting treatment effects or differences that we might want to follow up with in a future experiment.\nThe second approach is an analysis of variance (ANOVA). This usually tests much more general hypotheses about the presence of main and interaction effects. The two approaches are not mutually exclusive, but if the questions we are interested in are not answered by an analysis of variance, we should concentrate on the contrasts. The two approaches may also give what seem to be different answers.\nFor example, from the ANOVA F-test, we may see no evidence for interactions, but if we look at specific contrasts for interactions, there is evidence. This can happen; it is not a mistake in the methods, it is just a difference in the hypotheses that are being tested.\nOften, an ANOVA is expected in journal publications and research reports, even if it does not answer the specific research questions. The more specific questions are answered by constructing confidence intervals or tests for contrasts.\nLet’s revisit the specific research questions for the working example:\nWith these question, conducting an ANOVA is enough. We simply want to know if there are any main effects or interaction effects. We have answered that with the ANOVA above. But what if the questions were a bit more specific:\nSo far we have only contrasted two treatments. Sometimes we want to compare groups of treatments to one another. More generally, a contrast is defined as a linear combination of the parameters where the coefficients add up to zero:\n\\[L = \\sum_1^a h_iA_i\\]\nsuch that \\(\\sum_1^a h_i = 0\\). This ensures a fair comparison. For example, in a comparison of two group means we have:\n\\[ L = \\mu_1 - \\mu_2 = 1\\times \\mu + (-1) \\times \\mu_2\\] Here, the coefficients are \\(h_1 = +1\\) and \\(h_2 = -1\\) which sum to zero. This simple difference is the simplest form of a contrast. Effectively, \\(\\sum_1^a h_i = 0\\) represents the null hypothesis, that the difference equals 0.\nLet’s start with the first question. Remember the treatments were:\nTo answer the first question, our contrast should compare Audio-Visual vs. Audio-Only and we do this by averaging over the levels of playback speed.\nFirst we compute the average response for the two levels of content type, AV and AO.\n\\[\\frac{(\\mu_{1AV} + \\mu_{2AV})}{2}\\]\n\\[\\frac{(\\mu_{1AO} + \\mu_{2AO})}{2}\\]\nNow we are comparing groups of means. The first group contains the means for all treatments that included Audio-Visual level and the second contains the Audio-Only level. We are asking whether the AV level increased comprehension. So we are testing:\n\\[H_0: \\frac{(\\mu_{1AV} + \\mu_{2AV})}{2} = \\frac{(\\mu_{1AO} + \\mu_{2AO})}{2}\\]\n\\[\nH_1: \\frac{(\\mu_{1AV} + \\mu_{2AV})}{2} &gt; \\frac{(\\mu_{1AO} + \\mu_{2AO})}{2} &lt;=&gt; \\frac{(\\mu_{1AV} + \\mu_{2AV})}{2} - \\frac{(\\mu_{1AO} + \\mu_{2AO})}{2} &gt; 0\n\\]\nThe coefficients of the contrast sum to zero:\n\\[\n\\begin{aligned}\n&\\frac{(\\mu_{AV1} + \\mu_{AV2}) - (\\mu_{AO1} + \\mu_{AO2})}{2} \\\\\n&\\frac{(1) \\mu_{AV1} + (1) \\mu_{AV2} + (-1) \\mu_{AO1} + (-1) \\mu_{AO2}}{2} \\\\\n& (0.5) \\mu_{AV1} + (0.5) \\mu_{AV2} + (-0.5) \\mu_{AO1} + (-0.5) \\mu_{AO2} \\\\\n&0.5+.0.5-0.5-0.5 = 0\n\\end{aligned}\n\\] This is a linear combination of the model parameters. What does the contrast and coefficients look like for the second question? To test whether playback speed decreases comprehension, we need to compare treatments at 1x speed vs. 2x speed:\n\\[\\frac{(\\mu_{1AV} + \\mu_{1AO})}{2} - \\frac{(\\mu_{2AO} + \\mu_{2AV})}{2}\\] The coefficients sum to zero as before. This might be confusing but we are simply grouping treatments together and comparing them. To compute these contrasts in R, we first fit the model using lm() and extract the treatment means using emmeans from the emmeans package.\nThe emmeans function returns the treatment means, the standard error, degrees of freedom and the bounds of 95% confidence interval. Now we want to perform the two contrasts using the means saved in the object we created, means. First, note the order in which emmeans outputs the treatments:\nAO1, AV1, A02, AV2.\nWe are going to use this order and the coefficients were determined earlier to perform the ocntrasts with the function contrast() also from the package emmeans:\nWe supply the emmeans object means and then a list of contrasts we call c1 and c2 corresponding to the first and second question. Each contrast consists of the coefficients in the order in which the means appear in the means object and the scaling by 2. Then we need to specify by = NULL because we have manually coded the contrasts and don’t need to specify by which factor the contrasts should made. Lastly, we specify the type of test we want, that is, is it one sided or two sided. If it is one-sided, in which direction? We have specifically constructed the contrasts so that both are “one-sided greater than”.\nThe output shows the estimate of each contrast, the standard error of the difference in means, t-value and associated p-value. For the first contrast we see the difference in comprehension scores between the Audio-Visual and Audio-Only groups was 7.36, this means that the average response in the Audio-Visual group was higher than the average response in the Audio-Only group. We see that the p-value to test this contrast is 0.0003 which is extremely small, so it is unlikely that the difference in mean response is due to chance. There is strong evidence to indicate that the audio-visual type increased the mean response, the estimate of this the difference between groups is 7.36% (\\(t=3.492\\), \\(df = 196\\), \\(p = 0.0003\\)).\nFor the second contrast, the p-value still provides sufficient evidence against the null hypothesis that the difference is zero but it is not as strong as for the first contrast. However, we are still satisfied with the evidence against \\(H_0\\). The 2x speed decreased the average accuracy (averaged over the levels of content type) by 4.32% (\\(t=-2.050\\), \\(df = 196\\), \\(p = 0.021\\)).\nWhen we have factors with two levels (as we do here) and we conduct two sided contrasts, then the contrast is equivalent to testing for the presence of main effects which what the ANOVA table does! Remember we said that the ANOVA is an extension of the t-test and with two levels. Let’s go through this step-by-step:\nSince we conducted one-sided tests, the p-value is has been split between the tails. To get to the value of the p-value for a two-sided tests, we multiply the one-sided p-value by 2.\nCheck that these are the same as in the ANOVA table. The test statistics are also related in this case, \\(t^2 = F\\).\nLet’s answer the third question. Since we have two levels per factor, this question is asking about the interaction. The contrast for the interaction should compare the difference between audio-visual and audio-only in the two levels of playback speed:\nAt 1x playback speed, the effect of content type is given by:\n\\[\n(\\mu_{AV1} - \\mu_{AO1})\n\\]\nAt 2x playback speed, the effect of content type is given by:\n\\[\n(\\mu_{AV2} - \\mu_{AO2})\n\\]\nNow to examine whether the effect of content type is consistent across playback speeds, we compute:\n\\[\n\\begin{aligned}\n&(\\mu_{AV1} - \\mu_{AO1}) - (\\mu_{AV2} - \\mu_{AO2})\\\\\n& = \\mu_{AV1} - \\mu_{AO1} - \\mu_{AV2} + \\mu_{AO2}\n\\end{aligned}\n\\]\nThis contrast assesses whether the difference between Audio-Visual and Audio-Only is the same at 1x and 2x speeds.\nWe get the same p-value as in the ANOVA table which indicates a lack of evidence against the null hypothesis, there is no evidence to suggest that the two factors interact (\\(t=0.892\\), \\(df = 196\\), \\(p = 0.374\\)).\nIn practice, we would test the interaction first and then interpret the main effects if there is evidence to support their presence. Here we have done it this way around purely for educational purposes.\nWe can also visualise the interaction (especially useful for understanding the interaction if there is evidence for one!). There is a built-in function in R that can do this for us, but it will be useful to construct the plot from scratch to ensure you understand what it visualises.\nHave a look at the data set again.\nWe want to visualise the response per treatment for each combination of Speed and Content.Type (which is already combined in the column Condition). We did this with the emmeans function and stored the treatment means in the object means! To use it to visualise the treatment means we need to convert to a data frame, currently it is something called a “emmGrid”\nIt is easy to convert the object to a data frame:\nWe need to decide which factor will be on the x-axis, let’s do Speed. Below, I use a new package called ggplot2 to visualise the data. It creates nicer looking plots and is more intuitive in my opinion. If you want to see how to use base R to plot this, see the code at the end of this section.\nThe aes() function maps Speed to the x-axis, Mean Response to the y-axis, and uses Content Type for color and grouping. geom_point(size = 3) adds individual data points, while geom_line(size = 1) connects them to show trends. The labs() function provides axis labels and a title, and theme_minimal() specifies the theme for the plot.\nIt is evident that increasing Speed has a negative effect on the response, and switching from Audio-Visual to Audio-Only content reduces the mean response. When moving from 1x to 2x Speed in the Audio-Visual condition, the response decreases. A similar decline is observed for the Audio-Only condition. Although the decrease appears slightly larger for Audio-Visual than for Audio-Only, the difference is not substantial enough to conclude a significant interaction effect between Speed and Content Type as evidenced by the ANOVA and contrasts we did before.\nCode\n# Set up an empty plot\nplot(means_data$Speed[means_data$Content.Type == \"Audio-Visual\"], \n     means_data$emmean[means_data$Content.Type == \"Audio-Visual\"], \n     type = \"o\", \n     col = \"#F79256\", \n     pch = 16, \n     ylim = range(means_data$emmean), \n     xlab = \"Speed\", \n     ylab = \"Mean Response\", \n     main = \"Interaction Plot: Speed vs Content Type\",\n     xaxt = \"n\")\n\n# - plot(...) initializes the graph using Speed as the x-axis and Mean Response as the y-axis.\n# - The subset `means_data$Speed[means_data$ContentType == \"Audio-Visual\"]` extracts only Audio-Visual data to plot the first line.\n# - type = \"o\" specifies that both points and lines should be drawn.\n# - ylim = range(means_data$emmean) ensures that the y-axis spans the full range of data.\n# - xaxt = \"n\" suppresses the default x-axis, allowing for manual customization in the next step.\n# \n# Since the x-axis represents discrete categories (Speed levels), we manually specify the tick labels with the function `axis` and then we overlay the means for the Audio-Only groups with `points`. Lastly, we add a legend. \n\n# Add x-axis labels manually\naxis(1, at = unique(as.numeric(means_data$Speed)), labels = unique(means_data$Speed))\n\n# Add Audio-Only group\npoints(means_data$Speed[means_data$Content.Type == \"Audio-Only\"], \n       means_data$emmean[means_data$Content.Type == \"Audio-Only\"], \n       col = \"#5BC0EB\", \n       pch = 16, \n       type = \"o\")\n\n# Add legend\nlegend(\"topright\", legend = c(\"Audio-Visual\", \"Audio-Only\"), col = c(\"#F79256\", \"#5BC0EB\"), pch = 16, lty = 1)\n\n\nCode\n# OR WITH BUILT IN \n\ninteraction.plot(x.factor = means_data$Speed, #x-axis variable\n                 trace.factor = means_data$Content.Type, #variable for lines\n                 response = means_data$emmean, #y-axis variable\n                 fun = mean, #metric to plot\n                 ylab = \"Counts\",\n                 xlab = \"Seasons\",\n                 col = c(\"red\", \"blue\"),\n                 lty = 1, #line type\n                 lwd = 2, #line width\n                 trace.label = \"Species\")",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Contrasts</span>"
    ]
  },
  {
    "objectID": "00_ModelConcepts.html#steps-in-statistical-modelling",
    "href": "00_ModelConcepts.html#steps-in-statistical-modelling",
    "title": "Statistical Modelling",
    "section": "Steps in statistical modelling",
    "text": "Steps in statistical modelling\nStatistics is an integral part of the scientific method and should be involved in all parts of the scientific process. A summary of the steps involved in scientific research:\n\nInteresting scientific question.\nCome up with a number of working hypotheses.\nPlan / design study / experiment.\nData collection: field, experiment, observations.\nAnalysis: confront hypotheses with data.\nNew scientific questions, update working hypotheses.",
    "crumbs": [
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "00_ModelConcepts.html#what-is-a-model",
    "href": "00_ModelConcepts.html#what-is-a-model",
    "title": "Statistical Modelling",
    "section": "What is a Model?",
    "text": "What is a Model?\nA statistical model is a mathematical representation of how data is generated. It describes the relationship between observed data and underlying factors (parameters) while accounting for random variation. Suppose that we are interested in estimating the age of a tree from its stem diameter. To do this we need to know by how much the stem diameter increases per year. We could describe this relationship or process as follows:\n\\[D = \\alpha + \\beta \\times Age\\] describing a linear increase of diameter with age. Once we have a good idea of how fast diameter increases with age (β) we can predict diameter from age. The (mathematical) model above is a very simple representation of this process with only two parameters, the intercept and the growth rate.\nWith the chosen parameter values, diameter increases linearly with age. Of course, this model is not realistic except for special situations but it gives us powerful insights. In reality we don’t know \\(\\beta\\), but usually need to estimate it from data. Also, not every tree grows equally fast, because of environmental and individual differences between trees. We can accept that the above is a simple model for the average behaviour of a tree, but to capture variability between trees (because of variability between environmental conditions from tree to tree, variability between individual trees, measurement error), we add an error term.\n\\[D = \\alpha + \\beta \\times Age_i + e_i\\]\nThe response that we observe is then described by an average behaviour, but the actual observed value will vary around this average. To summarise, the statistical model has a stochastic component which captures variability in the response that cannot be explained by the deterministic part of the model. Another distinguishing feature of statistical modelling is that we obtain estimates of the parameter values from the data, e.g. by fitting a line to the observations, i.e. we learn from data.",
    "crumbs": [
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "18_ExtraExp.html",
    "href": "18_ExtraExp.html",
    "title": "18  ANOVA Model Explanation",
    "section": "",
    "text": "18.1 Step 1: The Purpose of Statistical Modeling\nA statistical model is used to describe how the data are generated. It breaks down the observed variability in the data into systematic effects (patterns we can explain) and random variation (unexplained noise). When we analyse experimental data, we are trying to explain the response variable (e.g., comprehension, plant growth, exam scores, blood pressure reduction) based on the factor we manipulated (e.g., playback speed, fertilizer type, teaching method, drug type).\nWe have a response Y and there is variation in the observations of this response - they aren’t all the same. Perhaps the variation that we see in our response is due to our manipulation of the conditions under which we measured the response (the treatments) or maybe it isn’t and it is random noise unrelated to our manipulation. Our goal is to compare the response between the treatment groups. Specifically we are going to ask if there any differences in the mean response between the treatments.\nLet’s apply this idea to a hypothetical example where we have three treatments. Below is a plot of the hypothetical data. It is clear to see that the values vary and we want to explain this variability.\nTo understand how variation in the response is structured, we begin with the simplest possible model. This model assumes that all observations come from a single overall mean, with only random variation around it. It assumes that every observation is just random noise around a single mean (i.e., all treatments are the same). The observations belong to a single mean with mean \\(\\mu\\). This would be:\n\\[Y_{ij} = \\mu + e_{ij}\\]\nHere, \\(\\mu\\) is the overall mean (mean of the entire response), and \\(e_{ij}\\) captures all random variation of observations around that mean.\nwhere the green line represents the overall mean, the mean of the entire response and the error is the difference between an observation and the green line.\nBut this ignores treatment groups entirely, it ignores the treatment structure in our data which is part of the data generating process (the experiment). In reality, our experiment was designed to test whether different treatments produce different responses, so we must extend our model to include treatment effects.",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ANOVA Model Explanation</span>"
    ]
  },
  {
    "objectID": "18_ExtraExp.html#step-2-introducing-the-treatment-effects",
    "href": "18_ExtraExp.html#step-2-introducing-the-treatment-effects",
    "title": "18  ANOVA Model Explanation",
    "section": "18.2 Step 2: Introducing the Treatment Effects",
    "text": "18.2 Step 2: Introducing the Treatment Effects\nNow, suppose we have \\(a\\) different treatments (e.g., four different playback speeds). If treatment factor did have an effect, then the mean response for all or some of the treatment groups should differ and we need to incorporate this into our model so we can test for any differences between means.\nInstead of assuming one overall mean for all observations, we recognize that each treatment has its own mean, say \\(\\mu_i\\) for treatment \\(i\\) which we define as a deviation from the overall mean by some specific treatment-effect \\(A_i\\):\n\\[\\mu_i = \\mu + A_i\\]\nwhere:\n\n\\(\\mu\\) is the grand mean (the average across all groups).\n\\(A_i\\) is the treatment effect for group \\(i\\) (i.e. how much the treatment deviates from the overall mean)\n\nLet’s visualise this for the hypothetical data. First we’re going to introduce the treatment variable to the x-axis to see the treatment structure clearly and then add the treatment means:\n\n\n\n\n\n\n\n\n\nThen in the plot below the treatment effects are visualised:\n\n\n\n\n\n\n\n\n\nThis means that for any individual observation in treatment \\(i\\), we write:\n\\[Y_{ij} = \\mu_i + e_{ij}\\]\nor equivalently,\n\\[Y_{ij} = \\mu + A_i + e_{ij}\\]\nUnder this model, each observation is decomposed into three components:\n\nA general level (overall mean, \\(\\mu\\))\nA treatment-specific effect (\\(A_i\\))\nRandom noise (\\(e_{ij}\\)), representing variation that is not explain by the treatment mean.\n\nThe reason we use the second parameterisation is because in that form it leads directly to sums of squares and to Analysis of Variance (ANOVA) which is the statistical test for testing the following hypotheses:\nNull hypothesis:\n\\(H0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu\\)\nIn words this hypothesis says that the treatment means are the same and they are equal to the overall mean. The response belongs to a single population with mean \\(\\mu\\). This is equivalent to the hypothesis:\n\\(H0: A_1 = A_2 = A_3 = A_4 = 0\\)\nwhich states that the treatment effects are zero, which reduces the model to:\n\\[Y_{ij} = \\mu + A_i + e_{ij}\\]\nAlternative hypothesis:\n\\(H1: \\text{at least one of the treatment means }(\\mu_i) \\text{ differ}\\)\nThere is a difference somewhere! Not that all treatment means are different. The response does not belong to single population, there is more than one population with a mean that deviates from the overall mean. Which equates to:\n\\(H1: \\text{At least one } A_i \\neq 0\\)\nAt least one of the treatment effects is not zero. At least one of the treatment means are different (deviate from the overall mean) and so we have evidence for a treatment effect. That is our manipulation, the treatments, induced a change in the response.",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ANOVA Model Explanation</span>"
    ]
  },
  {
    "objectID": "18_ExtraExp.html#conclusion",
    "href": "18_ExtraExp.html#conclusion",
    "title": "18  ANOVA Model Explanation",
    "section": "18.3 Conclusion",
    "text": "18.3 Conclusion\nEventually, we will be able to test these hypothesis but for now trust me when I say that with this data we would reject the null hypothesis. We can then further go a test for specific differences between means and we could find all three treatment means are different. What would the data look like if we weren’t going to reject the null hypothesis? Well we wouldn’t see those clear deviations from the overall mean, so something like this:\n\n\n\n\n\n\n\n\n\nIf we only plotted the response (without the treatments on the x-axis) it is clear that the simple model, where the observations belong to a single population with an overall mean \\(\\mu\\) is more plausible. It seems the treatment factor did not have effect. The treatment effects appear small, suggesting that the observed variation in the response could be due to random noise rather than systematic differences between treatments.",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>ANOVA Model Explanation</span>"
    ]
  },
  {
    "objectID": "19_AnovabyHand.html",
    "href": "19_AnovabyHand.html",
    "title": "19  Manual Calculation of One-Way ANOVA Table",
    "section": "",
    "text": "19.1 Example Data\nThis tutorial walks through the manual calculation of a One-Way ANOVA table step by step. The goal is to understand the decomposition of variance and statistical significance testing. It does not cover checking the assumptions or model fitting.\nWe analyse the effect of study environment on student performance. The dataset contains test scores from students studying in different environments. Below the data are shown in long format and in wide format. You should be able to use both formats.",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manual Calculation of One-Way ANOVA Table</span>"
    ]
  },
  {
    "objectID": "19_AnovabyHand.html#example-data",
    "href": "19_AnovabyHand.html#example-data",
    "title": "19  Manual Calculation of One-Way ANOVA Table",
    "section": "",
    "text": "Table 19.1: Data in two formats.\n\n\n\n\n\n\n\n(a) Long Format\n\n\n\n\n\nGroup\nScore\n\n\n\n\nLibrary\n78\n\n\nLibrary\n80\n\n\nLibrary\n75\n\n\nLibrary\n82\n\n\nLibrary\n77\n\n\nCoffeeShop\n70\n\n\nCoffeeShop\n74\n\n\nCoffeeShop\n72\n\n\nCoffeeShop\n69\n\n\nCoffeeShop\n73\n\n\nHome\n82\n\n\nHome\n85\n\n\nHome\n79\n\n\nHome\n81\n\n\nHome\n80\n\n\n\n\n\n\n\n\n\n\n\n(b) Wide Format\n\n\n\n\n\nID\nCoffeeShop\nHome\nLibrary\n\n\n\n\n1\n70\n82\n78\n\n\n2\n74\n85\n80\n\n\n3\n72\n79\n75\n\n\n4\n69\n81\n82\n\n\n5\n73\n80\n77",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manual Calculation of One-Way ANOVA Table</span>"
    ]
  },
  {
    "objectID": "19_AnovabyHand.html#step-1-compute-the-overall-mean",
    "href": "19_AnovabyHand.html#step-1-compute-the-overall-mean",
    "title": "19  Manual Calculation of One-Way ANOVA Table",
    "section": "19.2 Step 1: Compute the Overall Mean",
    "text": "19.2 Step 1: Compute the Overall Mean\n\\[\n\\bar{Y}_{..} = \\frac{\\sum Y_{ij}}{N}\n\\] where \\(Y_{ij}\\) are individual observations and \\(N\\) is the total number of observations.\n\\[\n\\bar{Y}_{..} = \\frac{78+80+75+82+77+70+74+72+69+73+82+85+79+81+80}{15} = 77.07\n\\]",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manual Calculation of One-Way ANOVA Table</span>"
    ]
  },
  {
    "objectID": "19_AnovabyHand.html#step-2-compute-the-treatment-means",
    "href": "19_AnovabyHand.html#step-2-compute-the-treatment-means",
    "title": "19  Manual Calculation of One-Way ANOVA Table",
    "section": "19.3 Step 2: Compute the Treatment Means",
    "text": "19.3 Step 2: Compute the Treatment Means\nFor each group: \\[\n\\bar{Y}_{i.} = \\frac{\\sum Y_{ij}}{n_i}\n\\]\nwhere \\(n_i\\) is the number of replicates per treatment.\nFor Library: \\[\n\\bar{Y}_1 = \\frac{78+80+75+82+77}{5} = 78.4\n\\]\nFor Coffee Shop: \\[\n\\bar{Y}_2 = \\frac{70+74+72+69+73}{5} = 71.6\n\\]\nFor Home: \\[\n\\bar{Y}_3 = \\frac{82+85+79+81+80}{5} = 81.4\n\\]",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manual Calculation of One-Way ANOVA Table</span>"
    ]
  },
  {
    "objectID": "19_AnovabyHand.html#step-3-compute-sum-of-squares",
    "href": "19_AnovabyHand.html#step-3-compute-sum-of-squares",
    "title": "19  Manual Calculation of One-Way ANOVA Table",
    "section": "19.4 Step 3: Compute Sum of Squares",
    "text": "19.4 Step 3: Compute Sum of Squares\nTreatment Sum of Squares (SSB or sometimes $SS_T$)\n\\[\nSSB = r \\sum (\\bar{Y}_{i.} - \\bar{Y}_{..})^2\n\\]\n\\[\nSSB = 5 [(78.4 - 77.07)^2 + (71.6 - 77.07)^2 + (81.4 - 77.07)^2]\n\\]\n\\[\nSSB = 5 [(1.33)^2 + (-5.47)^2 + (4.33)^2] = 5 [1.77 + 29.91 + 18.75] = 5(50.43) = 252.15\n\\]\nTotal Sum of Squares (SSTotal)\n\\[\nSST = \\sum (Y_{ij} - \\bar{Y}_{..})^2\n\\]\nComputing each squared difference:\n\\[\n(78-77.07)^2 + (80-77.07)^2 + (75-77.07)^2 + (82-77.07)^2 + (77-77.07)^2 +\n\\]\n\\[\n(70-77.07)^2 + (74-77.07)^2 + (72-77.07)^2 + (69-77.07)^2 + (73-77.07)^2 +\n\\]\n\\[\n(82-77.07)^2 + (85-77.07)^2 + (79-77.07)^2 + (81-77.07)^2 + (80-77.07)^2\n\\]\nSumming these values:\n\\[\n(0.86 + 8.56 + 4.29 + 24.29 + 0.005) + (49.98 + 9.43 + 26.17 + 65.17 + 16.56) + (24.29 + 63.17 + 4.17 + 15.48 + 8.56) = 321.96\n\\]\nError Sum of Squares (SSE)\n\\[\nSS_E = SST - SSB = 321.96 - 252.15 = 69.81\n\\]",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manual Calculation of One-Way ANOVA Table</span>"
    ]
  },
  {
    "objectID": "19_AnovabyHand.html#step-4-compute-mean-squares",
    "href": "19_AnovabyHand.html#step-4-compute-mean-squares",
    "title": "19  Manual Calculation of One-Way ANOVA Table",
    "section": "19.5 Step 4: Compute Mean Squares",
    "text": "19.5 Step 4: Compute Mean Squares\nTreatment Mean Square (MST/MSB)\n\\[\nMS_T = \\frac{SS_T}{df_T} = \\frac{252.15}{2} = 126.08\n\\]\nError Mean Square (MSE)\n\\[\nMS_E = \\frac{SS_E}{df_E} = \\frac{69.81}{12} = 5.82\n\\]",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manual Calculation of One-Way ANOVA Table</span>"
    ]
  },
  {
    "objectID": "19_AnovabyHand.html#step-5-compute-f-statistic",
    "href": "19_AnovabyHand.html#step-5-compute-f-statistic",
    "title": "19  Manual Calculation of One-Way ANOVA Table",
    "section": "19.6 Step 5: Compute F-statistic",
    "text": "19.6 Step 5: Compute F-statistic\n\\[\nF = \\frac{MS_T}{MS_E} = \\frac{126.08}{5.82} = 21.67\n\\]\nCompare \\(F\\) to the critical value from an F-distribution table or get p-value. The null distribution of this F test statistic is:\n\\[F\\sim F_{df1,df2} = F_{2,12}\\]\nUsing tables:\n\n\nCritical value approach (using the F-tables that are under the main Resources tab in the folder Statistical tables and formulae)\n\nGo along the column (numerator df) and look for 2. Then go down the rows and stop at the row for 12 denominator degrees of freedom (this is on the third page of F-tables and the section is highlighted in green above).\nAt 5% significance level, the critical value is 3.89. Our test statistic of 21.67 is much larger than 3.89. We do no have evidence against H0.\n\nP-value\n\nFocusing in on the same set of F-values on the table (so 2 numerator df and 12 denominator df), we see that there 4 F-values - each corresponding to a different probability in the upper tail. The last value is 6.93 which is associated with an area to the right of 0.01. In other words, the probability of an F-value of 6.93 or more extreme under the null hypothesis is 0.01. Our test statistic is much larger than this value, so we can deduce that the p-value (the probability of observing our test statistic or more extreme under the null hypothesis) is smaller than 0.01.\nIn R\n\nCritical value\n\nThis line of code gives us the critical F value for a significance value of 0.05.\n\nqf(0.05,2,12,lower.tail = FALSE)\n\n[1] 3.885294\n\n\n\nP-value\n\nUsing R we can get an exact p-value and we see that it is indeed smaller than 0.01.\n\npf(21.67, 2, 12, lower.tail = FALSE)\n\n[1] 0.0001039567",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manual Calculation of One-Way ANOVA Table</span>"
    ]
  },
  {
    "objectID": "19_AnovabyHand.html#step-6-interpretation",
    "href": "19_AnovabyHand.html#step-6-interpretation",
    "title": "19  Manual Calculation of One-Way ANOVA Table",
    "section": "19.7 Step 6: Interpretation",
    "text": "19.7 Step 6: Interpretation\nThere is strong evidence against the null hypothesis of equal means (F = 21.67, p-value = 0.0001). We conclude that the treatment (study environment) did result in at least one different mean score between the different environments.\nRemember, the alternative is not that all means are different. Only that there is a difference somewhere.",
    "crumbs": [
      "Extra resources",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manual Calculation of One-Way ANOVA Table</span>"
    ]
  },
  {
    "objectID": "13_FE_Intro.html#is-comprehension-affected-by-playback-speed-and-lecture-modality",
    "href": "13_FE_Intro.html#is-comprehension-affected-by-playback-speed-and-lecture-modality",
    "title": "13  Introduction",
    "section": "13.4 Is comprehension affected by playback speed and lecture modality?",
    "text": "13.4 Is comprehension affected by playback speed and lecture modality?\nIn keeping with the theme of students, learning and teaching. Have you ever wondered whether playback speed affects your comprehension of a lecture? Or whether your comprehension is better with audio-only lectures such as podcast versus recorded lectures with visuals? What about if you listen to a podcast at double speed versus a recorded lecture at double speed, is there difference in comprehension? To answer this question, researchers from the University of California conducted a \\(2\\times2\\) factorial experiment.\n\n\n\n\n\n\nLecture modality and playback speed\n\n\n\nChen et al. (2024) conducted an experiment to find out whether visual information improves comprehension when lectures are played at faster speeds. Specifically, they wanted to investigate the effect of lecture modality (audio-only or audio-visual) and playback speed (1x or 2x) on comprehension of students and whether these factors interact. We can summarise the research questions as follows:\n\nDoes lecture modality have an effect on comprehension?\nDoes playback speed have an effect on comprehension?\nIs there an interaction effect of modality and playback speed on comprehension?\n\nA total of 200 undergraduate students were randomly assigned to one of four groups:\n\nAudio-only at normal speed (1x)\nAudio-visual (with slides) at normal speed (1x)\nAudio-only at double speed (2x)\nAudio-visual (with slides) at double speed (2x)\n\nThe researchers chose two lectures: one about about real estate appraisals and another bout the history of the Roman Empire. The lectures were either presented as audio-visual clips which consisted of presentation slides and instructor images, and no subtitles or captions were provided. All the graphics (maps, figures) in the slides were static. For lectures presented as audio-only clips, only the instructor’s audio was made available.\nEach student was presented both lectures in the modality and speed they were assigned. Afterwards, they completed a comprehension test consisting of 25 multiple choice questions on each topic. The average of the scores was taken as the final measure of comprehension.\n\n\n\n\nWe will be using the actual data collected but we will only be using a subset of the information they recorded. The authors conducted a different analysis which incorporates this extra information. We will not be doing this as the method they used is outside th scope of this course.\nRight! Let’s begin with identifying the design. It should be clear that we have two treatment factors: lecture modality and playback speed each with the treatment levels. this means that we have a total of \\(2 \\times 2 = 4\\) treatments which are the combinations of the treatment levels. They investigated the effect of these factors on the comprehension of students - that means, comprehension is the response.\n\nResponse Variable: Comprehension\n\nTreatment Factors: Lecture modality and playback speed\n\nTreatment Levels: Lecture modality: Audio-only or Audio-visual; Playback speed: 1x or 2x\n\nTreatments: Audio-only at normal speed (1x); Audio-visual at normal speed (1x); Audio-only at double speed (2x); Audio-visual at double speed (2x)\n\nEach student was assigned to one the treatments indicating that students were the experimental units. The response was also measured on each student, they are the observational units as well. Therefore, since we had 200 students and 4 treatment groups, there was 50 students per group, the experiment had 50 replicates.\n\nExperimental Unit: Student (200)\n\nObservational Unit: Student (200)\n\nReplicates: 50 students per group\n\n\nLastly, we need to determine how randomisation was conducted. There is no indication of any blocking and treatments were randomised to the whole group of experimental units. So this is a Completely Randomised Design, specifically it is a \\(2\\times2\\) factorial CRD.\n\nDesign Type: \\(2\\times2\\) factorial Completely Randomized Design (CRD)\n\nBefore we do any further analysis, we need to talk a bit about effects!\n\n\n\n\nChen, Ashley, Suchita E Kumar, Rhea Varkhedi, and Dillon H Murphy. 2024. “The Effect of Playback Speed and Distractions on the Comprehension of Audio and Audio-Visual Materials.” Educational Psychology Review 36 (3): 79.",
    "crumbs": [
      "Factorial Experiments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction</span>"
    ]
  }
]