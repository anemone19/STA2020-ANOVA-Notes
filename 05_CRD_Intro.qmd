
# Introduction 

Completely Randomized Designs (CRDs) are the simplest experimental designs. They are used when experimental units are uniform enough. We expect them to react simirlary to a given treatment; we have no reason to suspect that a group of experimental units might react differently to the treatments. We also don't expect any effects (besides possibly a treatment effect) to cause any systematic changes in the response. In other words, we don't have to block for nuisance factors. 

Remember experimental design is the procedure for how experimental units are grouped and treatments are applied. We have already said that there are no blocks in CRDs. So randomisation occurs without restriction and to all experimental units. More generally, the $a$ treatments are randomly assigned to $r$ experimental units, such that each experimental unit is equally likely to receive any of the treatments. This means that there are $N = r \times a$ experimentnal units in total. We only consider designs that are *balanced* meaning that there an equal number of experimental units per treatment, i.e. a treatment is applied to $r$ units. 


## Example: The effect of social media multitasking on classroom performance.

As a student, I used to believe I could multitask effectively. I would scroll through my phone during lectures, study while texting friends, or listen to podcasts while driving. It felt like I was paying attention to everything, but in hindsight, I can barely recall the details of those podcasts. I often had to revisit lectures or restart study sessions because my focus wasn’t truly there.

This tendency extends beyond student life. In the average workplace, tasks are frequently interrupted by social media, email checks, or notifications. Many of us feel the constant pull of our phones when trying to concentrate, whether we’re working, studying, or even relaxing.

In an era of perceived multitasking, where devices and distractions dominate our attention, it’s worth asking: how does social media multitasking impact academic performance?

::: {#warning-example1 .callout-warning icon="false"}

## Example 5.1 

Two researchers from Turkey, @multitask2018, conducted a study to try and answer this question. Specifically, they examined the impact of social media multitasking during live lectures on students' academic performance. 

A total of 120 undergraduate students were randomly assigned to one of three groups:

1. **Control Group:** Students used traditional pen-and-paper note-taking.
2. **Experimental Group 1 (Exp 1):** Students engaged in SMS texting during the lecture.
3. **Experimental Group 2 (Exp 2):** Students used Facebook during the lecture.

Over a three-week period, participants attended the same lectures on Microsoft Excel. To measure academic performance, a standardised test was administered.
::: 

**The analysis of experimental data is determined by the design.** The design dictates the terms that we will include in our statistical model and so it is crucial to be able to identify the design and all factors included (blocking and treatment). It is also important to check that randomisation has been done correctly and determine the number of replicates used. In the previous chapter we started doing this by creating a summary of the design and we do the same here. From the description of the study, it is clear that:

- **Response Variable:** Academic performance, as measured by test scores.
- **Treatment Factor:** Level of social media multitasking.
- **Treatment Levels (Groups):** Control, Exp 1, and Exp 2.

Students were randomly assigned to one of the three groups, and performance was measured for each individual. Although this may seem obvious, they only took one measurement per student, so we don't have to worry about pseudoreplication. This setup indicates that the students are both the experimental units and the observational units in this study. With a total of 120 experimental units and three treatments, the experiment has 40 replicates. Since only one treatment factor was investigated, and no blocking was performed, this is classified as a **single-factor Completely Randomized Design (CRD).** Here is the study breakdown: 

- **Response Variable:** Academic Performance  
- **Treatment Factor:** Level of Social Media Multitasking  
- **Treatment Levels:** Control, Experimental 1 (SMS), Experimental 2 (Facebook)  
- **Treatments:** Control, Experiment 1, Experiment 2  
- **Experimental Unit:** Student (120)  
- **Observational Unit:** Student (120)  
- **Replicates:** 40 students per group  
- **Design Type:** Single-Factor Completely Randomized Design (CRD)  

Before we continue, now is the time to note that we won't be using the real data collected in this experiment. It wasn't available but I have simulated data to match their results. I've also made some other modifications such as the original study included 122 students but to ensure a balanced design I include only 120. 


## Exploratory data analysis

Before we start any analyses, two things need to be done. First, we start by exploring our data to get familar with the format and to get a feel for any patterns. In R, we read in the data set and then use some commands to inspect the dataset: 

```{r eda}

multitask <- read.csv("multitask_performance.csv")
nrow(multitask) # check number of rows
head(multitask) # check first 5 rows 
tail(multitask) # check last 5 rows 

summary(multitask)

```

The dataset consits of 120 rows (each row representing a student) and two columns (`Group` and `Posttest`). The first column, `Groups`, contains the treatment the student was assigned and the `Posttest` column contains the response measure. Using the functions `head` and `tail`, we can look at the first and last 5 rows and the function `summary` provies us with a descrption of each column. We do this to check that R has read in our data correctly (you can view the whole data set by running `view(multitask)` as well). The summary tells us that the `Group` column is of the class "character". For our analysis, we want it to be read as a factor: 

```{r factor}

multitask$Group <- as.factor(multitask$Group)
summary(multitask)
```
Now, we can see that there are 40 replicates per treatment group, confirming that the experiment is balanced. I have assumed that, based on the results shown, that the `Posttest` scores were recorded as percentages and using the summary we can quickly checked whether there are any observations that are not on the appropriate scale or might be outliers. Looks good so far! 

##  Model checking 

@multitask2018 had several research questions, but here we only consider the following: Are there any signficant differences in mean academic performance between the three groups? 

You might think that we could perform three t-tests (Control vs Exp 1, Control vs Exp 3, Exp 1 vs Exp 2). We could, but the problem with this approach is what we call mutliple testing. When conducting many tests, there is an increased risk of making a Type 1 Error (rejecting the null hypothesis when it is in fact true) [^1]. 

[^1]: Can't remember what a $t$-test is and/or need a refresher on hypothesis testing? Have a look <a href="https://www.youtube.com/watch?v=VekJxtk4BYM" target="_blank">this video on t-tests</a>
and document for a brief reminder. **Also, a quick (and cool) sidenote:** This study by @chen2024effect used a Completely Randomized Design (CRD), randomly assigning undergraduate students to playback speed groups (1x, 1.5x, 2x, and 2.5x) to measure the effect on comprehension of recorded lectures. Using ANOVA they found that comprehension was preserved up to 2x speed. I personally like to increase the playback speed to 1.5px if I just need to revise something quickly. 

When we have more than two groups, we can use a one-way analysis of variance (ANOVA) which can be seen as an extention of $t$-test and is called "one-way" because there is a single factor being considered. For both of these statistical approaches, the data should meet certain distributional assumptions: 

1. There are no outliers.
2. The errors are independent. 
3. The errors are normally distributed. 
4. All groups have equal population variances.

insert formal vs informal techniques 

### Outliers {.unnumbered}

Outliers are unusual observations (response values) that deviate substantially from the remaining data points. They can have a large influence on the estimates of our model. Think of statistics such as means and variances, outlying observations will shift the mean towards them and distort the variability of the data. 

If we're luckly, outliers are artefacts of data recording/entering issues, such as a missing decimal points or incorrect scaling (called error outliers). These types of outliers can be corrected and the analysis can be done as usual. If, however, there are freak observations that are not clearly due to anything like data inputting, then they are likely unusual responses and should not be discarded (interesting outliers). There are many ways of identifying and dealing with outliers (@outlier found 29 different ways in the literature). Here, it is recommended that the analyses should be run with and without the outliers to see whether the conclusion depends on their inclusion. When dealing with outliers, it is best to be transparent and clear about how they were handled. Simply removing outliers with no explanation is questionable research practice. 

A good way to check for outliers, is to inspect the data visually with a boxplot of your data grouped by treatment. 

```{r plot}

boxplot(Posttest ~ Group, data = multitask, col = c("skyblue", "lightgreen", "pink"), 
        main = "Posttest Scores by Group", 
        xlab = "Group", 
        ylab = "Posttest Scores")

stripchart(Posttest~Group, data = multitask, vertical = TRUE, add = TRUE, method = "jitter")

```

The first line of code plots the boxplot and by inputting `Posttest~Groups` as the first argument we are say plot the values of `Posttest` by `Groups`. There are extra graphical parameters specified to make the plot look a bit nicer. The function `stripchart` is used to overlay the data points. Based on these plots, there aren't any obvious outlying observations. 

### Equal population variances {.unnumbered}

Since we only have sample data, we would not expect that the variances to be the same, so just because they are different does not mean the population variances are different. We expect them to differ a bit due to chance simply because we are sampling from a population and everytime we take a sample, the dataset will be different. The sample variances need to be similar enough so that our assumption of equal population variances is reasonable. To check this assumption, we can inspect the boxplots again and compare the heights. More specifically, we look at the interquartile ranges (IQR). From looking at the plot, the IQRs do not vary widely. If you prefer to look at the actual values, we can use R to obtain them: 

```{r}
sort(tapply(multitask$Posttest,multitask$Group,IQR))
```

Another measure of variability we can look at, are the standard deviations (sd's). With the same line of code but just replacing the function we want to apply, we obtain the sd's of each group: 

```{r}
sort(tapply(multitask$Posttest,multitask$Group,sd))
```

The rule of thumb is to use the ratio of the smallest to largest standard deviation and check whether it is smaller than five. In our case, the smallest sd (of the Control group) is about 1.5 times smaller than the largest sd (of the Exp 2 group) which is acceptable. 


### Normally distributed errors {.unnumbered}


### Independent erros {.unnumbered}

In statistics, if one observation influences another in some way or another, they are said to be dependent. For the type of data considered here, there are two types of independence we require. Firstly, observations within treatments should be independent and second, observations between samples should be independent. Another way of saying this, is **there should independence within and among groups.** Depending on the direction of any violations, the within treatment variance or among treatment variance can either be deflated or inflated and treatment effects can be biased. This has considerable impact on the test statistic (F-ratio for ANOVAs, more on this later) which could lead to misleading results. [^2] 

Violations of independence typically occur when the experimental units within or among treatments are connected in some way. One way in which dependence between observations within a sample occurs, is if they are taken in a non-random sequence. Doing so typically allows some other variable to introduce dependence between successive observations. For example, measurement drift (when a tool's reading gradually changes over time), physical effects of the location of experimental units or the experimenter might become better (or worse) at taking the measurement as they move along. If these variables are not taken into account (by including them as factors in the model), it leads to a lack of independence in the errors of our model. Specifically, they lead to autocorrelated residuals; observations made closer together in time or space are more similar to each other than expected. 

Dependence bewteen treatments can occur if we apply the treatments to the same group of experimental units or experimental units being able to interact in some way during treatment. These types of violations including those mentioned above, are ones that we can avoid or control by properly designing experiments so that we can either avoid dependence or account for it as a factor in our model. 

Other sources of dependence may not be as obvious or easy to eliminate as we will see below. In the end, they may not have a strong impact on our estimates but it is important to carefully scrutinise your design and the system you are studying to identify possible sources of dependence so that these can be either addressed or dealt with properly. 

[^2]: @underwood_1996 has a very detailed explanation of the independence assumption (and the others) in the context of ANOVA. The book is for ecological experiments, but much of it pertains to all types of experiments.

In our example, within and among group dependence could be casued by the students interacting or influencing each other in some way (by sharing notes for example). During the lectures, this can be controlled by careful monitoring and randomising their position in the lecture theathre, but otuside of lectures, it is less easy to control. Here we can argue that if students interacted outside of lectures the impact on their academic performance (as neasured by the test) would likely be neglibile. Here the integrity of the students is at play. 

An informal check we could do is the plot the data in order in which they were taken (if this information is available) whether that is temporally or spatially to see if any patterns emerge. To do this in R, we can plot a Cleveland dot plot. 

```{r}
dotchart(multitask$Posttest, ylab = "Order of observation", xlab ="Post treatment test score")
```

We have assumed that the order in which the observations appear in the dataset are the order in which they were recorded. If there were any factors that caused sytematic trends, (i.e. dependence) in the observations, then there would be some kind of pattern in the dot chart. For our example, there is no clear pattern. After fitting the model, we can also plot the residuals against spatial coordinate or againt order to check for obvious patterns. This method only detects violations of independence if observations are related to time or space. 

Other types of dependence are more difficult to detect and require careful consideration of the experimental design and how data was collected. It is the onus of the experimenter to design and conduct experiments that ensure independence. With more thought (and if we're lucky, funding) all well-designed experiments lead should to independent data. If violations are found after the fact, they cannot typically be corrected and then methods that deal specifically with dependent data (if appropriate) should be used. 

In this course, you will always encounter data that has already been collected and the description of the experiment will likely not be as exhaustive. You might be task then with thinking about how the assumption of independence could have been violated, but for the most part we will assume the data are independent, both within and among samples (unless otherwise stated). 

TIPS: 

- increase focus, improve studying 
