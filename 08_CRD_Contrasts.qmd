```{r setup, echo = FALSE}
multitask <- read.csv("Datasets/multitask_performance.csv")
multitask$Group <- as.factor(multitask$Group)
m1 <- aov(Posttest~Group, data = multitask)
```

# Contrasts

The aim in many experiments is to compare treatments. To do this we contrast one group of means with another, i.e. we compare means, or groups of means, to see if treatments differ, and by how much they differ. A comparison of (groups of) treatments is called a contrast. If the experiment has been conducted as a result of specific research hypotheses, these will already define the contrasts we should construct first.

For single-factor completely randomised designs with only two treatments, we could conduct a t-test to compare the two means or construct a confidence interval to estimate the difference. But we know that with more than two treatments, we encounter problems of multiple testing. How do we contrast treatments when we have a factor with more than two levels? 

## Contrasting pairs of treatment means {.unnumbered}

We wrote the ANOVA model as: 

$$Y_{ij} = \mu + A_i + e_{ij}$$

with overall mean and the treatment effects as the parameters (as well as the error variance). Because the effects are constrained to sum to zero, i.e. $\sum_i^a A_i = 0$ we call this ANOVA model the *sum-to-zero* parameterisation. 

The above parameterisation is useful for constructing ANOVA tables. For estimating differences between treatments, however, a different parameterisation is more useful:

$$Y_{ij} = A_i + e_{ij}$$ 
In this version, we no longer have the overall mean as a parameter but use only the treatment effects αi. Remember that any model ultimately needs to describe the treatment means. There are a number of different ways in which to do this. One is the so-called *treatment contrast* parameterisation, which R uses as default for regression models. In this parameterisation, $A_1$ estimates the mean of the baseline treatment (by default, R orders the treatments alphabetically and takes the first one as baseline). The other parameters then estimate the difference between each treatment and the baseline treatment: $A_2$ estimates the difference between the second and the first treatment, $A_3$ estimates the difference between the third and the first, etc. 

To get a better understanding of this, let's fit the model to the social media data with this parameterisation. In R, this is done by using the `lm` function.


```{r}

m1.tc <- lm(Posttest ~ Group, data = multitask)
summary(m1.tc)

```


This is exactly the same output as you have seen before in the regression section! The intercept measures the mean of the baseline treatment (here it is the Control group). The next estimate `GroupExp1` is the difference between the mean of Experiment 1 and the mean of the Control group. Simirlarly, the last one is the difference between the mean of the Control Group and that of Experiment 2. You cn verify this by using the mean estimates we obatin when we fit the model previously: 

```{r}
model.tables(m1, type = "means")

62.88 - 75.63
52.24 - 75.63

```
Why is this useful? Now, we can formally test whether these differences are statistically significant using a hypothesis test!

Think back to regression—what was the null hypothesis for the coefficients in the output?

It was:

$$\beta_i = 0$$. 

The same principle applies here. We test whether the treatment effects ($A_i$) are equal to zero:

$$H_0: A_i = 0$$ 

Since we are interested in testing differences between groups, and the control group serves as the baseline, we are specifically testing:


$$
\begin{aligned}
H_0: A_2 &= 0 \\ 
H_0: A_3 &= 0
\end{aligned}
$$ 

This is the test that R conducts in the output above. It tests, for the last two parameters, the hypothesis that the difference between Experiment 1 and the Control is zero an that the difference between Experiment 2 and the Control is zero. In both cases, the p-values are extremely small which suggets that there are differences (the effects are not equal to zero). What about the intercept? This is testing that the mean of the Control group is zero. So, it tests whether the students in the control group scored zero on average. This doesn't really make sense and it is not a useful test. So not all tests that R carries out are necessarily useful or informative! Very often testing whether the intercept is different from zero is not interesting. 

What if we are intersted in the contrast R perform by default? but we wanted to know whetehr there is a difference between the other two groups? We simply need to change the baseline treatment that R uses and we can do this easily using the `relevel` command: 

```{r}

m1.tc <- lm(Posttest ~ relevel(Group, ref ="Exp1"), data = multitask)
summary(m1.tc)

```
Notice that the residual standard error, F-value and other statistics at the end of the output are exactly the same as for Model m1.tc above. The two models are equivalent and provide the same fit to the data. The only difference is that the parameters have different interpretations.



