
# Introduction 

Completely Randomized Designs (CRDs) are the simplest experimental designs. They are used when experimental units are uniform enough. We expect them to react simirlary to a given treatment; we have no reason to suspect that a group of experimental units might react differently to the treatments. We also don't expect any effects (besides possibly a treatment effect) to cause any systematic changes in the response. In other words, we don't have to block for nuisance factors. 

Remember experimental design is the procedure for how experimental units are grouped and treatments are applied. We have already said that there are no blocks in CRDs. So randomisation occurs without restriction and to all experimental units. More generally, the $a$ treatments are randomly assigned to $r$ experimental units, such that each experimental unit is equally likely to receive any of the treatments. This means that there are $N = r \times a$ experimentnal units in total. We only consider designs that are *balanced* meaning that there an equal number of experimental units per treatment, i.e. a treatment is applied to $r$ units. 


## Example: The effect of social media multitasking on classroom performance.

Can we really multitask? I remember as a student I thought I could multitask in lectures, while studying or driving and listening to a podcast. It felt like I was paying attention but in hindsight I can't remember those podcasts well, I know I had to revisit lectures and restart studying sessions. This extends beyond student life, where in the average workspace, tasks are interspersed with social media or email checks and notifcations (). I think most of us are almost always a little bit tempted by our cellphones when we study or work. 

So, if we live in an age of perceived multitasking and getting distracted by our phones and devices, what are the efffects of social media multitasking on our academic performance?  

::: {#warning-example1 .callout-warning icon="false"}

## Example 5.1 

Two researchers from Turkey, @multitask2018, conducted a study to investigate this question. Specifically, they examined the impact of social media multitasking during live lectures on students' academic performance. 

A total of 120 undergraduate students were randomly assigned to one of three groups:

1. **Control Group:** Students used traditional pen-and-paper note-taking.
2. **Experimental Group 1 (Exp 1):** Students engaged in SMS texting during the lecture.
3. **Experimental Group 2 (Exp 2):** Students used Facebook during the lecture.

Over a three-week period, participants attended lectures on Microsoft Excel. Pre-tests and post-tests were administered to measure learning outcomes.

::: 

**The analysis of experimental data is determined by the design.** The design dictates the terms that we will include in our statistical model and so it is crucial to be able to identify the design and blocking and treatment factors. It is also important to check that randomisation has been done correctly and determine the number of replicates used. In the previous chapter we started doing this by creating a summary of the design and we do the same here. From the description of the study, it is clear that:

- **Response Variable:** Academic performance, as measured by test scores.
- **Treatment Factor:** Level of social media multitasking.
- **Treatment Levels (Groups):** Control, Exp 1, and Exp 2.

Students were randomly assigned to one of the three groups, and performance was measured for each individual. Although this may seem obvious, they only took one measurement per student, so we don't have to worry about pseudoreplication. This setup indicates that the students are both the experimental units and the observational units in this study. With a total of 120 experimental units and three treatments, the experiment has 40 replicates. Since only one treatment factor was investigated, and no blocking was performed, this is classified as a **single-factor Completely Randomized Design (CRD).** Here is the study breakdown: 

- **Response Variable:** Academic Performance  
- **Treatment Factor:** Level of Social Media Multitasking  
- **Treatment Levels:** Control, Experimental 1 (SMS), Experimental 2 (Facebook)  
- **Treatments:** Control, SMS multitasking, Facebook multitasking  
- **Experimental Unit:** Student (120)  
- **Observational Unit:** Student (120)  
- **Replicates:** 40 students per group  
- **Design Type:** Single-Factor Completely Randomized Design (CRD)  

Before we continue, now is the time to note that we won't be using the real data collected in this experiment. It wasn't available but I have simulated data to match their results. I've also made some other modifications such as the original study included 122 students but to ensure a balanced design I include only 120. 


## Exploratory data analysis

Before we start any analyses, two things need to be done. First, we start by exploring our data to get familar with the format and to get a feel for any patterns. In R, we read in the data set and then peform a few fommands to check the dataset: 

```{r eda}

multitask <- read.csv("multitask_performance.csv")
nrow(multitask) # check number of rows
head(multitask) # check first 5 rows 
tail(multitask) # check last 5 rows 

summary(multitask)

```

The dataset consits of 120 rows (each row representing a student) and two columns (`Groups` and `Posttest`). The first column, `Groups`, contains the treatment the student was assigned and the `Posttest` column contains the response measure. Using the functions `head` and `tail`, we can look at the first and last 5 rows and the function `summary` provies us with a descrption of each column. We do this to check that R has read in our data correctly (you can view the whole data set by running `view(multitask)` as well). The summary tells us that the `Groups` column is of the class "character". For our analysis, we want it to be read as a factor: 

```{r factor}

multitask$Groups <- as.factor(multitask$Groups)
summary(multitask)
```
Now, we can see that there are 40 replicates per treatment group, confirming that the experiment was balanced. I have assumed that based on the resuts shown that the `Posttest` scores were stored as percentages and using the sumamry we can quickly checked whether there are any observations that are not on the appropriate scale. Looks good so far! 

##  Model checking 

@multitask2018 had several research questions, but here we only consider the following: Are there any signficant differences in mean academic performance between the three groups? 

You might think that we could perform three t-tests (Control vs Exp 1, Control vs Exp 3, Exp 1 vs Exp 2). We could, but the problem with this approach is what we call mutliple testing. When conducting many tests, there is an increased risk of making a Type 1 Error (rejecting the null hypothesis when it is in fact true) [^1]. 

[^1]: Can't remember what a $t$-test is and/or need a refresher on hypothesis testing? Have a look <a href="https://www.youtube.com/watch?v=VekJxtk4BYM" target="_blank">this video on t-tests</a>
and document for a brief reminder. **Also, a quick (and cool) sidenote:** This study by @chen2024effect used a Completely Randomized Design (CRD), randomly assigning undergraduate students to playback speed groups (1x, 1.5x, 2x, and 2.5x) to measure the effect on comprehension of recorded lectures. Using ANOVA they found that comprehension was preserved up to 2x speed. I personally like to increase the playback speed to 1.5px if I just need to revise something quickly. 

When we have more than two groups, we can use a one-way analysis of variance (ANOVA) which can be seen as an extention of $t$-test and is called one-way because there is a single factor being considered. For both of these statistical approaches the data should meet certain distributional assumptions: 

1. There are no outliers.
2. All groups have equal population variances.
3. The errors are normally distributed. 
4. The errors are independent. 

- why are outliers bad

- to check for outliers: 

```{r plot}

boxplot(Posttest~Groups, data=multitask)
stripchart(Posttest~Groups,data=multitask,vertical=TRUE, add=TRUE, method = "jitter")

```


- explain code 

- typical reasons for outliers and what to do 




TIPS: 

- increase focus, improve studying 
