[
  {
    "objectID": "07_CRD_ANOVA.html",
    "href": "07_CRD_ANOVA.html",
    "title": "7  Analysis of Variance",
    "section": "",
    "text": "7.1 An Intuitive Explanation\nThe ANOVA model we have introduced is identical to a regression model with categorical variables, it is just parameterised differently. So why the different names and emphasis on variance - ANalysis Of Variance? A well designed experiment as us to estimate the within-treatment variability and between treatment variability. More specifically, it enables the partitioning of the total sum of squares1 into independent parts, one for each factor in the model (treatment and blocking factors). This allows us unambiqously to estimate the variability in the response contributed by each factor and the experimental error variance! We can then use this partitioning to perform hypothesis tests. In other words: by looking at the variation we can find out if the response differs due to the treatments. An ANOVA applied to a single factor CRD is called a one-way ANOVA or between-subjects ANOVA or an independent factor ANOVA. It is a generalization of the ‘two-sample t-test assuming equal variances’ to the case of more than two populations.\nBefore we consider real data, we first want to look at a constructed example to explain the main ideas behind ANOVA. Assume that we carried out two experiments on plants removing nitrate (NO\\(_3\\)) from stormwater. In both experiments, we consider three plant species (uncreatively called ‘A’, ‘B’, and ‘C’). In both experiments, we have three replicates per treatment. We are only interested in comparing the species so there is no control treatment. We obtained the following data:\nIf you look at these datasets carefully, you will see that each of the three species had the same mean in the two experiments. However, the measurements were much more variable in Experiment 2 than in Experiment 1.\nThe basic idea of ANOVA relies on the ratio of the among-treatment-means variation to the within-treatment variation. This is the F-ratio. The F-ratio can be thought of as a signal-to-noise ratio:",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#an-intuitive-explanation",
    "href": "07_CRD_ANOVA.html#an-intuitive-explanation",
    "title": "7  Analysis of Variance",
    "section": "",
    "text": "Table 7.1: Hypothetical Experiment\n\n\n\n\n\n\n\n(a) Experiment 1\n\n\n\n\n\nSpecies\nA\nB\nC\n\n\n\n\n\n40\n48\n58\n\n\n\n42\n50\n62\n\n\n\n38\n52\n60\n\n\nAverage\n40\n50\n60\n\n\n\n\n\n\n\n\n\n\n\n(b) Experiment 2\n\n\n\n\n\nSpecies\nA\nB\nC\n\n\n\n\n\n40\n65\n45\n\n\n\n25\n35\n75\n\n\n\n55\n50\n60\n\n\nAverage\n40\n50\n60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich experiment has better evidence that the true mean NO₃ removal rate differs between species? Pause and think about this before reading on.\n\n\n\n\n\nIntuitively, we would say that Experiment 1 shows much stronger evidence for a true effect than Experiment 2. Why? Both experiments show the same differences among the treatment (species) means. So the variability in the treatment means is the same. However, the variability among the observations within treatments differs between the two experiments. In Experiment 1, the variability within treatments is much less than the variability among treatments. In Experiment 2, the variability within treatments is about the same as the variability among treatments.\n\n\n\n\n\nLarge ratios imply the signal (difference among the means) is large relative to the noise (variation within groups), providing evidence of a difference in the means.\nSmall ratios imply the signal (difference among the means) is small relative to the noise, indicating no evidence that the means differ.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#analysis-of-variance-for-crd",
    "href": "07_CRD_ANOVA.html#analysis-of-variance-for-crd",
    "title": "7  Analysis of Variance",
    "section": "7.3 Analysis of Variance for CRD",
    "text": "7.3 Analysis of Variance for CRD\nLet’s go back to the linear model for the single-factor completely randomized design that we examined earlier:\n\\[\nY_{ij} = \\mu + \\alpha_i + e_{ij}\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha_i\\) are the treatment effects (that is the difference between treatment means and the overall mean), and \\(e_{ij}\\) are the error terms (the differences between the observation and the fotted value, i.e. treatment mean). Remember that the estimated values for these parameters are the observed values:\n\\[\n\\begin{aligned}\n\\hat{\\mu} &= \\bar{Y}_{..} \\\\\n\\hat{\\alpha}_i &= \\bar{Y}_{i.} - \\bar{Y}_{..}\\\\\n\\hat{e}_{ij} &= Y_{ij} -  \\bar{Y}_{i.}\n\\end{aligned}\n\\]\nBy taking \\(\\mu\\) over to the left-hand-side in the equation, and substituting the above observed values we obtain:\n\\[\n\\begin{aligned}\nY_{ij} - \\mu &= (\\mu_i - \\mu) + (Y_{ij} - \\mu)\\\\\nY_{ij} - \\bar{Y} &= (\\bar{Y}_i - \\bar{Y}) + (Y_{ij} - \\bar{Y}_i) \\\\\n\\end{aligned}\n\\]\nSquaring and summing both sides gives the decomposition:\n\\[\n\\sum_i \\sum_j (Y_{ij} - \\bar{Y})^2 = \\sum_i \\sum_j (\\bar{Y}_i - \\bar{Y})^2 + \\sum_i \\sum_j (Y_{ij} - \\bar{Y}_i)^2\n\\]\nEach term represents squared deviations:\n\nThe first term is of observsations around the overall mean representing the total variation in the response.\nThe second is of the group means around the overall mean represeting the explained variation or variation between treatments and,\nThe last term represents the deviations of observations from their treatment means (unexplained or within treatment variation).\n\nWe could also call these:\n\\[\nSS_{\\text{total}} = SS_{\\text{between groups}} + SS_{\\text{within groups}}\n\\] or\n\\[\nSS_{\\text{total}} = SS_{\\text{treatment}} + SS_{\\text{error}}\n\\]\nThe analysis of variance is based on this identity2. The total sums of squares equals the sum of squares between groups plus the sum of squares within groups.\n2 In mathematics, an identity is an equation that is always true, regardless of the values of it’s variables. In other words, the identity is true for all observations.Back to our constructed example. What are the different sums of squares? For Experiment 1, we get: \\(SS_{\\text{total}} = 624; SS_{\\text{between groups}} = 600; SS_{\\text{within groups}} = 24\\). Verify these numbers and do the same for Experiment 2.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#anova-table",
    "href": "07_CRD_ANOVA.html#anova-table",
    "title": "7  Analysis of Variance",
    "section": "7.4 ANOVA Table",
    "text": "7.4 ANOVA Table\nThis division of the total sums of sqaures is typically summarised in an analysis of variance table. The first column contains the “source” of the varibility with the first entry (the order is not important, although this is the typical order) representing the between treatment variability (explained variation), second the error (unexplained variation) and lastly the total variation. The second column gives the sums of squares of each source. The third column contains the degrees of freedom.\n\n\n\n\n\n\n\n\n\n\nSource\nSums of Squares (SS)\ndf\nMeans Squares (MS)\nF\n\n\n\n\nTreatment\n\\(\\sum_i n_i(\\bar{Y}_i - \\bar{Y})^2\\)\n\\(a-1\\)\n\\(MS_A = SS_A / (a-1)\\)\n\\(MS_A / MSE\\)\n\n\nResiduals (Error)\n\\(\\sum_i \\sum_j (Y_{ij} - \\bar{Y}_i)^2\\)\n\\(N-a\\)\n\\(MSE = SS_E / (N-a)\\)\n\n\n\nTotal\n\\(\\sum_i \\sum_j (Y_{ij} - \\bar{Y})^2\\)\n\\(N-1\\)\n\n\n\n\n\nThe fourth column contains the Mean Sqaures. This is what we get when we divide sums of sqaures by the appropriate degrees of freedom.\n\\[ \\text{MS} = \\frac{SS}{df}\\]\nThis is simply an average and may be seen as an estimate of variance. So when we divide the treatment SS by its degrees of freedom, we get an estimate of the variation due to treatments and simirlary, for the the residual SS, we get an estimate of the error variance. You’ve seen this before!\n\\[\\text{MSE} = \\hat{\\sigma}^2 = \\frac{1}{N-a}\\sum_i\\sum_j(Y_{ij} - Y_{i.})^2\\]\n\n7.4.1 What Are Degrees of Freedom?\nDegrees of freedom (df) represent the number of independent pieces of information available for estimating a parameter. When making statistical calculations, we typically lose one degree of freedom for every estimated parameter before the current calculation.\nFor example, when estimating the standard deviation of a dataset, we first estimate the mean, thereby reducing the number of independent observations available to calculate variability. This is why the denominator in the variance formula is \\(N-1\\):\n\\[ s^2 = \\frac{\\sum(Y_i - \\bar{Y})^2}{N -1} \\] You can think of degrees of freedom as the number of independent deviations around a mean. If we have \\(n\\) observations and their mean, once we know \\(n-1\\) of the values, the last one is fixed—it must take on a specific value to satisfy the mean equation. Therefore, only \\(n-1\\) observations are truly free to vary.\nExample: Three Numbers Summing to a Fixed Mean\nSay we have three (\\(n=3\\)) numbers: (4, 6, 8). The mean of these three numbers is 6. If we only knew the first two numbers (4,6) and the mean, the third number must be 8:\n\\[\n\\begin{aligned}\n\\bar{x} &= \\frac{\\sum x_i}{n}\\\\\n6 &= \\frac{4+6+x_3}{3}\\\\\n18 &= 10 + x_3 \\\\\nx_3 &= 8\n\\end{aligned}\n\\]\nSince the third number is uniquely determined by the first two and the mean, we only have \\(n-1\\) (i.e., 2) degrees of freedom.\nAnother Intuitive Analogy\nImagine you are distributing a fixed amount of money among friends. If you have R100 and four friends, you can freely allocate money to three friends, but whatever is left must go to the fourth friend to ensure the total remains R100. Similarly, once the first \\(n-1\\) values are chosen, the last value is determined, limiting the degrees of freedom.\nIn ANOVA\nIf you look at the treatment sums of squares: \\(\\sum_i n_i (\\bar{Y}_{i.} - \\bar{Y}_{..})^2\\). We have \\(a\\) deviations around the grand mean. But once we know \\(a-1\\) of the treatment means and the grand mean3, the last mean is fixed. So we have \\(a-1\\) independent deviations around the overall mean.\n3 Remember, \\(\\mu = \\frac{\\sum \\mu_i}{a}\\).If you look at the treatment sums of squares: \\(\\sum_i \\sum_j (Y_{ij} - \\bar{Y}_{..})^2\\). We are using \\(N\\) observations and calculating the deviations of these observations around the overall mean. So, only \\(N-1\\) observations are free to vary, the last observation is fixed for the calculated mean to hold true.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#the-f-distribution",
    "href": "07_CRD_ANOVA.html#the-f-distribution",
    "title": "7  Analysis of Variance",
    "section": "7.4 The F-distribution",
    "text": "7.4 The F-distribution\nSo now we have two estimates of variances. One for variance due to treatments and one for error variance. When we take the ratio of two variances, it can be shown that the ratio has an F-distribution.\nThe test statistic is:\n\\[\nF = \\frac{MS_{\\text{treatment}}}{MS_E}\n\\]\nwith degrees of freedom \\((a-1, N-a)\\).\nFor Experiment 1:\n\\[\nF = \\frac{300}{4} = 75, \\quad p &lt; 0.001\n\\]\nFor Experiment 2:\n\\[\nF = \\frac{300}{225} = 1.33, \\quad p = 0.332\n\\]",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "00_ModelConcepts.html",
    "href": "00_ModelConcepts.html",
    "title": "Statistical Modelling",
    "section": "",
    "text": "What is a Model?\nA statistical model is a mathematical representation of how data is generated. It describes the relationship between observed data and underlying factors (parameters) while accounting for random variation. Suppose that we are interested in estimating the age of a tree from its stem diameter. To do this we need to know by how much the stem diameter increases per year. We could describe this relationship or process as follows:\n\\[D = \\alpha + \\beta \\times Age\\] describing a linear increase of diameter with age. Once we have a good idea of how fast diameter increases with age (β) we can predict diameter from age. The (mathematical) model above is a very simple representation of this process with only two parameters, the intercept and the growth rate.\nWith the chosen parameter values, diameter increases linearly with age. Of course, this model is not realistic except for special situations but it gives us powerful insights. In reality we don’t know \\(\\beta\\), but usually need to estimate it from data. Also, not every tree grows equally fast, because of environmental and individual differences between trees. We can accept that the above is a simple model for the average behaviour of a tree, but to capture variability between trees (because of variability between environmental conditions from tree to tree, variability between individual trees, measurement error), we add an error term.\n\\[D = \\alpha + \\beta \\times Age_i + e_i\\]\nThe response that we observe is then described by an average behaviour, but the actual observed value will vary around this average. To summarise, the statistical model has a stochastic component which captures variability in the response that cannot be explained by the deterministic part of the model. Another distinguishing feature of statistical modelling is that we obtain estimates of the parameter values from the data, e.g. by fitting a line to the observations, i.e. we learn from data.",
    "crumbs": [
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "00_ModelConcepts.html#more-generally",
    "href": "00_ModelConcepts.html#more-generally",
    "title": "Statistical Modelling",
    "section": "More generally",
    "text": "More generally\nStatistical models are not perfect predictors of the data, rather they attempt to describe the “central tendency” of the observations. To get to the actual observed value some deviation from the central tendency needs to added (i.e. error). Such models typically have the following the form:\n\\[\n\\text{Observed Response} = \\text{Model Predicted Response} + \\text{Error}\n\\] Mathematically this can be stated as:\n\\[ Y = \\hat{Y} + e\\]\nA simple example of a statistical model you may have encountered is the mean as a predictor. Suppose you measure the number of customers entering two stores over 20 days. The observed counts for each store fluctuate daily, but you may want to summarize the data using the average number of customers.\nFor each store \\(i\\), a basic statistical model for these observations would be:\n\\[\nY_{ij} = \\mu_i + e_{ij}\n\\]\nwhere:\n\n\\(Y_{ij}\\) is the number of customers observed on day \\(j\\) at store 1,\n\\(\\mu_i\\) is the true mean number of customers at store \\(i\\),\n\\(e_{ij}\\) is the error term, representing deviations from the mean.\n\nThe error term \\(e_{ij}\\) accounts for day-to-day fluctuations that cause the actual number of customers to vary around the mean. Below this data is simulated and plotted, with the model overlain. The black line is the mean and the red dashed line represents the error for one observation, i.e. deviation from the fitted model response, in this case the mean.\n\nAnother basic example of this structure is a linear regression model:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + e_i\n\\]\nwhere:\n\n\\(Y_i\\) is the observed response,\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters representing the intercept and slope,\n\\(X_i\\) is the predictor variable,\n\\(e_i\\) is the random error term.",
    "crumbs": [
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "00_ModelConcepts.html#notation",
    "href": "00_ModelConcepts.html#notation",
    "title": "Statistical Modelling",
    "section": "Notation",
    "text": "Notation\nWhen we fit the model to our data, we estimate the unknown parameters using observed data. We denote these estimates using hat notation to distinguish them from the true (but unknown) population parameters:\n\\[\n\\hat{\\beta}_0, \\quad \\hat{\\beta}_1\n\\]\nSimilarly, the fitted values (model-predicted responses) are denoted as:\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i.\n\\]\nThus, after fitting the model, the observed response can be rewritten as:\n\\[\nY_i = (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i) + e_i = \\hat{Y}_i + e_i\n\\]\nwhere:\n\n\\(\\hat{Y}_i\\) is the fitted (predicted) value, and\n\\(e_i = Y_i - \\hat{Y}_i\\) is the residual, representing the difference between the observed and predicted values.\n\nTrue model (unknown parameters):\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + e_i\n\\]\nand fitted model:\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n\\]",
    "crumbs": [
      "Statistical Modelling"
    ]
  },
  {
    "objectID": "01_ExpDesign_Why.html",
    "href": "01_ExpDesign_Why.html",
    "title": "1  Experiments and experimental design",
    "section": "",
    "text": "There are two fundamental ways to obtain information in research: by observation or by experimentation. In an observational study the observer watches and records information about the subject of interest. In an experiment, the experimenter actively manipulates variables hypothesized to affect the response (insert small example). Although both are important ways of understanding the world around us, only through experiments can we infer causality.\nThat is, by designing and conducting an experiment properly, if we observe a result such as a change in variable A leads to a change in our response (say variable B), we can conclude that A caused this change in B. If we were to merely study variable B and observe that as variable A changes, B also changes without conducting an experiment, then we can only say that variable A and B are associated. We could not conclude that any change in B is due to A. It could be some other factor that is correlated with A or it could be that B caused the change in A! The key is that a well-designed experiment controls and holds constant (as best we can) all other factors that might affect the response, so we can be sure the result is caused by the variable we manipulated.\nImagine a company wants to determine whether their voluntary employee training program (the explanatory variable) increases productivity (the response). They decide to track the productivity of employees who chose to complete the training and those who did not. They note that, on average, trained employees are more productive. Can we confidently conclude that the training program caused increased productivity?\nThis is an observational study since no variable was actively manipulated, they merely observed and recorded the productivity of two groups of employees. So, we cannot conclude that completing the training program increases productivity - we cannot infer causality. It could be due to many other factors, either observed or unobserved, such as maybe employees who choose to do the training program are inherently more motivated and thus productive. Can you think of any other factors?\nIf they actively manipulate the explanatory variable, training program, by randomly assigning employees to complete the training program or not and control other factors by ensuring the employees are as similar as possible accross the groups (i.e. conducted an experiment). Any differences in productivity between the two groups could then be ascribed to the training program. If they happen to find that the employees who were assigned the training program are more productive, they can confidently say that the program caused increased productivity (and perhaps make it compulsory for all employees!).\nExperimental studies are extremely important in research and in practice. They are almost the only way in which one can control all factors to such an extent as to eliminate any other possible explanation for a change in a response other than the variable actively manipulated. In this course, we only consider experimental studies and those which aim to compare the effects of a number of treatments.\nHere are some other reasons for conducting experiments:\n\nThey are easy to analyse. A well designed experiment results in independent estimates of treatment effects which allow us to easily interpret the effects. EXPAND - independent treatment effects and/or independent treatment variables?\nExperiments are frequently used to find optimal levels of variables which will maximise (or minimise) the response. Such experiments can save enormous amounts of time and money. Imagine trying to find the optimal settings for producing electricity from coal without proper experimentation. Such a trial and error process would be extremely costly, wasteful and time consuming. In a similar vein, what if the fictional company in our previous example decided to invest a bunch of money in fine-tuning their training program based solely on the results of an observational study. In reality though, it turns out that adjusting their hiring process to identify more keen candidates would have been much more efficient and inexpensive.\nIn an experiment we can choose exactly those settings or treatment levels we are interested in, e.g. we can investigate the effect of different shift lengths (6, 8 or 9 hours) on employee productivity or test specific price points (R100, R150, R200) to determine which price maximizes sales or revenue. We can actively manipulate the variable(s) to the levels we are interested in.\n\nExperimental studies and their design are fundamental to science, allowing us to further knowledge and test theories. So lets define them more rigorously. We’ll start by introducing some terminology.\nINSERT WHAT THEY NEED TO KNOW / MAIN TAKEAWAYS Perhaps?",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments and experimental design</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html",
    "href": "02_ExpDesign_Term.html",
    "title": "2  Terminology",
    "section": "",
    "text": "Treatment factors, treatment levels and treatments:\nThe treatment factor is the factor or variable that the experimenter actively manipulates to measure its effect on the response. All factors/variables that are investigated, controlled, manipulated, thought to influence the response, are called the treatment factors. They become the explanatory variables (mostly categorical) in the model. For each treatment factor, we actively choose a set of levels. For example, the treatment factor “temperature” can have levels 10, 20, and 50°C. If temperature is the only treatment factor in the experiment, the treatments1 will also be 10, 20, and 50°C.\nIf we manipulate more than one factor (e.g., temperature and pressure), we have two treatment factors. When several treatment factors are manipulated, the experiment is called factorial and the treatments are all possible combinations of the factor levels. If we have pressure levels “low” and “high,” there are 6 treatments in total:\nFigure 2.1: Visualization of how treatments are formed as combinations of treatment levels.\nIn the figure above, there are two treatment factors: Temperature (on the y-axis) and Pressure (on the x-axis). The axis ticks represent the levels of each treatment factor, and the blocks within the grid represent the treatments, which are specific combinations of the levels of Temperature and Pressure. Each treatment is labeled with the corresponding combination of levels (e.g., ‘50, Low’ or ‘10, High’).\nWhen faced with a text like this, it is useful to identify the treatment factors, their levels and the treatments, as well the response. Clearly, from the question, we are interested in the effect of therapy on test anxiety. A statement like this can generally be read as the effect of the treatment factor on the response. Nowhere is another treatment factor mentioned, so we only have one in this example. What are the levels of therapy we set? The levels are 5, 10 and 15 hours of therapy and since we only have one factor these are also the treatments. Let’s summarise this as follows:",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#treatment-factors-treatment-levels-and-treatments",
    "href": "02_ExpDesign_Term.html#treatment-factors-treatment-levels-and-treatments",
    "title": "2  Terminology",
    "section": "",
    "text": "1 The terminology of treatments can be traced back to 1920’s when it was first applied by Ronald Fisher in the agricultural sciences. He is often refered to as the Founder of Statistics! Have a look at the very first application of ANOVA here and also a very nice article describng the history of statistics and his contribution to the field.\n\n\n\n\n\n\n\n\nExample 1\n\n\n\nThree groups of students, 5 in each group, were receiving therapy for severe test anxiety. Group 1 received 5 hours, group 2 received 10 hours and group 3 received 15 hours. At the end of therapy each subject completed an evaluation of test anxiety. Did the amount of therapy have an effect on the level of test anxiety?\nThe three groups of students received the scores on the Test Anxiety index (TAI) at the end of treatment shown in the table below.\n\n\n\nGroup 1\nGroup 2\nGroup 3\n\n\n\n\n48\n55\n51\n\n\n50\n52\n52\n\n\n53\n53\n50\n\n\n52\n55\n53\n\n\n50\n53\n50\n\n\n\n\n\n\n\n\n\nResponse: Test Anxiety\n\nTreatment Factor: Therapy\n\nTreatment Levels: 5, 10, and 15 hours of therapy\n\nTreatments: 5, 10, and 15",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#experimental-and-observational-unit",
    "href": "02_ExpDesign_Term.html#experimental-and-observational-unit",
    "title": "2  Terminology",
    "section": "Experimental and observational unit",
    "text": "Experimental and observational unit\nThe experimental unit is the entity (e.g. material, object, or individual) to which a treatment is assigned or that receives the treatment. By contrast, the observational unit is the entity from which the response is recorded. This distinction is very important because it is the experimental units which determine how often the treatment has been replicated and therefore the precision with which we can measure the treatment effect. In the methods that we cover in this course, we require that in the end there is only one ‘observation’ (response value) per experimental unit. If several measurements have been taken on an experimental unit, we will combine these into one observation, typically by taking the mean. Very often, the experimental unit is also the observational unit.\nFor See Example (example-box?) for more details, what are the experiemental units? To determine this, revisit the text of Example 1 and ask yourself: what entity received the treatments or to what were treatments applied? Most of you, will probably answer the students and this is correct. Each student received the respective treatment (number of hours in therapy) assigned to their group and so there are \\(5 \\times 3 = 15\\) experimental units.\nThere is an argument to be made that it is not clear whether the students received therapy on their own or that the groups of students received therapy together. In that case, treatments were applied to groups of students and so there would be three experimental units. This will usually be clear from the text, but we’ll use this scenario to illustrate some concepts as we go.\nWe also need to know what the observational units are. The text states that at the end of therapy, each student completed an evaluation to determine their level of test anxiety. So the response, test anxiety, was measured on the student level which means students are the observational units. In the first scenario, the students are both the experimental units and observational units. But this would not be the case if groups are the experimental unit.\nWe also require that there is only one observation per experimental unit, the first scenario meets this requirement. For the second scenario, we have 5 observations per group and so we would have to take the mean of these values to end up with one response value per group.\nLet’s add to the summary assuming students are the experimental units:\n\nExperimental unit (no): Student (15)\n\nObservational unit (no): Student (15)",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#homogeneity-of-experimental-units",
    "href": "02_ExpDesign_Term.html#homogeneity-of-experimental-units",
    "title": "2  Terminology",
    "section": "Homogeneity of experimental units",
    "text": "Homogeneity of experimental units\nWhen the set of experimental units are as similar as possible such that there are no distinguishable differences between them, they are said to be homogeneous (a fancy word for saying they are of the same kind). The more homogeneous the units are, the smaller the experimental error variance (natural variation between between observations of the same treatments) will be. It is super important to have fairly homogeneous units because it allows us to detect differences between treatments more easily.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#blocking",
    "href": "02_ExpDesign_Term.html#blocking",
    "title": "2  Terminology",
    "section": "Blocking",
    "text": "Blocking\nIf the experimental units are not fairly similar but are heterogeneous (the opposite of homogeneous), we can group them into sets of similar units. This process is called blocking and the groups are considered “blocks”. We compare the treatments within each block as if each block is its own mini-experiment. This way we account for the differences between blocks and can better isolate the effect of the treatments.\n\n\n\n\n\n\nExample 2.2 EDIT THIS STILL\n\n\n\nImagine you’re testing the effectiveness of two marketing strategies (A and B) to increase sales at a chain of coffee shops. The coffee shops are located in different neighborhoods, where factors like income levels might influence sales. To prevent these differences from skewing the results, you group the coffee shops into “blocks” based on neighborhood income level (e.g., low, medium, high).\nWithin each block, you randomly assign coffee shops to either Strategy A or Strategy B. This approach allows you to compare the strategies while controlling for variability caused by differences in neighborhood income levels.\nWithout blocking, would you be able to confidently attribute differences in sales to the strategies alone? Likely not, as any observed differences could be due to neighborhood-specific factors rather than the strategies themselves.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "02_ExpDesign_Term.html#replication-and-pseudoreplication",
    "href": "02_ExpDesign_Term.html#replication-and-pseudoreplication",
    "title": "2  Terminology",
    "section": "Replication and pseudoreplication",
    "text": "Replication and pseudoreplication\nIf a treatment is applied independently to more than one experimental unit it is said to be replicated. Treatments must be replicated! Making more than one observation on the same experimental unit is not replication, but pseudoreplication. Pseudoreplication is a common fallacy (REF?). The problem is that without true replication, we don’t have an estimate of uncertainty, of how repeatable, or how variable the result is if the same treatment were to be applied repeatedly.\nIn Example 1, if experimental units were the groups and we didn’t take the average of the observations per group, we would have pseudoreplication as each student would not be an independent replicate of a treatment - effectively, we have only applied each treatment once. You might notice that we then only have one true replicate per treatment group and this is problematic. To get an estimate of uncertainty, we would have to repeat this experiment a few more times to get more than one proper replicate.\nThe first scenario, however, did not have this problem and each treatment was replicated five times. After going through all this, we have the following summary:\n\nResponse: Test Anxiety\n\nTreatment Factor: Therapy\n\nTreatment Levels: 5, 10, and 15 hours of therapy\n\nTreatments: 5, 10, and 15\n\nExperimental unit (no): Student (15)\n\nObservational unit (no): Student (15)\n\nReplicates: 5\n\n\n\n\n\n\n\nTip\n\n\n\nCreating a summary like this, is a handy exercise for any experiment you come across, and we’ll keep doing it for every experiment in this book. As we go along, we’ll also add information about the type of experiment that was conducted.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Terminology</span>"
    ]
  },
  {
    "objectID": "03_ExpDesign_RRR.html",
    "href": "03_ExpDesign_RRR.html",
    "title": "3  The three R’s of experimental design",
    "section": "",
    "text": "3.1 Replication\nExperimental Design is a detailed procedure for grouping, if blocking is necessary, experimental units and for how treatments are assigned to the experimental units. There are three fundamental principles, known as the ‘three R’s of experimental design’ which are at the core of a good experiment. The following section might feel a bit repetitive, but these concepts cannot be emphasised enough.\nLet’s define it again: replication is when each treatment is applied to several experimental units. This ensures that the variation between two or more units receiving the same treatment can be estimated and valid comparisons can be made between treatments. In other words, replication allows us to separate variation due to differences between treatments from variation within treatments. For true replication, each treatment should be independently applied to several experimental units. If this is not the case, treatment effects become confounded with other factors.\nConfounding means that is not possible to separate the effects of two (or more) factors on the response, i.e. it is not possible to say which of the two factors is responsible for any changes in the response. This is what happened in the Example 1 when groups are the experimental units. With only one replicate per treatment, the effect of therapy is confounded with the experimental unit or the effect of group on test anxiety. The reason why this is a problem is that any difference between the treatments could be due to any differences between the groups and not just the number of therapy hours. The same would be true if we only had one student per group. Why? Take a moment to think about this.\nConsider the first row of the data from Example 1. It looks like the student in group 2 scored the highest, followed by group 3 and then group 1. So does longer therapy sessions lead to higher test anxiety? Likely not! With only one student per treatment, we are not able to say that any differences in the response are due to the treatments. It could be due to any differences between the individuals. Maybe the student in group 3 tends to score higher on anxiety tests regardless of the treatment, or perhaps the student in group 1 was unusually calm that day. Without replication, these individual differences could mask (or mimic) the true effects of the treatments.\nBy replicating the treatments across multiple students, we can average out these individual differences and gain a clearer picture of whether therapy duration truly impacts test anxiety. With five students per group, we might observe that group 1 consistently scores lower than group 3. This consistency would provide stronger evidence that the treatments, and not just individual variation, are responsible for the observed differences. So by replication, we can compare within treatment variation to variation between treatments.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The three R's of experimental design</span>"
    ]
  },
  {
    "objectID": "03_ExpDesign_RRR.html#replication",
    "href": "03_ExpDesign_RRR.html#replication",
    "title": "3  The three R’s of experimental design",
    "section": "",
    "text": "Treatment 1\nTreatment 2\nTreatment 3\n\n\n\n\n48\n55\n51\n\n\n50\n52\n52\n\n\n53\n53\n50\n\n\n52\n55\n53\n\n\n50\n53\n50\n\n\n\n\n\n\n\n\n\nExample 3.1\n\n\n\nMaybe the co2 uptake data?",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The three R's of experimental design</span>"
    ]
  },
  {
    "objectID": "03_ExpDesign_RRR.html#randomisation",
    "href": "03_ExpDesign_RRR.html#randomisation",
    "title": "3  The three R’s of experimental design",
    "section": "3.2 Randomisation",
    "text": "3.2 Randomisation\nRandomisation refers to the process of randomly assigning treatments to experimental units such that each experimental unit has equal chance of receiving a specific treatment. Randomisation ensures that:\n\nThere is no bias on the part of the experimenter, either conscious or unconscious, when assigning treatments to experimental units.\nNo experimental unit is favored to receive a particular treatment.\nPossible differences between units are equally distributed amongst treatments. If there are clear differences between units, then blocking should be performed and randomisation occurs within blocks. We’ll talk more about this in Chapter INSERT\nWe can assume independence between observations.\n\nRandomisation is not haphazard. In statistics (and here in the context of experimental design), randomisation has a specific meaning: namely that each experimental unit has the same chance of being allocated any of the treatments. This can be done using random number generators such as with software packgaes, dice or drawing number from a hat (provided the number have been shuffled adequately and have equal chance to be picked).\nLet’s have a look at randomisation in R. Suppose we have 4 treatments (A, B, C, and D) and 32 experimental units. There are no differences between the units, so we don’t have to block, and we can equally split the units across the treatments, which means we have 8 units per treatment, i.e., 8 replicates. In R, we first create a long vector of 8 As, 8 Bs, 8 Cs, and 8 Ds called all.treat. Then shuffle the vector to obtain a randomisation using the function sample.\n\n# repeat the vector A, B, C, D 8 times \nall.treats &lt;- rep(c(\"A\",\"B\",\"C\",\"D\"), times = 8)\n\n# permutation of all.treats (sample withut replacement)\nrand1 &lt;- sample(all.treats)\n\n# example output\nrand1\n\n [1] \"B\" \"A\" \"D\" \"C\" \"B\" \"D\" \"D\" \"A\" \"C\" \"D\" \"A\" \"A\" \"C\" \"C\" \"C\" \"B\" \"D\" \"D\" \"B\"\n[20] \"C\" \"D\" \"A\" \"A\" \"C\" \"D\" \"A\" \"B\" \"A\" \"B\" \"C\" \"B\" \"B\"\n\n\nExperimental unit 1 recipes the first treatment that appears as the first element in the shuffled vector, experimental unit 2 receives the second and so on.\nNotes on randomisation?",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The three R's of experimental design</span>"
    ]
  },
  {
    "objectID": "03_ExpDesign_RRR.html#reduction-of-unexplained-variation-blocking",
    "href": "03_ExpDesign_RRR.html#reduction-of-unexplained-variation-blocking",
    "title": "3  The three R’s of experimental design",
    "section": "3.3 Reduction of Unexplained Variation (Blocking)",
    "text": "3.3 Reduction of Unexplained Variation (Blocking)\nUnexplained variation (or experimental error variance or within treatment variance) is largely due to inherent differences between experimental units. The larger this unexplained variation, the more difficult it becomes to detect treatment differences (a treatment signal). To minimise experimental error variance we can control extraneous factors (i.e. keeping all else constant) and by choosing homogeneous experimental units. Otherwise, we can block experimental units to reduce the variation.\nBlocking variables are nuisance factors that might affect your response or introduce systematic variation in the response and we are typically, not interested in these. Often, they are factors that cannot be randomised, e.g. biological sex of a person, time of day, location of a warehouse etc. We control the effect of such variables on the response by blocking for them so that we can investigate the possible effect of a variable that we are interested in. Usually, in a complete block experiment, there are as many experimental units per block as there are treatments, so that each treatment is applied once in every block. Treatments are randomized to the experimental units in the blocks. We can then compare the effects of treatments on similar experimental units, and we can estimate the variation induced in the response due to the differences between blocks. This variation due to blocks can then be removed from the unexplained variation.\nEXAMPLE\nBlocking also offers the oopurutnity to test treatments over a wider range of conditions, e.g. if I only use people of one age in my experiment (say students) I cannot generalize my results to older people. However, if i use different age blocks I will be able to tell whether the treatments have similar effects in all age groups or not.\nLastly, if blocking is not feasible, randomization will ensure that at least treatments and nuisance factors are not confounded.\n\n“Block what you can, randomize what you cannot.”\n— Box, Hunter & Hunter (1978)",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The three R's of experimental design</span>"
    ]
  },
  {
    "objectID": "04_ExpDesign_DesigningExp.html",
    "href": "04_ExpDesign_DesigningExp.html",
    "title": "4  Designing an Experiment",
    "section": "",
    "text": "When planning an experiment we need to decide on:\n\ntreatment factors and their levels\nthe response\nexperimental material / units\nblocking factors\nnumber of replicates\n\nSome of these will be determined by the research question and how experimental units are assigned to treatments are determined by the design. The design that will be chosen for a particular experiment depends on the treatment structure (determined by the research question) and the blocking structure (determined by the available experimental units).\nHere are two ways the treatments can be structured:\n\nSingle factor: the treatments are the levels of a single treatment factor.\nFactorial: when more than one factor are of interest, then the experiment is said to be a factorial experiment. The treatments are constructed by crossing the treatment factors like we did in Figure 2.1 such that the treatments are all possible combinations of the treatment levels. For example, if factor A has \\(a\\) levels and factor B has \\(b\\) levels, there are \\(a \\times b\\) treatments. Such an experiment would then be called an \\(a \\times b\\) factorial experiment.\n\nThe blocking structure is determined the set of experimental units chosen or available for the experiment.are there any structures/differences that need to be blocked? Do I want to include experimental units of different types to make the results more general? How many experimental units are available in each block? For the simplest design in this course, the number of experimental units in each block corresponds to the number of treatments. This is called a complete block experiment. There are several other blocking structures, such as incomplete blocks and blocks with missing values, all with specific analysis which we will not cover here.\nIn this course, we cover two basic designs: Completely Randomized Designs (CRD) and Randomized Block Designs (RBD). For both designs, the treatment structure can be single or factorial. Where they differ is in terms of the experimental units and how randomization occurs.\nCompletely Randomized Designs (CRD)\nWhen all experimental units are fairly homogeneous, a CRD is used. Treatments are randomized to all experimental units.\nRandomized Block Design\nThis design is used when all experimental units are not homogeneous or blocking is required to control a nuisance factor. The treatments are randomized to the units within blocks.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Designing an Experiment</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html",
    "href": "05_CRD_Intro.html",
    "title": "5  Introduction",
    "section": "",
    "text": "5.1 Example: The effect of social media multitasking on classroom performance.\nCompletely Randomized Designs (CRDs) are the simplest experimental designs. They are used when experimental units are uniform enough and we expect them to react simirlary to a given treatment. In other words, we have no reason to suspect that a group of experimental units might react differently to the treatments. We also don’t expect any effects (besides possibly a treatment effect) to cause any systematic changes in the response. So, we don’t have to block for differeing experimental units or any nuisance factors.\nRemember experimental design is the procedure for how experimental units are grouped and treatments are applied. We have already said that there are no blocks in CRDs. So randomisation occurs without restriction and to all experimental units. More generally, the \\(a\\) treatments are randomly assigned to \\(r\\) experimental units, such that each experimental unit is equally likely to receive any of the treatments. This means that there are \\(N = r \\times a\\) experimentnal units in total. We only consider designs that are balanced meaning that there an equal number of experimental units per treatment, i.e. a treatment is applied to \\(r\\) units. The experiment is then said to have \\(r\\) replicates.\nAs a student, I used to believe I could multitask effectively. I would scroll through my phone during lectures, study while texting friends, or listen to podcasts while driving. It felt like I was paying attention to everything, but in hindsight, I can barely recall the details of those podcasts. I often had to revisit lectures or restart study sessions because my focus wasn’t truly there. This tendency extends beyond student life. In the average workplace, tasks are frequently interrupted by social media, email checks, or notifications. Many of us feel the constant pull of our phones when trying to concentrate, whether we’re working, studying, or even relaxing.\nIn an era of perceived multitasking, where devices and distractions dominate our attention, it’s worth asking: Does social media multitasking impact academic performance of students?\nThe analysis of experimental data is determined by the design. The design dictates the terms that we will include in our statistical model and so it is crucial to be able to identify the design and all factors included (blocking and treatment). It is also important to check that randomisation has been done correctly and determine the number of replicates used. In the previous chapter we started doing this by creating a summary of the design and we do the same here. From the description of the study, it is clear that:\nStudents were randomly assigned to one of the three groups, and performance was measured for each individual. Although this may seem obvious, they only took one measurement per student, so we don’t have to worry about pseudoreplication. This setup indicates that the students are both the experimental units and the observational units in this study. With a total of 120 experimental units and three treatments, the experiment has 40 replicates. Since only one treatment factor was investigated, and no blocking was performed, this is classified as a single-factor Completely Randomized Design (CRD). Here is the study breakdown:\nBefore we continue, now is the time to note that we won’t be using the real data collected in this experiment. It wasn’t available but I have simulated data to match their results. I’ve also made some other modifications such as the original study included 122 students but to ensure a balanced design I include only 120.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html#example-the-effect-of-social-media-multitasking-on-classroom-performance.",
    "href": "05_CRD_Intro.html#example-the-effect-of-social-media-multitasking-on-classroom-performance.",
    "title": "5  Introduction",
    "section": "",
    "text": "Example 5.1\n\n\n\nTwo researchers from Turkey, Demirbilek and Talan (2018), conducted a study to try and answer this question. Specifically, they examined the impact of social media multitasking during live lectures on students’ academic performance.\nA total of 120 undergraduate students were randomly assigned to one of three groups:\n\nControl Group: Students used traditional pen-and-paper note-taking.\nExperimental Group 1 (Exp 1): Students engaged in SMS texting during the lecture.\nExperimental Group 2 (Exp 2): Students used Facebook during the lecture.\n\nOver a three-week period, participants attended the same lectures on Microsoft Excel. To measure academic performance, a standardised test was administered.\n\n\n\n\nResponse Variable: Academic performance, as measured by test scores.\nTreatment Factor: Level of social media multitasking.\nTreatment Levels (Groups): Control, Exp 1, and Exp 2.\n\n\n\nResponse Variable: Academic Performance\n\nTreatment Factor: Level of Social Media Multitasking\n\nTreatment Levels: Control, Experimental 1 (SMS), Experimental 2 (Facebook)\n\nTreatments: Control, Experiment 1, Experiment 2\n\nExperimental Unit: Student (120)\n\nObservational Unit: Student (120)\n\nReplicates: 40 students per group\n\nDesign Type: Single-Factor Completely Randomized Design (CRD)",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html#exploratory-data-analysis-eda",
    "href": "05_CRD_Intro.html#exploratory-data-analysis-eda",
    "title": "5  Introduction",
    "section": "5.2 Exploratory data analysis (EDA)",
    "text": "5.2 Exploratory data analysis (EDA)\nBefore we start any analyses, we have to conduct some exploratory data analysis to get a feel for our data. We start by checking whether it has been read in correctly and then look at some descriptive statsitics.\nIn R, we read in the data set and then use some commands to inspect the dataset:\n\nmultitask &lt;- read.csv(\"Datasets/multitask_performance.csv\")\nnrow(multitask) # check number of rows\n\n[1] 120\n\nhead(multitask) # check first 5 rows \n\n    Group Posttest\n1    Exp1 86.39427\n2    Exp1 64.19996\n3    Exp2 52.75394\n4 Control 67.81147\n5    Exp1 52.39911\n6    Exp1 56.58150\n\ntail(multitask) # check last 5 rows \n\n      Group Posttest\n115 Control 77.94344\n116 Control 63.58444\n117    Exp1 55.17758\n118    Exp2 67.16150\n119    Exp2 32.58373\n120    Exp2 49.58119\n\nsummary(multitask)\n\n    Group              Posttest    \n Length:120         Min.   :23.38  \n Class :character   1st Qu.:52.67  \n Mode  :character   Median :65.01  \n                    Mean   :63.59  \n                    3rd Qu.:76.32  \n                    Max.   :98.78  \n\n\nThe dataset consits of 120 rows (each row representing a student) and two columns (Group and Posttest). The first column, Groups, contains the treatment the student was assigned and the Posttest column contains the response measure. Using the functions head and tail, we can look at the first and last 5 rows and the function summary provies us with a descrption of each column. We do this to check that R has read in our data correctly (you can view the whole data set by running view(multitask) as well). The summary tells us that the Group column is of the class “character”. For our analysis, we want it to be read as a factor:\n\nmultitask$Group &lt;- as.factor(multitask$Group)\nsummary(multitask)\n\n     Group       Posttest    \n Control:40   Min.   :23.38  \n Exp1   :40   1st Qu.:52.67  \n Exp2   :40   Median :65.01  \n              Mean   :63.59  \n              3rd Qu.:76.32  \n              Max.   :98.78  \n\n\nNow, we can see that there are 40 replicates per treatment group, confirming that the experiment is balanced. I have assumed that, based on the results shown, that the Posttest scores were recorded as percentages and using the summary we can quickly checked whether there are any observations that are not on the appropriate scale or might be outliers. Looks good so far!",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "05_CRD_Intro.html#checking-assumptions",
    "href": "05_CRD_Intro.html#checking-assumptions",
    "title": "5  Introduction",
    "section": "5.3 Checking assumptions",
    "text": "5.3 Checking assumptions\nDemirbilek and Talan (2018) had several research questions, but here we only consider the following: Are there any signficant differences in mean academic performance between the three groups?\n\nDemirbilek, Muhammet, and Tarik Talan. 2018. “The Effect of Social Media Multitasking on Classroom Performance.” Active Learning in Higher Education 19 (2): 117–29.\n1 Can’t remember what a \\(t\\)-test is and/or need a refresher on hypothesis testing? Have a look this video on t-tests and document for a brief reminder. Also, a quick (and cool) sidenote: This study by Chen et al. (2024) used a Completely Randomized Design (CRD), randomly assigning undergraduate students to playback speed groups (1x, 1.5x, 2x, and 2.5x) to measure the effect on comprehension of recorded lectures. Using ANOVA they found that comprehension was preserved up to 2x speed. I personally like to increase the playback speed to 1.5px if I just need to revise something quickly.\nChen, Ashley, Suchita E Kumar, Rhea Varkhedi, and Dillon H Murphy. 2024. “The Effect of Playback Speed and Distractions on the Comprehension of Audio and Audio-Visual Materials.” Educational Psychology Review 36 (3): 79.\nYou might think that we could perform three t-tests (Control vs Exp 1, Control vs Exp 3, Exp 1 vs Exp 2). We could, but the problem with this approach is what we call mutliple testing. When conducting many tests, there is an increased risk of making a Type 1 Error (rejecting the null hypothesis when it is in fact true) 1.\nWhen we have more than two groups, we can use a one-way analysis of variance (ANOVA) which can be seen as an extention of \\(t\\)-test and is called “one-way” because there is a single factor being considered. For both of these statistical approaches, the data should meet certain distributional assumptions:\n\nThere are no outliers.\nThe errors are independent.\nThe errors are normally distributed.\nAll groups have equal population variances.\n\nWe need to check the validity of these assumptions. There are both formal and informal techniques. Formal techniques (i.e. hypothesis tests) are not always appropriate for several reasons such as small datasets or that testing one assumption usually requires that the other two hold, complicating the order of tests. Informal techniques are more than sufficient and in this course, we stick with them.\n\nOutliers\nOutliers are unusual observations (response values) that deviate substantially from the remaining data points. They can have a large influence on the estimates of our model. Think of statistics such as means and variances, outlying observations will shift the mean towards them and distort the variability of the data.\nIf we’re luckly, outliers are artefacts of data recording/entering issues, such as a missing decimal points or incorrect scaling (called error outliers). These types of outliers can be corrected and the analysis can be done as usual. If, however, there are freak observations that are not clearly due to anything like data inputting, then they are likely genuine unusual responses (called interesting outliers) and should not be discarded. There are many ways of identifying and dealing with outliers ((outlier?) found 29 different ways in the literature). Here, it is recommended that the analyses should be run with and without the outliers to see whether the conclusion depends on their inclusion. When dealing with outliers, it is best to be transparent and clear about how they were handled. Simply removing outliers with no explanation is questionable research practice.\nA good way to check for outliers, is to inspect the data visually with a boxplot of your data grouped by treatment.\n\nboxplot(Posttest ~ Group, data = multitask, col = c(\"skyblue\", \"lightgreen\", \"pink\"), \n        main = \"Posttest Scores by Group\", \n        xlab = \"Group\", \n        ylab = \"Posttest Scores\")\n\nstripchart(Posttest~Group, data = multitask, vertical = TRUE, add = TRUE, method = \"jitter\")\n\n\n\n\n\n\n\nFigure 5.1: Boxplots of Post treatment scores by group.\n\n\n\n\n\nThe first line of code plots the boxplot and by inputting Posttest~Groups as the first argument we are say plot the values of Posttest by Groups. There are extra graphical parameters specified to make the plot look a bit nicer. The function stripchart is used to overlay the data points. Based on these plots, there aren’t any obvious outlying observations.\n\n\nEqual population variance\nSince we only have sample data, we would not expect that the sample variances to be exactly the same. If they are different it does not mean the population variances are different. We expect them to differ a bit due to chance simply because we are sampling from populations. Everytime we sample from a population, the dataset will be different and so will it’s variability. The sample variances need to be similar enough so that our assumption of equal population variance is reasonable. To check this assumption, we can inspect the boxplots again and compare the heights. More specifically, we look at the interquartile ranges (IQR). From looking at the plot, the IQRs do not vary widely. If you prefer to look at the actual values, we can use R to obtain them:\n\nsort(tapply(multitask$Posttest,multitask$Group,IQR))\n\n Control     Exp2     Exp1 \n14.01068 20.94529 21.97001 \n\n\nAnother measure of variability we can look at, are the standard deviations (sd’s). With the same line of code but just replacing the function we want to apply, we obtain the sd’s of each group:\n\nsort(tapply(multitask$Posttest,multitask$Group,sd))\n\n Control     Exp1     Exp2 \n10.82887 14.60601 16.42678 \n\n\nThe rule of thumb is to use the ratio of the smallest to largest standard deviation and check whether it is smaller than five. In our case, the smallest sd (of the Control group) is about 1.5 times smaller than the largest sd (of the Exp 2 group) which is acceptable.\n\n\nNormally distributed errors\nWe can check this assumption by looking at the residuals after model fitting. A common misconception is to think that the response needs to be normally distributed. However, it is only the unexplained variation, i.e. the errors or rasiduals, that we assume to be normally distributed. of course, if the response has a clearly non-normal distribution (e.g. Binomial), then the residuals are likely to be non-normal as well. So, we can check our response values before hand for obivous deviation from normality, but we have to check this assumption again after fitting our model. Things to look for are assymetric boxplots which indicate skew distributions. We also want to check that the data points tend to cluster around the median. In Figure 5.1, there are no signs of any clear deviation from normality. Other graphs we coudl look at are histograms or Quantile-Quantile (Q-Q) plots. Q-Q plots show the theoretical quantiles of the standard normal distribution against the actual quantiles of our data. We want our data to be as close to the xy line as possible (deviations in the tails are expected).\n\nqqnorm(multitask$Posttest, pty = 4, col =\"blue\")\nqqline(multitask$Posttest, col = \"red\")\n\n\n\n\n\n\n\n\nThe qqnorm function plots the theoretical quantiles on the x-axis and the sample quantile son the y-axis. So each point on the plot corresponds to a quantile from the sample plooted against the expected quantile from the standard normal distribution. As a reference we add a straight 45-degree line (in red) using the qqline function to indicate what perfect normality would look like.\n\n\nIndependent erros\nIn statistics, if one observation influences another in some way or another, they are said to be dependent. For the type of data considered here, there are two types of independence we require. Firstly, observations within treatments should be independent and second, observations between samples should be independent. Another way of saying this, is there should be independence within and among treatments. Depending on the direction of any violations, the within treatment variance or among treatment variance can either be deflated or inflated and treatment effects can be biased. This has considerable impact on the test statistic (F-ratio for ANOVAs, more on this later) which could lead to misleading results. 2\n2 Underwood (1996) has a very detailed explanation of the independence assumption (and the others) in the context of ANOVA. The book is for ecological experiments, but much of it pertains to all types of experiments.\nUnderwood, A. J. 1996. “Analysis of Variance.” In Experiments in Ecology: Their Logical Design and Interpretation Using Analysis of Variance, 140–97. Cambridge University Press.\nViolations of independence typically occur when the experimental units within or among treatments are connected in some way. Deependence within a sample can occurs when they are taken in a non-random sequence. Doing so typically allows some other variable to introduce dependence between successive observations. For example, measurement drift (when a tool’s reading gradually changes over time), physical effects (e.g. temperature) of the location of experimental units or the experimenter might become better (or worse) at taking the measurement as they move along. If these variables are not taken into account (by including them as factors in the model), it leads to a lack of independence in the errors of our model. Specifically, they lead to autocorrelated residuals; observations made closer together in time or space are more similar to each other than expected.\nAn informal check we could do, is to plot the data in the order in which they were collected (if this information is available) whether that is temporally or spatially to see if any patterns emerge. To do this in R, we can create a Cleveland dot plot.\n\ndotchart(multitask$Posttest, ylab = \"Order of observation\", xlab =\"Post treatment test score\")\n\n\n\n\n\n\n\n\nWe have assumed that the order in which the observations appear in the dataset are the order in which they were recorded. If there were any factors that caused systematic trends, (i.e. dependence) in the observations, then there would be some kind of pattern in the dot chart. For our example, there is no clear pattern. After fitting the model, we can also plot the residuals against spatial coordinate or against order to check for obvious patterns. This method, however, only detects violations of independence if observations are related to time or space.\nDependence between treatments can occur if we apply the treatments to the same group of experimental units or if experimental units from different treatments are able to interact in some way during the experiment. These types of violations including those mentioned above, are ones that we can mostly prevent or control by properly designing the experiment. When we control for factors that might induce dependence, we can include them in our model.\nOther reasons for dependence may not be as obvious or easy to eliminate as we will see below. In the end, they may not have a strong impact on our estimates but it is important to carefully scrutinise your design and the system you are studying to identify possible sources of dependence so that these can be either addressed and dealt with properly.\nIn our example, within and among group dependence could be casued by the students interacting or influencing each other in some way (by sharing notes for example). During the lectures, this can be controlled by careful monitoring and randomising their position in the lecture theatre, but outside of lectures, it is less easy to control. Here we can argue that if students interacted outside of lectures the impact on their academic performance (as measured by the test) would likely be neglibile. Here the integrity of the students is at play. It is not really possible to diagnose this type of dependence after the fact, only with careful design and implementation can these be avoided.\nIt is the onus of the experimenter to design and conduct experiments that ensure independence. With more thought (and if we’re lucky, funding) all well-designed experiments should lead to independent data. If violations are found after the fact, they cannot typically be corrected and then methods that deal specifically with dependent data (if appropriate) should be used3.\n3 A few of these methods are repeated measures ANOVA, mixed-models or hierarchical models.\n\n\n\n\n\nNote\n\n\n\n\nIn this course, you will always encounter data that has already been collected and the description of the experiment will likely not be very exhaustive. You might be task then with thinking about how the assumption of independence could have been violated, but for the most part we will assume the data are independent, both within and among samples (unless otherwise stated).\nNo real data set ever meets all assumptions of a model perfectly (remember “all models are wrong…”). Judging whether a particular data set meets our assumptions reasonably well is therefore a bit of an art. You will likely read and hear that being able to identify violations comes from experience. The best way to get experience is to look at lots of data sets where you know how well they meet the assumptions. That’s best done via simulation. We therefore encourage you to use the attached R code to simulate data where various assumptions are violated. Run the code a number of times to get a feeling for how variable your actual sample can be even if the data generating mechanism doesn’t change. You may also want to play around with the sample sizes and you can change the degree to which the assumptions are violated to get a feeling for how these violations show up in the plots.\n\n\n\nNote:\nTIPS:\n\nincrease focus, improve studying",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html",
    "href": "06_CRD_Model.html",
    "title": "6  A Simple Model for a CRD",
    "section": "",
    "text": "6.1 The model\nTo analyse data collected from a Completely Randomised Design we could use t-tests and compare the samples two at a time. This approach is problematic for two reasons. Firstly, the test statistic of a t-test is calculated with a standard deviation based only on the two samples it considers. We want our test statistic to consider the variability in all samples collected. Second, when we conduct multiple tests the overall Type 1 Error rate increases. That is, when doing many tests, the chance of making at least one wrong conclusion increases with the number of tests (if you want to know more see the box below). To avoid this, we will use the ANOVA method which was specifically developed for comparing multiple means.\nWhen we collect samples, we usually want to learn something about the populations from which they were drawn. To do this, we can develop a model for the observations that reflects the different sources of variation believed to be at play.\nFor Completely Randomised Designs (CRDs), we have \\(a\\) treatment with population means \\(\\mu_1, \\mu_2, \\mu_3, \\ldots, \\mu_a\\). We are interested in modelling the means of the treatments and the differences between them. Ultimately we want to test whether they are equal which we’ll get to in the next section. First, we construct a simple model for each observation \\(Y_{ij}\\):\n\\[\nY_{ij} = \\mu_{i} + e_{ij},\n\\]\nwhere\n\\[\n\\begin{aligned}\ni & = 1, \\dots, a \\quad (a = \\text{number of treatments}) \\\\\nj & = 1, \\dots, r \\quad (r = \\text{number of replicates}) \\\\\nY_{ij} & = \\text{observation of the } j^{th} \\text{ unit receiving treatment } i \\\\\n\\mu_i & = \\text{mean of treatment } i \\\\\ne_{ij} & = \\text{random error with } e_{ij} \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]\nThat is, each observation is modelled as the sum of its population mean and some random variation, \\(e_{ij}\\). This random variation represents unexplained differences between individual observations within the same group which we assume to follow a normal distribution with mean 0 and constant variance across all treatment groups. 1\nWe can change the notation slightly by arbitrarily dividing each mean into a sum of two components: the overall mean \\(\\mu\\) (the mean of the entire dataset, which is the same as the mean of the \\(a\\) means2) and the difference between the population mean and the overall mean. In symbols, this translates to:\n\\[\\begin{equation}\n\\begin{aligned}\n\\mu_1 &= \\mu + (\\mu_1 - \\mu) \\\\\n\\mu_2 &= \\mu + (\\mu_2 - \\mu) \\\\\n&\\;\\;\\vdots \\notag \\\\\n\\mu_a &= \\mu + (\\mu_a - \\mu)\n\\end{aligned}\n\\end{equation}\\]\nThe difference \\((\\mu_i - \\mu)\\) is the effect of treatment \\(i\\), denoted by \\(A_i\\). So each population mean is the sum of the overall mean and the part that we attribute to the particular treatment (\\(A_i\\)):\n\\[\n\\mu_i = \\mu + A_i, \\quad i = 1, 2, \\dots, a,\n\\]\nwhere \\(\\sum A_i = 0\\).\nReplacing \\(\\mu_i\\) in the model above leads to the common parameterisation of a single-factor ANOVA model3:\n\\[\nY_{ij} = \\mu + A_{i} + e_{ij}\n\\]\nwhere\n\\[\\begin{equation}\n\\begin{aligned}\ni & = 1, \\dots, a \\quad (a = \\text{number of treatments}) \\\\\nj & = 1, \\dots, r \\quad (r = \\text{number of replicates}) \\\\\nY_{ij} & = \\text{observation of the } j^{th} \\text{ unit receiving treatment } i \\\\\n\\mu & = \\text{overall or general mean} \\\\\nA_i & = \\text{effect of the } i^{th} \\text{ level of treatment factor A} \\\\\ne_{ij} & = \\text{random error with } e_{ij} \\sim N(0, \\sigma^2)\n\\end{aligned}\n\\end{equation}\\]\nThe model can be interpreted as follows:\nEach observation, \\(Y_{ij}\\), is the sum of the overall mean (\\(\\mu\\)), plus the effect of the treatment it belongs to (\\(A_i\\)), and some random error (\\(e_{ij}\\)). We use two subscripts on the \\(Y\\). One to identify the group (treatment) and the other to identify the subject (experimental unit) within the group:\n\\[\\begin{equation}\n\\begin{aligned}\nY_{1j} &= \\mu + A_1 + e_{1j} \\\\\nY_{2j} &= \\mu + A_2 + e_{2j} \\\\\nY_{3j} &= \\mu + A_3 + e_{3j} \\\\\n\n&\\;\\;\\vdots \\notag \\\\\nY_{aj} &= \\mu + A_a + e_{aj} \\\\\n\n\\end{aligned}\n\\end{equation}\\]",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#the-model",
    "href": "06_CRD_Model.html#the-model",
    "title": "6  A Simple Model for a CRD",
    "section": "",
    "text": "1 As opposed to non-constant variance across all treatment groups: \\(e_{ij} \\sim N(0, \\sigma^2_{i})\\) where the \\(\\sigma_i^2\\)’s are different.2 \\(\\mu = \\frac{\\sum\\mu_i}{a}\\)\n\n\n\n\n\n\n\n\n\n\nWhy the \\(\\sum A_i = 0\\) constraint?\n\n\n\n\n\nThis constraint ensures that the treatment effects are expressed as deviations from the overall mean. To see why this holds, take the sum of both sides of the equation:\n\\[\n\\sum_{i=1}^{a} \\mu_i = \\sum_{i=1}^{a} (\\mu + A_i).\n\\]\nExpanding the right-hand side:\n\\[\n\\sum_{i=1}^{a} \\mu_i = a\\mu + \\sum_{i=1}^{a} A_i.\n\\]\nBy definition, the overall mean \\(\\mu\\) is the mean of the treatment means:\n\\[\n\\mu = \\frac{1}{a} \\sum_{i=1}^{a} \\mu_i.\n\\]\nMultiplying both sides by \\(a\\) gives:\n\\[\n\\sum_{i=1}^{a} \\mu_i = a\\mu.\n\\]\nComparing this with our earlier equation:\n\\[\na\\mu = a\\mu + \\sum_{i=1}^{a} A_i.\n\\]\nSubtracting \\(a\\mu\\) from both sides, we get:\n\\[\n\\sum_{i=1}^{a} A_i = 0.\n\\]\nThis constraint is standard in ANOVA models to ensure that the treatment effects are relative to the overall mean rather than being arbitrarily defined. It is not an additional assumption; any \\(a\\) means can be written in this way.\n\n\n\n\n3 Often called Model I.\n\n\n\n\n\n\n\n\nComparison to regression\n\n\n\n\n\nIf you wanted to you could rewrite this with the regression notation you’ve encountered before as a regression model with a single categorical explanatory variable:\n\\[ Y_i = \\beta_0 + \\beta_1 T2_i + \\beta_2 T3_i + e_i \\]\nwhere \\(T2\\) and \\(T3\\) are indicator variables (i.e. \\(T2 = 1\\) if observation \\(i\\) is from treatment 2 and 0 otherwise). The intercept estimates the mean of the baseline category, here it is T1.\nThese two models are equivalent. The data are exactly the same: in both situations we have \\(a\\) groups and we are interested in the mean response of these groups and the difference between them. The model notation is just slightly different. In the ANOVA model we use \\(\\mu\\) and \\(A_i\\) instead of \\(\\beta_0\\) and \\(\\beta_i\\) which have different meanings.\n\n\n\n\n\n\n\nRegression\nANOVA\n\n\n\n\n\\(\\beta_0\\) is the mean of the baseline category\n\\(\\mu\\) is the overall mean\n\n\n\\(\\beta_1\\) is the difference between the means of category 2 and the baseline category.\n\\(A_i\\) is the effect of treatment \\(i\\), i.e. change in mean response relative to the overall mean.\n\n\n\nWhen all the explanatory variables are categorical, which is mostly the case in experimental data, it is more convenient to write the model in the ANOVA form, for two reasons:\n\nThe \\(A_i\\) notation is more concise, because we don’t have to add all the dummy variables. This makes it easier to read and understand because there is only one term per factor.\nMathematically it is more convenient. In this format all terms are deviations from a mean. This leads directly to sums of squares (squared deviations from a mean) and analysis of variance. We will see later that we can partition the total sum of squares into one part for every factor in the model. This allows us to investigate the variability in the response contributed by every model term (or factor).",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#estimation",
    "href": "06_CRD_Model.html#estimation",
    "title": "6  A Simple Model for a CRD",
    "section": "6.2 Estimation",
    "text": "6.2 Estimation\nOkay, so we have a model which we now need to fit to our data. When we do this, we estimate the model parameters using our data. The parameters we want to estimate are \\(\\mu\\) (the overall mean), the treatment effects (\\(A_i\\)) and \\(\\sigma^2\\) (the error variance). As for regression, we find least squares estimates for the parameters which minimise the residual or error sum of squares4:\n4 error = observed - fitted.\\[ \\text{SSE} = \\sum_i\\sum_j e_{ij}^2 = \\sum_i\\sum_j (Y_{ij} - \\hat{Y}_i)^2 = \\sum_i\\sum_j (Y_{ij} - \\mu - A_i)^2\\]\nIt turns out when we solve for the estimates that minimise the SSE5, we obtain the following estimators:\n5 Another name for this is the residual sums of squares (RSS).\\[\n\\begin{aligned}\n\\hat{\\mu} = \\bar{Y}_{..} \\\\\n\\hat{\\mu}_i = \\bar{Y}_{i.}\n\\end{aligned}\n\\]\nand\n\\[\\hat{A}_i =  \\bar{Y}_{i.} - \\bar{Y}_{..}\\]\nFrom linear model theory we know that the above are unbiased estimates6 of \\(\\mu\\) and the \\(A_i\\)’s. What does this tell you? It tells you that we can use the sample means as estimates for the true means. The estimated mean response for treatment \\(i\\) is the observed sample mean of treatment \\(i\\) and the observed overall mean is the estimated grand mean.\n6 Unbiased means that the expected value of these statistics equals the parameter being estimated. In other words, the statistic equals the true parameter on average.For the last parameter, the error variance, an unbiased estimator is found by dividing the minimised SSE (i.e. calculated with the least squares estimates) by its degrees of freedom:\n\\[ s^2 = \\frac{1}{N-a}\\sum_{ij}(Y_{ij} - \\bar{Y}_{i.})^2 \\]\nThis quantity is called the Mean Squares for Error (MSE) or residual mean square. It has \\((N-a)\\) degrees of freedom since we have \\(N\\) observations and have estimated \\(a\\) means. If you look at the formula you’ll notice that it is an average of the observed variance estimates from the different treatment groups.\n\n\n\n\n\n\nCompare this with regression\n\n\n\n\n\nCompare this with the equations you saw in the regression section. Barring the extra subscript, the only difference is the equation for calculating the fitted/predicted value.\nIn regression, the fitted value is:\n\\[ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i \\]\nand here it is:\n\\[ \\hat{Y}_{ij} = \\bar{Y}_{i.} = \\hat{\\mu} - \\hat{A}_i \\]",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#in-context-of-the-social-media-multitasking-example",
    "href": "06_CRD_Model.html#in-context-of-the-social-media-multitasking-example",
    "title": "6  A Simple Model for a CRD",
    "section": "6.3 In context of the social media multitasking example",
    "text": "6.3 In context of the social media multitasking example\nLet’s take what we’ve learned so far and apply it to our example. We had \\(a = 3\\) treatments each with \\(r=40\\) replicates. The model equation is:\n\\[ Y_{ij} = \\mu + A_{i} + e_{ij}  \\]\nwhere\n\\[\\begin{equation}\n\\begin{aligned}\ni & = 1, \\dots, 3  \\\\\nj & = 1, \\dots, 40 \\\\\n\\end{aligned}\n\\end{equation}\\]\nIf we write the model out for each treatment, we get:\n\\[\\begin{equation}\n\\begin{aligned}\nY_{Cj} &= \\mu + A_C + e_{Cj} \\\\\nY_{E1j} &= \\mu + A_{E1} + e_{E1j} \\\\\nY_{E2j} &= \\mu + A_{E2} + e_{E2j} \\\\\n\\end{aligned}\n\\end{equation}\\]\nand when we fit the model to the data, the predicted means for the treatments are:\n\\[\\begin{equation}\n\\begin{aligned}\n\\hat{Y}_{C} &= \\hat{\\mu} + \\hat{A}_C = \\bar{Y}_{C.}\\\\\n\\hat{Y}_{E1} &= \\hat{\\mu} + \\hat{A}_{E1} = \\bar{Y}_{E1.}\\\\\n\\hat{Y}_{E2} &= \\hat{\\mu} + \\hat{A}_{E2} = \\bar{Y}_{E2.}\n\\end{aligned}\n\\end{equation}\\]\nTo fit this model in R, we use the aov function and then use another function to extract the estimated parameters. By specifying type = “effects”, the function returns the \\(\\hat{A_i}\\)’s\n\nm1 &lt;- aov(Posttest~Group, data = multitask)\n\nmodel.tables(m1, type = \"effects\")\n\nTables of effects\n\n Group \nGroup\nControl    Exp1    Exp2 \n 12.049  -0.703 -11.345 \n\n\nThis tells us that the average score for students in the control group is roughly 12% higher than the overall average7. Both experimental groups performed worse, with students in the second group scoring, on average, about 11% less than the mean across all groups. We can also extract the overall mean and the treatment means by specifying type = “means”:\n7 Remember: \\(\\mu_i = \\mu + A_i\\)\nmodel.tables(m1, type = \"means\")\n\nTables of means\nGrand mean\n         \n63.58527 \n\n Group \nGroup\nControl    Exp1    Exp2 \n  75.63   62.88   52.24 \n\n\nThe grand mean (i.e. average of all test scores) was 64% in this experiment. The control group scored on average 76% which is 12% higher than the overall mean and so on. So we have the estimates for the effects, grand mean and treatment means.\nThe last parameter we need to estimate is the error variance \\(\\sigma^2\\). Have a look at the formula again:\n\\[ s^2 = \\frac{1}{N-a}\\sum_{ij}(Y_{ij} - \\bar{Y}_{i.})^2 \\]\nIf we focus on the sum and break into sums of squares for each treatment \\(i\\), we get for the first treatment (let’s say that is the control group):\n\\[ \\sum_{j}(Y_{1j} - \\bar{Y}_{1.})^2 \\] Which is the sum of the squared differences between the observations in the control group and the mean score of the group. We can easily calculated that in R:\n\ncontrol_scores &lt;- multitask$Posttest[multitask$Group == \"Control\"] # extract all scores for control group\nmean_control_scores &lt;- mean(control_scores) # calculate bar Y_1. - that is the mean score for control group\n\ncontrol_sum_squares &lt;- sum((control_scores - mean_control_scores)^2) # calculate sum of squares (not square of sum!)\n\nFirst, we subset the dataset for the scores in the control group. Then we find the mean and calculate the squared differences, which is all summed together to give the sums of squares for treatment group 1. We can repeat this for the remaining treatments and sum of the three sum of squares together and divide by \\(N-a\\) to get the MSE.\n\n# Expermiment 1 \nexp1_scores &lt;- multitask$Posttest[multitask$Group == \"Exp1\"]\nmean_exp1_scores &lt;- mean(exp1_scores)\nexp1_sum_squares &lt;- sum((exp1_scores - mean_exp1_scores)^2)\n\n# Expermiment 2 \nexp2_scores &lt;- multitask$Posttest[multitask$Group == \"Exp2\"]\nmean_exp2_scores &lt;- mean(exp2_scores)\nexp2_sum_squares &lt;- sum((exp2_scores - mean_exp2_scores)^2)\n\n# Total sums of sqaures\nsum_squares &lt;- sum(control_sum_squares + exp1_sum_squares + exp2_sum_squares)\n\nN &lt;- nrow(multitask) # number of observations overall\na &lt;- 3 # number of treatment groups \n\n\nsum_squares/(N-a) # MSE \n\n[1] 200.1463\n\n\nLater we will see that we can extract this quantity easily from the ANOVA table. But for now, this is a useful exercise to make sure you understand the formula.\nINSERT POOLED AVERAGE this will form basis for all variance estimates coming\nSo, \\(\\hat{\\sigma^2} = s^2 = 200\\) (rounded off to the nearest integer) and \\(\\hat{\\sigma} = s = 14\\).",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "06_CRD_Model.html#standard-errors-and-confidence-intervals",
    "href": "06_CRD_Model.html#standard-errors-and-confidence-intervals",
    "title": "6  A Simple Model for a CRD",
    "section": "6.4 Standard errors and confidence intervals",
    "text": "6.4 Standard errors and confidence intervals\nIn the previous section we saw how the parameters of the ANOVA model are estimated. We also need a measure of uncertainty for each of these estimates (in the form of a standard error, variance, or confidence interval). Let’s start with the variance of a treatment mean estimate:\n\n\nVariance, Standard Deviation and Standard Error: what’s all this again? The variance (Var) is a good way of measuring variability. The Standard Deviation (SD) is the square root of the variance of a sample or population. The Standard Error (SE) is the SD of an estimate (read that again).\n\\[Var(\\mu_i) = \\frac{\\sigma^2}{n_i} \\]\nRemember that the sampling distribution of the mean is \\(N(\\mu,\\frac{\\sigma^2}{n})\\) and here we assumed that the groups have equal population variances.\nIf we assume that two treatment means are independent, the variance of the difference between two means is:\n\\[\nVar(\\hat{\\mu}_i - \\hat{\\mu}_j) = Var(\\hat{\\mu}_i) + Var(\\hat{\\mu}_j) = \\frac{\\sigma^2}{n_i} + \\frac{\\sigma^2}{n_j}\n\\] To estimate these variances we substitute the MSE for \\(\\sigma^2\\) as it is an unbiased estimate of the error variance (the variability within each group). The standard errors of the estimates are found by taking the square root of the variances. The standard error is the standard deviation of an estimated quantity, and is a measure of its precision (uncertainty); how much it would vary in repeated sampling.\nWe can assume normal distributions for our estimates because we have assumed a normal linear model and because they are means (or differences between means). This means that confidence intervals for the population treatment means are of the form:\n\\[ \\text{estimate} \\pm t^{\\alpha/2}_v \\times \\text{SE}(estimate)\\] where \\(t^{\\alpha/2}_v\\) is the \\({\\alpha/2}^{th}\\) percentile of the Student’s t distribution with \\(v\\) degrees of freedom. The degrees of freedom are the error degrees of freedom, \\(N-a\\) for CRD.\nWhat are the standard errors associated with the parameter estimates in the social media example? We can easily extract this by specifying an extra argument to the model.tables function.\nStandard error of the effects:\n\nmodel.tables(m1, type = \"effects\", se = TRUE)\n\nTables of effects\n\n Group \nGroup\nControl    Exp1    Exp2 \n 12.049  -0.703 -11.345 \n\nStandard errors of effects\n        Group\n        2.237\nreplic.    40\n\n\nand for the treatment means:\n\nmodel.tables(m1, type = \"means\", se = TRUE)\n\nTables of means\nGrand mean\n         \n63.58527 \n\n Group \nGroup\nControl    Exp1    Exp2 \n  75.63   62.88   52.24 \n\nStandard errors for differences of means\n        Group\n        3.163\nreplic.    40\n\n\nSo, now we have parameter estimated and their standard errors. Equipped with these, we are closer to answering the original question: Does social media multitasking impact academic performance of students? Based on the model we diited and the parameters we estimated, how do we test this? The answer is with an ANOVA table.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A Simple Model for a CRD</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Experimental Design\n\nA systematic method to plan experiments in a way that ensures valid and unbiased results.\n\nANOVA (Analysis of Variance)\n\nA statistical method used to compare the means of three or more groups to determine if at least one differs significantly.\n\nReplication\n\nWhen treatments are applied to more than one experimental unit. The number of experimental units per treatment is the number of replicates an experiment has.\n\nRandomization\n\nA process of randomly assigning subjects or experimental units to treatments.\n\nBlocking\n\nA technique to account for variability by grouping similar experimental units.\n\nTreatment Factor\n\nAn independent variable in an experiment.\n\nTreatment Level\n\nThe specific values or categories of a factor.\n\n\n[Treatments]",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#which-experiment-has-better-evidence-that-the-true-mean-no₃-removal-rate-differs-between-species-pause-and-think-about-this-before-reading-on.",
    "href": "07_CRD_ANOVA.html#which-experiment-has-better-evidence-that-the-true-mean-no₃-removal-rate-differs-between-species-pause-and-think-about-this-before-reading-on.",
    "title": "7  Analysis of Variance",
    "section": "7.2 Which experiment has better evidence that the true mean NO₃ removal rate differs between species? Pause and think about this before reading on.",
    "text": "7.2 Which experiment has better evidence that the true mean NO₃ removal rate differs between species? Pause and think about this before reading on.\nIntuitively, we would say that Experiment 1 shows much stronger evidence for a true effect than Experiment 2. Why? Both experiments show the same differences among the treatment (species) means. So the variability in the treatment means is the same. However, the variability among the observations within treatments differs between the two experiments. In Experiment 1, the variability within treatments is much less than the variability among treatments. In Experiment 2, the variability within treatments is about the same as the variability among treatments. :::\nThe basic idea of ANOVA relies on the ratio of the among-treatment-means variation to the within-treatment variation. This is the F-ratio. The F-ratio can be thought of as a signal-to-noise ratio:\n\nLarge ratios imply the signal (difference among the means) is large relative to the noise (variation within groups), providing evidence of a difference in the means.\nSmall ratios imply the signal (difference among the means) is small relative to the noise, indicating no evidence that the means differ.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#what-are-degrees-of-freedom",
    "href": "07_CRD_ANOVA.html#what-are-degrees-of-freedom",
    "title": "7  Analysis of Variance",
    "section": "7.4 What Are Degrees of Freedom?",
    "text": "7.4 What Are Degrees of Freedom?\nDegrees of freedom (df) represent the number of independent pieces of information available for estimating a parameter. When making statistical calculations, we typically lose one degree of freedom for every estimated parameter before the current calculation.\nFor example, when estimating the standard deviation of a dataset, we first estimate the mean, thereby reducing the number of independent observations available to calculate variability. This is why the denominator in the variance formula is (N-1):\n\\[ s^2 = \\frac{\\sum(Y_i - \\bar{Y})^2}{N -1} \\]\nI like this video on degrees of freedom. Here is the definition the presenter gives:\n“Generally, the degrees of freedom is equal to the number of observations we use to estimate a parameter minus the number of intermediate parameters we need to estimate.”\nI would refine this by adding:\n“…we need to estimate prior to the current calculation.”\nYou can think of degrees of freedom as the number of independent deviations around a mean. If we have ( n ) observations and their mean, once we know ( n-1 ) of the values, the last one is fixed—it must take on a specific value to satisfy the mean equation. Therefore, only ( n-1 ) observations are truly free to vary.\nExample: Three Numbers Summing to a Fixed Mean\nSay we have three (( n=3 )) numbers: (4, 6, 8). The mean of these three numbers is 6. If we only knew the first two numbers (4,6) and the mean, the third number must be 8:\n\\[\n\\begin{aligned}\n\\bar{x} &= \\frac{\\sum x_i}{n}\\\\\n6 &= \\frac{4+6+x_3}{3}\\\\\n18 &= 10 + x_3 \\\\\nx_3 &= 8\n\\end{aligned}\n\\]\nSince the third number is uniquely determined by the first two and the mean, we only have ( n-1 ) (i.e., 2) degrees of freedom.\nAnother Intuitive Analogy\nImagine you are distributing a fixed amount of money among friends. If you have $100 and four friends, you can freely allocate money to three friends, but whatever is left must go to the fourth friend to ensure the total remains $100. Similarly, once the first ( n-1 ) values are chosen, the last value is determined, limiting the degrees of freedom.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#hypothesis-test",
    "href": "07_CRD_ANOVA.html#hypothesis-test",
    "title": "7  Analysis of Variance",
    "section": "7.5 Hypothesis test",
    "text": "7.5 Hypothesis test\nSo now we have two estimates of variances. One for variation due to treatments and one for within-treatment variation. When we take the ratio of two variances, it can be shown that the ratio has an F-distribution.\nThe test statistic is:\n\\[\nF = \\frac{MS_{\\text{treatment}}}{MS_E}\n\\]\nwith degrees of freedom \\((a-1, N-a)\\).\nFor Experiment 1:\n\\[\nF = \\frac{300}{4} = 75, \\quad p &lt; 0.001\n\\]\nFor Experiment 2:\n\\[\nF = \\frac{300}{225} = 1.33, \\quad p = 0.332\n\\]",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#the-f-test",
    "href": "07_CRD_ANOVA.html#the-f-test",
    "title": "7  Analysis of Variance",
    "section": "7.2 The F-test",
    "text": "7.2 The F-test\nWhen we take the ratio of two variances, it can be shown that the ratio follows an F-distribution with degrees of freedom equal to those of the two variances.\nSo, for example, say we want to compare the variability between two independent groups, each with normally distributed observations. We define the test statistic as the ratio of the two sample variances:\n\\[\nF = \\frac{s_1^2}{s_2^2}\n\\]\nwhere \\(s_1^2\\) and \\(s_2^2\\) are the variances of the two groups. The resulting statistic follows an F-distribution with degrees of freedom:\n\n\\(df_1 = n_1 - 1\\) for the numerator (corresponding to variance \\(s_1^2\\))\n\\(df_2 = n_2 - 1\\) for the denominator (corresponding to variance \\(s_2^2\\))\n\nThe F-distribution is a probability distribution that arises frequently in hypothesis testing, particularly in ANOVA and regression analysis. It is defined only for positive values and is right-skewed. The shape of the distribution depends on the degrees of freedom in the numerator and denominator.\n\n\nCode\n# Define the range of F-values\nx &lt;- seq(0, 5, length.out = 500)\n\n# Define degrees of freedom pairs\ndf_pairs &lt;- list(\n  c(1, 10),\n  c(5, 10),\n  c(10, 10),\n  c(20, 20)\n)\n\n# Define colors for different lines\ncolors &lt;- c(\"red\", \"blue\", \"green\", \"purple\")\n\n# Create an empty plot\nplot(x, df(x, df_pairs[[1]][1], df_pairs[[1]][2]), type=\"n\",\n     xlab=\"F value\", ylab=\"Density\",\n     main=\"F-distribution for Varying Degrees of Freedom\")\n\n# Loop through df pairs and add lines\nfor (i in seq_along(df_pairs)) {\n  lines(x, df(x, df_pairs[[i]][1], df_pairs[[i]][2]), col=colors[i], lwd=2)\n}\n\n# Add a legend\nlegend(\"topright\", legend=paste(\"df1 =\", sapply(df_pairs, `[[`, 1), \", df2 =\", sapply(df_pairs, `[[`, 2)), \n       col=colors, lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\n\nKey properties of the F-distribution:\n\nIt is always non-negative: \\(F \\geq 0\\).\nIt is asymmetric and skewed to the right, particularly for small degrees of freedom.\nAs the degrees of freedom increase, the F-distribution approaches a normal shape.\nThe mean of an F-distribution with \\((df_1, df_2)\\) degrees of freedom is approximately:\n\n\\[\nE[F] = \\frac{df_2}{df_2 - 2}, \\quad \\text{for } df_2 &gt; 2\n\\]",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#the-f-test-in-anova",
    "href": "07_CRD_ANOVA.html#the-f-test-in-anova",
    "title": "7  Analysis of Variance",
    "section": "7.6 The F-test in ANOVA",
    "text": "7.6 The F-test in ANOVA\nWe first set up the null and alternate hypothesis. The null hypothesis is that all treatments have the same mean, or equivalently, that all treatment effects are zero.\n\\[H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_a\\] And the alternate hypothesis is the opposite of that:\n\\[H_A: \\text{At least one } \\mu_i \\text{ is different.}\\] :::{.column-margin} Read that again. The alternative is that at least one treatment is different. It is not that all treatment means are different. :::\nIf \\(H_0\\) is true, the among-treatment-means variation should equal the within-treatment variation. We can use the F-ratio to test \\(H_0\\):\n\\[ F = \\frac{MS_A}{MSE} \\]\nThis ratio has an F-distribution with \\(a-1\\) numerator degrees of freedom and \\(N-a\\) denominator degrees of freedom.\nYou can think of the F-ratio as a signal-to-noise ratio. If \\(H_0\\) is true, \\(F\\) is expected to be close to 1. If \\(H_0\\) is false, \\(F\\) is expected to be much larger than 1. This means that the F-test we conduct is a one-sided upper tailed test. If \\(H_0\\) is false, the means squares for treatment will be much larger than the MSE, resulting in large F-values. We are only interested in this one side of possible outcomes therefore, a one-sided test.\nIn Experiment 1, \\(F = \\frac{300}{4} = 75\\), which leads to a very small \\(p\\)-value (\\(&lt; 0.001\\)). The signal was much larger than the noise, and our data are very unlikely if \\(H_0\\) were true. So we have good evidence that the treatments differ.\nIn Experiment 2, \\(F = \\frac{300}{225} = 1.33\\), which leads to a large \\(p\\)-value (\\(0.33\\)). Signal and noise were of similar magnitude, and our data are not unlikely if \\(H_0\\) were true. So we have no evidence against \\(H_0\\), i.e., no evidence that nitrate extraction differs between species.\nHow did we get these p-values? This is the same as in any hypothesis test. We have a test statistic (which quantifies the strength of …) and to say somehting about how likely this test statistic (or more extreme is) under the null hypothesis, we need the null distribution of the test statistic (that is the sampling distribution of the test statistic as if the null hypothesis were true). We then compared the observed value of the test statistic to that null distribution and asked ourselves how unusual it is in light of that distribution. Does out test statistic belong to this null distribution?\nFor the F-value of test statistic follows an F distribution as specified above.\n\\[\\text{F}^* \\sim \\text{F}_{(a-1),\\;(N-a)}\\]\nFor both experiment, this equates to an F distribution with 2 numerator and 6 denominator degrees of freedom which looks like this:\n\n\nCode\n# Define the range of F-values\nx &lt;- seq(0, 100, length.out = 500)\ny &lt;- df(x, df1 = 2, df2 = 6)\nplot(x, y, type=\"l\",\n     xlab=\"F value\", ylab=\"Density\",\n     main=\"\")\n\n\n\n\n\n\n\n\n\nWe can plot the test statistics on the graph as well and highlight the area under the curve to the right of each of these test statistics:\n\n\nCode\n# Define x values\nx &lt;- seq(0, 100, length.out = 500)\ny &lt;- df(x, df1 = 2, df2 = 6)\n\n# Define test_stats\ntest_stats &lt;- c(75, 1.33)\n\n# Plot the F-distribution density curve\nplot(x, y, type = \"l\", col = \"black\", lwd = 2,\n     xlab = \"F value\", ylab = \"Density\",\n     main = \"\")\n\n# Add vertical lines at test_stats\nabline(v = test_stats, col = \"red\", lty = 2, lwd = 2)\n\n# Shade the areas to the right of the test_stats\npolygon(c(test_stats[1], x[x &gt;= test_stats[1]], max(x)), \n        c(0, y[x &gt;= test_stats[1]], 0), col = rgb(0, 0, 1, 0.3), border = NA)\n\npolygon(c(test_stats[2], x[x &gt;= test_stats[2]], max(x)), \n        c(0, y[x &gt;= test_stats[2]], 0), col = rgb(1, 0, 0, 0.3), border = NA)\n\n# Add points at the critical values\npoints(test_stats, df(test_stats, df1 = 2, df2 = 6), pch = 19, col = \"black\")\n\n\n\n\n\n\n\n\n\nRemember sampling distributions are probability distributions. For continuous random variables, the area under the curve represents probability. Specifically, the probability of a random variable taking on a specific value or larger, is the area udner the curve to the right of that value. For test statistics and their probability distribution, that probability is the p-value. The p-value is the probability of observing a test statistic at least as extreme as we did if the null hypothesis was in fact true. The smaller the p-value, the stronger the evidence against \\(H_0\\).\nWe can obtain the p-value in two ways (you will need to be able to do both):\n\nUsing Software.\n\nIn R, there are several built-in functions for certain probability distributions. These functions typically follow a naming convention:\n\nd&lt;dist&gt;() for density functions\np&lt;dist&gt;() for cumulative probability functions\nq&lt;dist&gt;() for quantile functions\nr&lt;dist&gt;() for random sampling\n\nFor example, when working with the F-distribution, we use:\n\ndf(x, df1, df2) for the probability density function (PDF)\npf(x, df1, df2) for the cumulative distribution function (CDF)\nqf(p, df1, df2) for quantiles\nrf(n, df1, df2) for random sampling\n\nTo obtain a p-value, we often use the cumulative probability functions (p&lt;dist&gt;()) with returns \\(Pr[X&lt;x]\\) so \\(Pr[X&gt;x] = 1 - Pr[X&lt;x]\\). Below is how to obtain the p-value for the second experiment:\n\nf_statistic &lt;- 1.33\ndf1 &lt;- 2  # Numerator degrees of freedom\ndf2 &lt;- 6  # Denominator degrees of freedom\n\n# Upper-tail probability (right-tailed test)\np_value &lt;- 1 - pf(f_statistic, df1, df2)\np_value\n\n[1] 0.332583\n\n\nThis value is quite large and corresponds to the area to the right of an F value of 1.33 for the distribution above. We interpret this p-value as the test statistic is quite likely to have come from this null distribution, there is a 33% chance of observing this test statistic or more exteme if the null hypothesis is true. We do not have strong evidence against the null hypothesis of equal means.\n\n\n\n\n\n\nCaution\n\n\n\nA large p-value does not mean that \\(H_0\\) is true!\n\nThe p-value is not the probability that the null hypothesis is true.\nThe p-value is not the probability that the alternative hypothesis is false.\nThe p-value is a statement about the relation of the data to the null hypothesis.\nThe p-value does not indicate the size or biological importance of the observed pattern.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can round the p-value if you need to enter the value to a certian number of decimals in a quiz or test using the function round.\n\n\n\nUsing tables.\n\nBefore the days of programming, statisticians used tablse..",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#other",
    "href": "07_CRD_ANOVA.html#other",
    "title": "7  Analysis of Variance",
    "section": "7.7 Other",
    "text": "7.7 Other\nSo now we have two estimates of variances. One for variation due to treatments(\\(MS_A\\)) and one for within-treatment variation \\(MS_E\\).\nThe test statistic is:\n\\[\nF = \\frac{MS_{\\text{treatment}}}{MS_{\\text{error}}}\n\\]\nwith degrees of freedom \\((a-1, N-a)\\).\nFor Experiment 1:\n\\[\nF = \\frac{300}{4} = 75, \\quad p &lt; 0.001\n\\]\nFor Experiment 2:\n\\[\nF = \\frac{300}{225} = 1.33, \\quad p = 0.332\n\\]",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "07_CRD_ANOVA.html#back-to-the-constructed-example",
    "href": "07_CRD_ANOVA.html#back-to-the-constructed-example",
    "title": "7  Analysis of Variance",
    "section": "7.5 Back to the constructed example",
    "text": "7.5 Back to the constructed example\nWhat does the ANOVA table look like for our constructed example? You’ve already worked out the sums of squares. What are the df’s and Mean Sqaures?\nLet’s have a look at Experiment 1 first.\n\n# Experiment 1 data \nexp1data &lt;- data.frame(species = rep(c(\"A\",\"B\",\"C\"), each = 3),\n                       response = c(40,42,38,48,50,52,58,62,60))\n\nexp1_anova &lt;- aov(response~species, data = exp1data)\nsummary(exp1_anova)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nspecies      2    600     300      75 5.69e-05 ***\nResiduals    6     24       4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd then Experiment 2:\n\n# Experiment 2 data \nexp2data &lt;- data.frame(species = rep(c(\"A\",\"B\",\"C\"), each = 3),\n                       response = c(40,25,55,65,35,50,45,75,60))\n\nexp2_anova &lt;- aov(response~species, data = exp2data)\nsummary(exp2_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nspecies      2    600     300   1.333  0.332\nResiduals    6   1350     225               \n\n\nSince the overall mean and the treatment means were the same in both experiment, we expected the \\(SS_{\\text{treatment}}\\) to be the same in both experiments. This was indeed the case – they are 600 in both experiments. The sample sizes were also the same in both experiments, so we would expect the df to be the same. With 9 observations, we have 8 df in total. Three treatments (Species) leads to 2 treatment df and 6 df remain for the residuals. The difference between the two experiments is that the observations were much more variable in Experiment 2 than in Experiment 1. Accordingly, we find that \\(SS_{\\text{error}}\\) was much larger in Experiment 2, and this led to larger MSE in Experiment 2. How does this affect the conclusions we draw from each of the experiments? This is where the F-ratio comes in.",
    "crumbs": [
      "Single Factor Completely Randomised Designs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "00_HypoTesting.html",
    "href": "00_HypoTesting.html",
    "title": "A brief guideline to hypothesis testing",
    "section": "",
    "text": "The General Framework\nThese notes have been adapted from the STA1007 notes (authored by Dr Res Altwegg and Dr Greg Distiller and some other texbooks.\nHypothesis testing is a statistical procedure of using sample data to make inferences about populations. Unlike estimation, where the goal is to quantify a parameter, hypothesis testing assesses whether an observed effect is statistically significant. More specifically, a hypothesis test evaluates two mutally exclusive statements about the population and determines which statement the data supports\nHypothesis testing follows a structured process:\nThe basic idea of hypothesis testing is that we set up a so-called null hypothesis and then ask how likely our data are if the null hypothesis were true. If they are unlikely, we conclude that we have found evidence against the null hypothesis, i.e. the null hypothesis is probably not true.\nThe alternative hypothesis covers all the possibilities not covered by the null hypothesis. If we conclude that the null hypothesis is probably not true, that means that the alternative hypothesis is probably true. Notice that these two hypotheses are not on equal footing. You can think of the null hypothesis as representing a baseline against which the data are compared, whereas the alternative hypothesis is what we really care about, worry about or want to demonstrate. This is an important asymmetry and will need some careful reflection.\nBelow is an example:\nNull Hypothesis (\\(H_0\\)): “The average weight of chocolate bars is 100g.”\nNull Hypothesis (\\(H_A\\)): “The average weight of chocolate bars is less than 100g.”\nLack of evidence against \\(H_0\\) is not the same as evidence for \\(H_0\\). We never say that we have evidence for \\(H_0\\) or that we accept \\(H_0\\) as true.\nA numerical function of the data that quantifies the strength of the observed effect, whose valye determines the result of the test. Examples include the mean difference, proportion difference, or z-score.\nIn order to say something about the .. The expected distribution of the test statistic under (\\(H_0\\)).\nThe probability of obtaining a result as extreme as the observed one if H₀ is true. A small P-value (typically &lt;0.05) suggests strong evidence against (\\(H_0\\)).",
    "crumbs": [
      "A brief guideline to hypothesis testing"
    ]
  },
  {
    "objectID": "00_HypoTesting.html#one-sided-vs.-two-sided-tests",
    "href": "00_HypoTesting.html#one-sided-vs.-two-sided-tests",
    "title": "A brief guideline to hypothesis testing",
    "section": "One-Sided vs. Two-Sided Tests",
    "text": "One-Sided vs. Two-Sided Tests\nTwo-sided test: Tests for deviations in both directions. Example: “The average human body temperature is different from 37°C.”\nOne-sided test: Tests for deviations in a single direction. Example: “Students who study more than an hour score higher.”",
    "crumbs": [
      "A brief guideline to hypothesis testing"
    ]
  },
  {
    "objectID": "00_HypoTesting.html#decision-making-in-hypothesis-testing",
    "href": "00_HypoTesting.html#decision-making-in-hypothesis-testing",
    "title": "A brief guideline to hypothesis testing",
    "section": "Decision Making in Hypothesis Testing",
    "text": "Decision Making in Hypothesis Testing\nA small P -value constitutes evidence against \\(H_0\\). But how small is small enough? Sometimes, we want to make a firm decision about whether we can believe that the observed pattern is real or not. This requires us to choose a threshold for P. This threshold is called the significance level and denoted by \\(\\alpha\\). If we obtain a P -value that is smaller than \\(\\alpha\\), we say that we have obtained a “statistically significant result” or that “\\(H_0\\) is rejected”. If our P -value is larger than \\(\\alpha\\), we say that our result is “not significant” or that “\\(H_0\\) is not rejected”. In most situations, researchers choose a significance level of \\(\\alpha\\) = 0.05, which roughly corresponds to the probability of obtaining five heads in a row when tossing a fair coin, as we have seen. Different values for \\(\\alpha\\) are also sometimes used; the next most common significance level is \\(\\alpha\\) = 0.01.\nBefore we go further, we want to emphasize that there is nothing magic about a specific value of \\(\\alpha\\). This threshold is an arbitrary choice and should not be taken too seriously. You understand now that there is not much difference between a P-value of 0.051 and 0.049. Both constitute about the same strength of evidence against \\(H_0\\). Yet, when we apply \\(\\alpha\\) = 0.05, we would reach opposite conclusions in the two cases. It is always better to report the exact P-value rather than just state P &gt; 0.05 or P &lt; 0.05 or state that a result is “not significant” or “significant”. And it is particularly important not to imply that a “non-significant” result means that there is no effect (that would be saying \\(H_0\\) is true when we might in fact have some evidence against it)!\nAlas, dividing results into “significant” vs “not significant” is very entrenched in many fields and you will encounter these terms a lot. And used wisely, this distinction can have its merits. So we’ll stick with it.",
    "crumbs": [
      "A brief guideline to hypothesis testing"
    ]
  }
]